{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkeMZG7QXMo1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# An Independent Implementation of \"Parental environment and student achievement: Does a Matthew effect exist?\"\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2510.18481v1-b31b1b.svg)](https://arxiv.org/abs/2510.18481v1)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/parental_environment_and_student_achievement)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Economics%20of%20Education-00529B)](https://github.com/chirindaopensource/parental_environment_and_student_achievement)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Madrid%20Gov%20%7C%20Barro--Lee-lightgrey)](https://github.com/chirindaopensource/parental_environment_and_student_achievement)\n",
        "[![Core Method](https://img.shields.io/badge/Method-IV%20%7C%20Fixed%20Effects-orange)](https://github.com/chirindaopensource/parental_environment_and_student_achievement)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Causal%20Inference%20%7C%20Matthew%20Effect-red)](https://github.com/chirindaopensource/parental_environment_and_student_achievement)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/matplotlib-%2311557c.svg?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)\n",
        "[![linearmodels](https://img.shields.io/badge/linearmodels-003F72-blue)](https://bashtage.github.io/linearmodels/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Faker](https://img.shields.io/badge/Faker-lightgrey)](https://faker.readthedocs.io/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "---\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/parental_environment_and_student_achievement`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Parental environment and student achievement: Does a Matthew effect exist?\"** by:\n",
        "\n",
        "*   Gaëlle Aymeric\n",
        "*   Emmanuelle Lavaine\n",
        "*   Brice Magdalou\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and feature engineering to the core econometric estimations (OLS and IV with Fixed Effects) and the final generation of all tables, figures, and reports.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_full_study`](#key-callable-execute_full_study)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Parental environment and student achievement: Does a Matthew effect exist?\". The core of this repository is the iPython Notebook `parental_environment_and_student_achievement_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of all analytical tables and figures.\n",
        "\n",
        "The paper investigates the causal impact of parental environment on student achievement, distinguishing between a static \"persistent effect\" and a dynamic \"Matthew Effect\" (the changing influence of parental background over time). This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Process raw student survey data and external instrument data, applying a sequence of cleaning, harmonization, and feature engineering steps.\n",
        "-   Estimate all 33 econometric models from the paper, including pooled OLS, interaction OLS, and their 2SLS instrumental variable counterparts, all with high-dimensional (school and year) fixed effects and clustered standard errors.\n",
        "-   Run a full suite of diagnostic and robustness checks to validate the credibility of the findings.\n",
        "-   Automatically generate all key figures and a final narrative report summarizing the results and policy implications.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern microeconometrics and causal inference.\n",
        "\n",
        "**1. High-Dimensional Fixed Effects (HDFE) OLS:**\n",
        "The baseline models for the persistent effect (Equation 1) and the Matthew Effect (Equation 2) are estimated via OLS with fixed effects for both school ($b_s$) and academic year ($a_t$).\n",
        "$$\n",
        "Y_{its} = \\alpha_0 + \\sum_{j=1,2} \\alpha_j p_{ji} + \\text{Controls} + a_t + b_s + \\epsilon_{its} \\quad (1)\n",
        "$$\n",
        "$$\n",
        "Y_{igts} = \\dots + \\sum_{j=1,2} \\theta_j p_{ji} I(u,v) + \\dots \\quad (2)\n",
        "$$\n",
        "The fixed effects are absorbed using the Frisch-Waugh-Lovell theorem to control for all time-invariant school characteristics and year-specific shocks.\n",
        "\n",
        "**2. Two-Stage Least Squares (2SLS) with an Instrumental Variable (IV):**\n",
        "To address the endogeneity of parental education (`PHE`), the study employs a 2SLS strategy. The instrument is the gender gap in tertiary education in the parent's country of origin in 1960 (`GG`).\n",
        "-   **First Stage (Relevance):** The endogenous variable is regressed on the instrument and all exogenous controls.\n",
        "    $$\n",
        "    \\text{PHE}_i = \\pi_0 + \\pi_1 \\text{GG}_i + \\text{Controls} + a_t + b_s + \\nu_{its} \\quad (3)\n",
        "    $$\n",
        "-   **Second Stage (Causal Effect):** The outcome is regressed on the *predicted* value of the endogenous variable from the first stage.\n",
        "    $$\n",
        "    Y_{its} = \\alpha_0 + \\alpha_1 \\widehat{\\text{PHE}}_i + \\text{Controls} + a_t + b_s + \\epsilon_{its} \\quad (4)\n",
        "    $$\n",
        "This methodology is extended to the interaction models to estimate the *causal* Matthew Effect.\n",
        "\n",
        "**3. Cluster-Robust Standard Errors:**\n",
        "All models are estimated with standard errors clustered at the `school_id` level. This accounts for the fact that residuals for students within the same school may be correlated, providing more conservative and reliable statistical inference.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`parental_environment_and_student_achievement_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 27 distinct, modular tasks, each with its own orchestrator function, ensuring clarity and testability.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file, allowing for easy customization and replication.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, content integrity, and statistical properties of all input data before any analysis begins.\n",
        "-   **Systematic Feature Engineering:** A transparent, step-by-step process for constructing the main endogenous variable (`PHE`), the instrumental variable (`GG`), and the full set of dummy-coded control variables.\n",
        "-   **Advanced Econometric Engine:** A generic estimation function that correctly handles high-dimensional fixed effects and clustered standard errors for both OLS and 2SLS models using the `linearmodels` library.\n",
        "-   **Comprehensive Estimation Suite:** Programmatically specifies and estimates all 33 models from the paper.\n",
        "-   **Automated Diagnostics and Reporting:** Concludes by automatically generating all key figures, a full suite of robustness checks, and a final narrative summary of the findings.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Validation & Cleaning (Tasks 1-5):** Ingests and validates the `config.yaml` and raw data, cleanses the student survey data by applying inclusion criteria, and harmonizes all categorical codings.\n",
        "2.  **Feature Engineering (Tasks 6-8):** Constructs the `PHE` variable and its dummies, creates the full set of control variables, and constructs the `GG` instrumental variable.\n",
        "3.  **Sample & Model Definition (Tasks 9-11):** Programmatically defines the 33 distinct analysis samples using listwise deletion and creates the formal specifications for all OLS and IV models.\n",
        "4.  **Estimation (Tasks 12-20):** Executes all OLS and IV regressions (pooled, by-grade, and interaction), extracting and formatting the results.\n",
        "5.  **Visualization & Diagnostics (Tasks 21-24):** Generates all CDF and coefficient plots, and performs final diagnostic checks on sample composition and data normalization.\n",
        "6.  **Robustness & Reporting (Tasks 25-27):** Provides top-level orchestrators to run the full pipeline, execute a suite of robustness checks, and synthesize the final narrative report.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `parental_environment_and_student_achievement_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_full_study`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_full_study`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, from data validation to the final report generation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `matplotlib`, `linearmodels`, `pyyaml`, `faker`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/parental_environment_and_student_achievement.git\n",
        "    cd parental_environment_and_student_achievement\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy matplotlib linearmodels pyyaml Faker\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two `pandas.DataFrame` objects with specific schemas, which are rigorously validated by the pipeline. A synthetic data generator is included in the notebook for demonstration purposes.\n",
        "1.  **`student_parent_survey_raw`**: The primary dataset containing student and parent information.\n",
        "2.  **`barro_lee_raw`**: The external dataset with historical education data for constructing the instrument.\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `parental_environment_and_student_achievement_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_full_study` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # The notebook includes a synthetic data generation stage.\n",
        "    # We assume `student_parent_survey_raw` and `barro_lee_raw` are in memory.\n",
        "    \n",
        "    # Load the configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # Define the top-level directory for all outputs.\n",
        "    RESULTS_DIRECTORY = \"study_replication_output\"\n",
        "\n",
        "    # Execute the entire replication study.\n",
        "    full_results = execute_full_study(\n",
        "        student_parent_survey_raw=student_parent_survey_raw,\n",
        "        barro_lee_raw=barro_lee_raw,\n",
        "        config=config,\n",
        "        output_dir=RESULTS_DIRECTORY\n",
        "    )\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline creates a `base_output_dir` and returns a comprehensive dictionary containing all analytical artifacts, structured as follows:\n",
        "-   **`main_analysis`**: Contains all primary results.\n",
        "    -   `results`: Nested dictionary of all regression tables (DataFrames).\n",
        "    -   `diagnostics`: Diagnostic tables (e.g., sample flow).\n",
        "    -   `figures`: List of paths to all generated plots (PNG files).\n",
        "-   **`robustness_checks`**: Contains all sensitivity analysis reports.\n",
        "-   **`final_summary_report`**: A string containing the final narrative summary.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "parental_environment_and_student_achievement/\n",
        "│\n",
        "├── parental_environment_and_student_achievement_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                               # Master configuration file\n",
        "├── requirements.txt                                          # Python package dependencies\n",
        "│\n",
        "├── study_replication_output/                                 # Example output directory\n",
        "│   └── figures/\n",
        "│       ├── cdf_math_grade_3.png\n",
        "│       └── ...\n",
        "│\n",
        "├── LICENSE                                                   # MIT Project License File\n",
        "└── README.md                                                 # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all study parameters, including date ranges, variable definitions, and model specifications, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Instruments:** Integrating and testing alternative instrumental variables to check the robustness of the causal claims.\n",
        "-   **Heterogeneity Analysis:** Exploring how the Matthew Effect varies across different subgroups of the population (e.g., by gender, immigrant status, or school type).\n",
        "-   **Machine Learning Models:** Using non-parametric or machine learning methods (e.g., causal forests) to explore potential non-linearities in the treatment effects.\n",
        "-   **Structural Modeling:** Building and estimating a structural model of skill formation to provide deeper theoretical insights into the observed dynamic patterns.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{aymeric2025parental,\n",
        "  title={Parental environment and student achievement: Does a Matthew effect exist?},\n",
        "  author={Aymeric, Ga{\\\"e}lle and Lavaine, Emmanuelle and Magdalou, Brice},\n",
        "  journal={arXiv preprint arXiv:2510.18481},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Professional-Grade Implementation of \"Parental environment and student achievement: Does a Matthew effect exist?\".\n",
        "GitHub repository: https://github.com/chirindaopensource/parental_environment_and_student_achievement\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Gaëlle Aymeric, Emmanuelle Lavaine, and Brice Magdalou** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Matplotlib, and the `linearmodels` library**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `parental_environment_and_student_achievement_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "Tl3aUkFDBPMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Parental environment and student achievement: Does a Matthew effect exist?*\"\n",
        "\n",
        "Authors: Gaëlle Aymeric, Emmanuelle Lavaine, Brice Magdalou\n",
        "\n",
        "E-Journal Submission Date: 21 October 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2510.18481v1\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This paper investigates the causal impact of the parental environment on the student's academic performance in mathematics, literature and English (as a foreign language), using a new database covering all children aged 8 to 15 of the Madrid community, from 2016 to 2019. Parental environment refers here to the parents' level of education (i.e. the skills they acquired before bringing up their children), and parental investment (the effort made by parents to bring up their children). We distinguish the persistent effect of the parental environment from the so-called Matthew effect, which describes a possible tendency for the impact of the parental environment to increase as the child grows up. Whatever the subject (mathematics, literature or English), our results are in line with most studies concerning the persistent effect: a favourable parental environment goes hand in hand with better results for the children. As regards the Matthew effect, the results differ between subjects: while the impact of the parental environment tends to diminish from the age of 8 to 15 in mathematics, it forms a bell curve in literature (first increasing, then decreasing) and increases steadily in English. This result, which is encouraging for mathematics and even literature, confirms the social dimension involved in learning a foreign language compared to more academic subjects.\n"
      ],
      "metadata": {
        "id": "uUzwCXx3XW3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary and Analysis of Aymeric, Lavaine, and Magdalou (2025)**\n",
        "\n",
        "#### **Core Research Question and Contribution**\n",
        "\n",
        "The paper investigates the causal impact of the \"parental environment\" on a child's academic achievement over time. It seeks to disentangle two distinct phenomena:\n",
        "\n",
        "1.  **The Persistent Effect:** The baseline observation that a more favorable parental environment (higher parental education, greater investment) is consistently associated with better student outcomes at any given point in time.\n",
        "2.  **The Matthew Effect:** The central hypothesis under investigation. Named after a biblical passage (\"the rich get richer\"), this effect posits that the *impact* of the parental environment *grows* as a child ages. In other words, initial advantages accumulate and widen the achievement gap over time.\n",
        "\n",
        "The paper's primary contribution is to test for the existence and nature of this Matthew effect across three core academic subjects—mathematics, literature, and English—using a novel and comprehensive dataset.\n",
        "\n",
        "#### **Data and Descriptive Analysis**\n",
        "\n",
        "The authors construct a unique and powerful dataset from the Madrid Community in Spain, covering all students aged 8, 11, and 15 (Grades 3, 6, and 10) from 2016 to 2019.\n",
        "\n",
        "*   **Source:** Administrative data from the Ministry of Education and Research of the Community of Madrid.\n",
        "*   **Scope:** It is a census, not a sample, covering over 320,000 students in both public and private schools.\n",
        "*   **Variables:**\n",
        "    *   **Outcome:** Standardized test scores in mathematics, literature, and English (foreign language), normalized to a mean of 500 and a standard deviation of 100 (similar to PISA scores).\n",
        "    *   **Key Explanatory Variable:** Parental environment, operationalized through two channels:\n",
        "        1.  **Parents' Highest Level of Education:** Categorized into three ISCED levels (0-2, 3-5, 6-8).\n",
        "        2.  **Parental Investment:** Proxied by survey responses on the frequency of parent-child discussions about school.\n",
        "    *   **Controls:** A rich set of student and household characteristics (e.g., gender, country of birth, student's time spent on homework).\n",
        "\n",
        "**Descriptive Finding:** A preliminary look at the data via Cumulative Distribution Functions (CDFs) reveals a clear pattern of **first-order stochastic dominance**. For any given score, a child whose parents have a higher level of education has a higher probability of achieving that score or better. This strongly suggests the existence of the *persistent effect* and inequality of opportunity.\n",
        "\n",
        "#### **Econometric Strategy I - OLS with Interaction Terms**\n",
        "\n",
        "The authors first employ a linear regression model with fixed effects to establish baseline correlations.\n",
        "\n",
        "*   **Model 1 (Persistent Effect):** They regress student scores on dummy variables for parental education levels, controlling for student/household characteristics, school fixed effects, and year fixed effects.\n",
        "    \n",
        "    `Score_its = α₀ + α₁·ParentEd_i¹ + α₂·ParentEd_i² + Controls_i + Year_t + School_s + ε_its`\n",
        "    \n",
        "    Here, `α₁` and `α₂` measure the average score advantage for children from medium and high-education families, respectively, compared to the low-education reference group.\n",
        "    \n",
        "*   **Model 2 (Matthew Effect):** To test how this effect evolves, they introduce interaction terms between the parental education dummies and grade-level dummies.\n",
        "    \n",
        "    `Score_igts = ... + θ₁·(ParentEd_i¹ × Grade_g) + θ₂·(ParentEd_i² × Grade_g) + ...`\n",
        "    \n",
        "    The coefficients `θ₁` and `θ₂` are the core of this analysis. A positive and significant `θ` would indicate that the achievement gap associated with parental education widens as students move to a higher grade, providing evidence for a Matthew effect. A negative `θ` would suggest the gap is closing.\n",
        "\n",
        "#### **Results from the OLS Analysis**\n",
        "\n",
        "The OLS models yield nuanced, subject-specific results regarding the Matthew effect.\n",
        "\n",
        "*   **Persistent Effect:** Confirmed across all subjects. Higher parental education is strongly and significantly associated with higher student scores.\n",
        "*   **Matthew Effect (via interaction terms):**\n",
        "    *   **Mathematics:** The impact of parental education *diminishes* between Grade 6 and Grade 10. The achievement gap narrows.\n",
        "    *   **Literature:** The impact follows a **bell-shaped curve**. It increases from Grade 3 to 6 and then decreases from Grade 6 to 10.\n",
        "    *   **English (Foreign Language):** The impact **continuously and significantly increases** across all grades. This is the only subject that exhibits a classic, unambiguous Matthew effect.\n",
        "\n",
        "#### **Econometric Strategy II - Instrumental Variable (IV) Analysis**\n",
        "\n",
        "Recognizing that OLS estimates may be biased by endogeneity (e.g., unobserved variables like parental motivation or inherited ability that correlate with both parental education and child achievement), the authors implement a more rigorous instrumental variable (IV) strategy to identify a causal effect.\n",
        "\n",
        "*   **The Instrument:** The **gender gap in tertiary education in 1960 in the parent's country of origin**.\n",
        "*   **Justification for Validity:**\n",
        "    1.  **Relevance:** This historical, country-level variable is plausibly correlated with a parent's own educational attainment, especially for immigrant families. The first-stage regressions confirm this with high F-statistics (well above the conventional threshold of 10), indicating a strong instrument.\n",
        "    2.  **Exclusion Restriction:** The instrument is assumed to affect the child's current academic performance in Madrid *only through* its effect on their parent's education. It is unlikely that the gender education gap in, for example, Ecuador in 1960 has a direct impact on a child's test scores today, other than via the channel of parental human capital.\n",
        "\n",
        "*   **Implementation:** They use a two-stage least squares (2SLS) approach, first predicting parental education using the instrument and then using these predicted values to estimate the causal effect on student scores.\n",
        "\n",
        "#### **Results from the IV Analysis**\n",
        "\n",
        "The IV estimates largely confirm and strengthen the OLS findings, lending them a causal interpretation.\n",
        "\n",
        "*   **Persistent Effect:** The causal impact of parental education on student scores is positive, significant, and large. For instance, a one-category increase in parental education causally increases scores by 37 points in math, 51 in literature, and a striking 72 in English.\n",
        "*   **Matthew Effect (via IV with interactions):** The dynamic pattern observed in the OLS models holds.\n",
        "    *   **Mathematics & Literature:** The causal influence of parental education diminishes as students age (particularly after Grade 6).\n",
        "    *   **English:** The causal influence is unambiguously amplified as students mature, confirming a strong Matthew effect.\n",
        "\n",
        "#### **Analysis of Student Effort and Parental Investment**\n",
        "\n",
        "The paper also examines the evolving impact of student effort (proxied by time spent on homework) and parental investment (frequency of school-related talks).\n",
        "\n",
        "*   **Student Effort:** The impact of homework becomes increasingly positive as students get older. In Grade 3, more homework is correlated with *lower* scores (likely reverse causality), but by Grade 10, it is a strong positive predictor of success, especially in literature and math.\n",
        "*   **Parental Investment:** The impact of parental involvement decreases over time in math and literature but, consistent with the main findings, *increases* in English.\n",
        "\n",
        "#### **Discussion and Policy Implications**\n",
        "\n",
        "The authors conclude by interpreting their heterogeneous findings and drawing policy implications.\n",
        "\n",
        "*   **Interpretation:** The divergence between subjects is key.\n",
        "    *   In **mathematics and literature**, students appear to gain autonomy as they age. Their own effort becomes a more dominant factor, and the influence of their family background wanes. This aligns with cognitive development theories (e.g., Piaget) where children develop intrinsic learning capabilities.\n",
        "    *   In **English (foreign language)**, learning is presented as a more socially-embedded process. Parental resources, exposure, and attitudes (integrative motivation) become *more* critical over time, creating a path-dependent accumulation of advantage (aligning with theories from Vygotsky and Cunha & Heckman).\n",
        "\n",
        "*   **Policy Implications:**\n",
        "    1.  **Remediation:** The diminishing influence of family background in math and literature suggests that remediation programs for adolescents in these subjects can be effective, as student effort plays a larger role.\n",
        "    2.  **Equality of Opportunity:** The strong Matthew effect in English implies that using foreign language proficiency as a key criterion for university or elite school admission can significantly exacerbate inequality, as it heavily favors students from advantaged backgrounds.\n",
        "\n",
        "### **Overall Assessment**\n",
        "\n",
        "This is a methodologically robust and compelling paper. Its strength lies in the use of a unique, large-scale dataset combined with a credible IV strategy to move from correlation to causation. The central finding—that the Matthew effect is not a universal phenomenon but is highly subject-dependent—is a significant contribution to the literature on educational inequality."
      ],
      "metadata": {
        "id": "N9f0muG4ZBnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "OPuOwQU6yihr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Replication Pipeline for \"Parental environment and student achievement:\n",
        "#  Does a Matthew effect exist?\"\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Parental environment and student\n",
        "#  achievement: Does a Matthew effect exist?\" by Aymeric, Lavaine, and\n",
        "#  Magdalou (2025). It delivers a fully automated and reproducible system for\n",
        "#  investigating the causal impact of parental environment on student\n",
        "#  achievement, with a focus on identifying subject-specific \"Matthew Effects.\"\n",
        "#\n",
        "#  The pipeline is designed to enable a transition from uniform educational\n",
        "#  policies to dynamic, evidence-based strategies by identifying the precise\n",
        "#  age groups and subjects where parental background has the strongest causal\n",
        "#  influence.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • High-dimensional fixed effects (HDFE) OLS models for associational analysis.\n",
        "#  • Two-Stage Least Squares (2SLS) with HDFE for causal inference, using a\n",
        "#    historical gender gap in education as an instrumental variable.\n",
        "#  • Estimation of both persistent (average) and dynamic (Matthew Effect)\n",
        "#    impacts of parental education.\n",
        "#  • School-level cluster-robust standard errors for valid statistical inference.\n",
        "#  • Systematic analysis across three subjects (Mathematics, Literature, English)\n",
        "#    and three grade levels (3, 6, 10).\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A modular, multi-stage pipeline covering data validation, cleaning, feature\n",
        "#    engineering, model specification, estimation, visualization, and reporting.\n",
        "#  • Use of the `linearmodels` library for efficient estimation of panel data\n",
        "#    models with absorbed fixed effects.\n",
        "#  • Programmatic generation of all analysis samples, model specifications,\n",
        "#    result tables, and figures to ensure perfect reproducibility.\n",
        "#  • A comprehensive suite of diagnostic and robustness checks, including\n",
        "#    sensitivity to control variable inclusion and instrument validity tests.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Aymeric, G., Lavaine, E., & Magdalou, B. (2025). Parental environment and\n",
        "#  student achievement: Does a Matthew effect exist?. arXiv preprint\n",
        "#  arXiv:2510.18481v1.\n",
        "#  https://arxiv.org/abs/2510.18481v1\n",
        "#\n",
        "#  Author: [Your Name/Organization]\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Core Libraries ---\n",
        "# Used for data manipulation and numerical operations.\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# --- Third-Party Libraries ---\n",
        "# Primary library for data analysis and manipulation.\n",
        "import pandas as pd\n",
        "\n",
        "# Primary library for numerical operations, especially array manipulation.\n",
        "import numpy as np\n",
        "\n",
        "# Primary library for creating static, animated, and interactive visualizations.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Primary library for advanced econometric modeling, especially panel data.\n",
        "from linearmodels.panel import PanelOLS, IV2SLS\n",
        "from linearmodels.panel.results import PanelEffectsResults, IVPanelEffectsResults\n",
        "\n",
        "# --- Typing ---\n",
        "# Used for providing detailed type hints for function signatures and variables,\n",
        "# enhancing code clarity, and enabling static analysis.\n",
        "from typing import (\n",
        "    Any,\n",
        "    Dict,\n",
        "    List,\n",
        "    Optional,\n",
        "    Set,\n",
        "    Tuple,\n",
        "    Union\n",
        ")\n"
      ],
      "metadata": {
        "id": "mabaHDTlym0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "l3vCaKcbyoQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional Specification of Pipeline Components**\n",
        "\n",
        "#### **Task 1: `validate_configuration`**\n",
        "\n",
        "*   **Inputs:** `config: Dict[str, Any]`.\n",
        "*   **Process:** Performs a multi-step validation of the `config` dictionary.\n",
        "    1.  **Structural Validation (`_validate_config_structure`):** Verifies that all 15 required top-level keys exist and that their corresponding values are dictionaries. This confirms the basic hierarchical structure.\n",
        "    2.  **Scope Validation (`_validate_scope_and_period`):** Extracts and asserts the exact values of critical study parameters (e.g., `start_year == 2016`, `grades_of_interest == [3, 6, 10]`). This locks the analysis to the specific scope of the replication.\n",
        "    3.  **Model Parameter Validation (`_validate_variables_and_models`):** Extracts and asserts the exact values of parameters that define the econometric models (e.g., `endogenous_variable == 'PHE'`, `fixed_effects == ['school_id', 'academic_year']`).\n",
        "*   **Outputs:** `bool` (`True` on success). Raises `ConfigurationError` on failure.\n",
        "*   **Research Role:** This callable is the **Protocol Enforcement Layer**. It acts as a non-negotiable guardrail, ensuring that the entire pipeline is executed with a set of parameters that are a perfect match for the methodology described in the paper. It prevents any deviation, intentional or accidental, from the replication protocol.\n",
        "\n",
        "#### **Task 2: `validate_student_survey_data`**\n",
        "\n",
        "*   **Inputs:** `student_parent_survey_raw: pd.DataFrame`, `config: Dict[str, Any]`.\n",
        "*   **Process:** Performs a three-step validation of the primary input data.\n",
        "    1.  **Schema Validation (`_validate_dataframe_schema`):** Compares the column names and data types of the input DataFrame against a canonical, hard-coded schema of 26 required columns. It reports missing columns, extra columns, and any dtype mismatches.\n",
        "    2.  **Content Integrity Validation (`_validate_content_integrity`):** For 16 specific categorical columns, it checks that all non-null values fall within their predefined valid domains (e.g., `gender` must be in `{1, 2}`). It also enforces the structural rule that no record can have `grade == 10` and `academic_year == 2016`.\n",
        "    3.  **Normalization Validation (`_validate_score_normalization`):** For each of the 3 score columns, it groups the data by `(grade, academic_year)` and calculates the mean and standard deviation for each cell. It asserts that the mean is within `500 ± 3` and the standard deviation is within `100 ± 3`.\n",
        "*   **Outputs:** `bool` (`True` on success). Raises `SchemaValidationError` or `DataIntegrityError` on failure.\n",
        "*   **Research Role:** This callable is the **Data Quality Assurance Layer**. It empirically verifies the claims made in the \"Data\" section of the paper, ensuring that the raw data is structurally sound, internally consistent, and has been correctly pre-processed (specifically, the IRT score normalization) before the replication pipeline begins.\n",
        "\n",
        "#### **Task 3: `validate_barro_lee_data`**\n",
        "\n",
        "*   **Inputs:** `barro_lee_raw: pd.DataFrame`, `student_parent_survey_raw: pd.DataFrame`, `config: Dict[str, Any]`.\n",
        "*   **Process:** Performs a three-step validation of the external instrument data.\n",
        "    1.  **Schema Validation (`_validate_barro_lee_schema`):** Checks the column names and dtypes of the `barro_lee_raw` DataFrame against its 7 required columns.\n",
        "    2.  **Content Integrity Validation (`_validate_barro_lee_content`):** Verifies that `reference_year` is always 1960, that `share_...` columns are valid proportions in [0,1], and that `country_cob` strings are valid 3-character uppercase ISO codes.\n",
        "    3.  **Coverage Validation (`_validate_instrument_coverage`):** Extracts the unique set of parent country codes from the student data and the unique set of available country codes from the Barro-Lee data. It computes the set difference to identify any required countries for which no instrument data is available and checks that the `config` specifies a valid policy (`\"drop\"`) for these cases.\n",
        "*   **Outputs:** `bool` (`True` on success). Raises `SchemaValidationError` or `DataIntegrityError` on failure.\n",
        "*   **Research Role:** This callable is the **Instrument Feasibility Layer**. It validates the external data source for the instrumental variable and, critically, performs a pre-analysis to determine the extent to which the instrument can be matched to the analysis sample. This provides an early diagnostic on the potential sample size and external validity of the causal analysis.\n",
        "\n",
        "#### **Task 4: `cleanse_student_survey_data`**\n",
        "\n",
        "*   **Inputs:** `student_parent_survey_raw: pd.DataFrame`, `config: Dict[str, Any]`.\n",
        "*   **Process:** Transforms the raw data into the final analytical population by applying a sequence of filters.\n",
        "    1.  **Inclusion Criteria (`_apply_inclusion_criteria`):** Filters the DataFrame to keep only rows where `parent_response_complete == True` and `grade` is in `{3, 6, 10}`.\n",
        "    2.  **Invalid Record Removal (`_remove_invalid_records`):** Applies study-specific exclusions (e.g., drops rows where `grade == 10` and `academic_year == 2016`). It then performs sequential checks to remove any rows with missing values in the critical identifiers `student_id`, `school_id`, or `academic_year`.\n",
        "    3.  **Deduplication (`_resolve_duplicates`):** Identifies all records that are duplicates on the key `(student_id, academic_year, grade)`. For each duplicate set, it deterministically selects the single \"best\" record to keep by sorting on data completeness and original index, then drops all others.\n",
        "*   **Outputs:** `Tuple[pd.DataFrame, Dict[str, Any]]` (the cleansed DataFrame, a detailed attrition log).\n",
        "*   **Research Role:** This callable constructs the **Analytical Population**. It is the precise, reproducible implementation of the sample selection process that defines the universe of observations for the study. The attrition log it produces is the basis for the sample flow table in the paper's appendix.\n",
        "\n",
        "#### **Task 5: `harmonize_and_standardize_data`**\n",
        "\n",
        "*   **Inputs:** The cleansed DataFrame from Task 4, `config: Dict[str, Any]`.\n",
        "*   **Process:** Performs in-place data cleaning and metadata extraction.\n",
        "    1.  **Country Code Standardization (`_standardize_country_codes`):** For the three `..._cob` columns, it applies `.str.upper().str.strip()` and validates the 3-character alphabetic format, nullifying any invalid codes.\n",
        "    2.  **Numeric Validation (`_validate_numeric_categoricals`):** For 12 specific numeric categorical columns, it enforces domain integrity, either by clipping values to a valid range (for `days_homework_raw`) or by nullifying out-of-domain values (for all others).\n",
        "    3.  **Metadata Extraction (`_document_reference_categories`):** It parses the `config` dictionary to create a new dictionary that explicitly maps each categorical variable to its reference category for modeling.\n",
        "*   **Outputs:** `Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any]]` (the harmonized DataFrame, a cleaning log, the reference category metadata).\n",
        "*   **Research Role:** This callable is the **Data Standardization Layer**. It ensures that all categorical variables are clean and consistently formatted, preventing errors in merges and dummy variable creation. The reference category dictionary it creates is a critical piece of metadata that guarantees all subsequent models are specified with the same baseline assumptions.\n",
        "\n",
        "#### **Task 6: `construct_parental_education_variable`**\n",
        "\n",
        "*   **Inputs:** The harmonized DataFrame from Task 5, `config: Dict[str, Any]`.\n",
        "*   **Process:** Engineers the primary independent variable, `PHE`.\n",
        "    1.  **Max Education (`_compute_phe_raw`):** Implements $PHE\\_raw_i = \\max(\\text{parent\\_edu\\_mother}_i, \\text{parent\\_edu\\_father}_i)$ using vectorized operations.\n",
        "    2.  **Binning (`_bin_phe_raw_to_phe`):** Maps the `PHE_raw` value (0-8) to the three-level `PHE` variable (0, 1, 2) based on the ISCED groupings defined in the `config`.\n",
        "    3.  **Dummification (`_create_phe_dummies`):** Creates the two binary indicator columns, `p1` and `p2`, for use as regressors in OLS models, correctly omitting the reference category `PHE=0`.\n",
        "*   **Outputs:** `Tuple[pd.DataFrame, Dict[str, Any]]` (the DataFrame augmented with the four new PHE-related columns, and a log).\n",
        "*   **Research Role:** This callable operationalizes the core concept of \"Parental environment\" as defined by the authors. It transforms the raw parental education data into the specific endogenous variable (`PHE`) and regressors (`p1`, `p2`) whose effects are the central subject of the paper.\n",
        "\n",
        "#### **Task 7: `construct_control_variables`**\n",
        "\n",
        "*   **Inputs:** The DataFrame from Task 6, the reference category metadata from Task 5.\n",
        "*   **Process:** This function is a large-scale feature engineering step. It systematically iterates through all remaining categorical control variables (student effort, parental investment, household resources, gender, etc.). For each, it calls a generic helper (`_create_dummies_from_categorical`) that creates a set of dummy variables, correctly omitting the reference category specified in the metadata dictionary.\n",
        "*   **Outputs:** `Tuple[pd.DataFrame, List[str]]` (the fully-featured DataFrame, a list of all newly created dummy variable names).\n",
        "*   **Research Role:** This callable constructs the full set of control variables (represented as $s_{ki}$ and $h_{ki}$ in the paper's equations). This is the implementation of the \"ceteris paribus\" condition, ensuring that the estimated effect of `PHE` is not confounded by a wide range of observable student and household characteristics.\n",
        "\n",
        "#### **Task 8: `construct_instrumental_variable`**\n",
        "\n",
        "*   **Inputs:** The DataFrame from Task 7, the validated `barro_lee_raw` DataFrame, `config: Dict[str, Any]`.\n",
        "*   **Process:** Constructs the instrumental variable, `GG`.\n",
        "    1.  **Parent Selection (`_select_instrument_country_of_birth`):** Implements the hierarchical logic to select the country of birth of the more-educated parent for each student.\n",
        "    2.  **Merge (`_merge_with_instrument_data`):** Performs a `left` merge, joining the student data with the Barro-Lee data on the selected country code.\n",
        "    3.  **Computation (`_compute_gender_gap_instrument`):** Implements the formula $GG_i = \\text{share\\_tertiary\\_women\\_1960}_i - \\text{share\\_tertiary\\_men\\_1960}_i$ on the merged data.\n",
        "*   **Outputs:** `Tuple[pd.DataFrame, Dict[str, Any]]` (the final DataFrame now including the `GG` column, and a log).\n",
        "*   **Research Role:** This callable implements the \"Identification strategy\" of the paper. It creates the instrumental variable that is the key to moving from correlation to causation by addressing the endogeneity of parental education.\n",
        "\n",
        "#### **Task 9: `define_analysis_samples`**\n",
        "\n",
        "*   **Inputs:** The final DataFrame from Task 8, the list of control variables from Task 7, `config: Dict[str, Any]`.\n",
        "*   **Process:** This function is a data management workhorse. It programmatically generates all 33 distinct analysis samples required by the study. For each model specification (OLS, IV, pooled, interaction, etc.), it assembles the full list of required variables and then applies listwise deletion (`.dropna()`) to the master DataFrame to create a complete-case sample for that specific model.\n",
        "*   **Outputs:** `Tuple[Dict[str, Any], Dict[str, Any]]` (a nested dictionary containing all 33 sample DataFrames, and a parallel dictionary logging their final sizes).\n",
        "*   **Research Role:** This callable enforces the **complete-case analysis** assumption for every regression. By creating these distinct samples, it ensures that each model is estimated on the maximum number of valid observations available for its specific set of variables.\n",
        "\n",
        "#### **Tasks 10 & 11: `specify_ols_models`, `specify_iv_models`**\n",
        "\n",
        "*   **Inputs:** The list of control variables, `config: Dict[str, Any]`.\n",
        "*   **Process:** These are pure metadata-generating functions. They do not process data. They translate the theoretical equations from the paper into machine-readable dictionaries that define every component of every model to be estimated (dependent variable, regressors, fixed effects, etc.).\n",
        "*   **Outputs:** Nested dictionaries of model specifications.\n",
        "*   **Research Role:** These callables are the **Architectural Blueprint** for the entire econometric analysis. They programmatically define every regression equation (Equations 1-5), ensuring that the models estimated in the subsequent tasks are exact, faithful replications of the paper's methodology.\n",
        "\n",
        "#### **Task 12: `estimate_model`**\n",
        "\n",
        "*   **Inputs:** An analysis sample DataFrame, a model specification dictionary.\n",
        "*   **Process:** This is the **Estimation Engine**. It takes a sample and a specification, prepares the data for panel analysis by setting a `MultiIndex`, and then instantiates and fits the appropriate `linearmodels` estimator (`PanelOLS` or `IV2SLS`). It correctly configures the estimator to absorb high-dimensional fixed effects and to compute cluster-robust standard errors at the school level.\n",
        "*   **Outputs:** A fitted model results object from `linearmodels`.\n",
        "*   **Research Role:** This callable is the computational heart of the project. It executes the complex fixed-effects regressions with clustered standard errors that are central to the paper's empirical strategy. It implements the cluster-robust variance estimator: $\\widehat{V}_{\\text{cluster}} = (X'X)^{-1} \\left( \\sum_{g=1}^{G} X_g' \\hat{u}_g \\hat{u}_g' X_g \\right) (X'X)^{-1}$.\n",
        "\n",
        "#### **Tasks 13-20 (Orchestrators): `run_..._estimation`**\n",
        "\n",
        "*   **Inputs:** The dictionaries of samples and specifications.\n",
        "*   **Process:** Each of these orchestrators manages a specific block of estimations. They are essentially loops that iterate through subjects, grades, or grade-pairs, and for each iteration, they call `estimate_model` and then call a helper (`_extract_result_summary`) to parse the results object into a clean table.\n",
        "*   **Outputs:** Dictionaries of formatted results tables (DataFrames).\n",
        "*   **Research Role:** These callables execute the full empirical analysis, producing the numerical results for the key coefficients of interest: the persistent effects ($\\alpha_j$, $\\alpha_1$) and the Matthew effects ($\\theta_j$, $\\alpha_2$). They generate the content for Tables 8-17 in the paper.\n",
        "\n",
        "#### **Tasks 21-23 (Orchestrators): `generate_..._plots`**\n",
        "\n",
        "*   **Inputs:** The final dataset and the results dictionaries.\n",
        "*   **Process:** These orchestrators manage the creation of all figures. They transform the data and results into a \"tidy\" format and then use `matplotlib` to generate and save the publication-quality CDF plots and coefficient plots.\n",
        "*   **Outputs:** A list of file paths to the saved image files.\n",
        "*   **Research Role:** These callables replicate the paper's key visual evidence (Figures 1-5, 8-10), providing intuitive visualizations of the main findings.\n",
        "\n",
        "#### **Task 24: `perform_diagnostic_checks`**\n",
        "\n",
        "*   **Inputs:** The cleansed DataFrame and various log files.\n",
        "*   **Process:** This function performs final sanity checks. It checks for composition effects, re-validates score normalization, and synthesizes logs into a sample attrition table.\n",
        "*   **Outputs:** A dictionary of diagnostic tables and warnings.\n",
        "*   **Research Role:** This callable provides the empirical evidence for methodological claims, such as the absence of significant composition effects, thereby bolstering the study's internal validity.\n",
        "\n",
        "#### **Task 25: `run_parental_environment_study_pipeline`**\n",
        "\n",
        "*   **Inputs:** The two raw DataFrames (`student_parent_survey_raw`, `barro_lee_raw`) and the `config` dictionary.\n",
        "*   **Process:** This function orchestrates the entire main analysis, from Task 1 through Task 24.\n",
        "*   **Outputs:** A tuple containing the main results dictionary and a dictionary of key intermediate artifacts.\n",
        "*   **Research Role:** This callable represents the complete, self-contained replication of the paper's primary analysis, packaged as a reusable component.\n",
        "\n",
        "#### **Task 26: `run_robustness_checks`**\n",
        "\n",
        "*   **Inputs:** The outputs from the main pipeline (Task 25).\n",
        "*   **Process:** This function executes sensitivity analyses. It re-estimates models with fewer controls, systematically reviews all IV diagnostics, and formally validates the developmental patterns.\n",
        "*   **Outputs:** A dictionary of robustness reports.\n",
        "*   **Research Role:** This callable is the **Internal Peer Review Layer**. It stress-tests the main findings, providing crucial evidence on their stability and credibility.\n",
        "\n",
        "#### **Task 27: `synthesize_final_report`**\n",
        "\n",
        "*   **Inputs:** The main pipeline outputs.\n",
        "*   **Process:** This function programmatically translates the final quantitative results into a human-readable, narrative summary of the findings and their policy implications.\n",
        "*   **Outputs:** A single formatted string containing the final report.\n",
        "*   **Research Role:** This callable replicates the \"Discussion, related literature, and policy implications\" section of the paper, translating the empirical evidence into a conclusive narrative.\n",
        "\n",
        "#### **Top-Level Orchestrator: `execute_full_study`**\n",
        "\n",
        "*   **Inputs:** The two raw DataFrames and the `config` dictionary.\n",
        "*   **Process:** This is the ultimate entry point. It calls the main pipeline orchestrator (Task 25), then passes the outputs to the robustness check orchestrator (Task 26), and finally calls the report synthesis orchestrator (Task 27).\n",
        "*   **Outputs:** A single, comprehensive dictionary containing all artifacts from the entire project.\n",
        "*   **Research Role:** This callable represents the entire research project, from raw data to final narrative, as a single, atomic, and perfectly reproducible function. It is the embodiment of computational social science at its most rigorous.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Examples**\n",
        "\n",
        "### High-Fidelity Example: Executing the End-to-End Pipeline\n",
        "\n",
        "This example demonstrates how to use the top-level orchestrator function, `execute_full_study`, to run the entire replication of Aymeric, Lavaine, and Magdalou (2025). It covers the three necessary inputs:\n",
        "1.  **`student_parent_survey_raw`**: A synthetic but realistic DataFrame representing the primary student and parent survey data.\n",
        "2.  **`barro_lee_raw`**: A synthetic DataFrame representing the historical data for the instrumental variable.\n",
        "3.  **`config`**: The master configuration dictionary, loaded from the `config.yaml` file.\n",
        "\n",
        "The example will proceed in three stages:\n",
        "1.  **Setup:** Import necessary libraries and create the `config.yaml` file.\n",
        "2.  **Data Generation:** Create high-fidelity synthetic versions of the two required DataFrames.\n",
        "3.  **Execution:** Load the configuration, call the `execute_full_study` function, and briefly inspect the outputs.\n",
        "\n",
        "#### **Stage 1: Setup**\n",
        "\n",
        "First, we ensure all necessary libraries are imported. We will use `pyyaml` to load the configuration file and `faker` to help generate realistic synthetic data.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries for data generation and file handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "# Initialize the Faker generator for creating synthetic data\n",
        "fake = Faker()\n",
        "```\n",
        "\n",
        "#### **Stage 2: Synthetic Data Generation**\n",
        "\n",
        "Now, we will create realistic, synthetic versions of the two input DataFrames. This is a critical step for testing and demonstrating the pipeline without access to the original sensitive microdata. The data generation process is designed to mimic the structure and properties of the real data as described in the paper and the data dictionary.\n",
        "\n",
        "##### **Input 1: `barro_lee_raw` DataFrame**\n",
        "\n",
        "This DataFrame contains the historical data for our instrument. We will create a sample of countries with realistic 1960 tertiary education shares.\n",
        "\n",
        "```python\n",
        "# Define a list of sample countries with their ISO alpha-3 codes.\n",
        "# This list includes a mix of developed and developing countries for realism.\n",
        "country_list = [\n",
        "    'USA', 'GBR', 'FRA', 'DEU', 'JPN', 'CAN', 'AUS', # Developed\n",
        "    'ESP', 'ITA', 'PRT', 'GRC', # Southern Europe\n",
        "    'BRA', 'ARG', 'COL', 'MEX', 'PER', # Latin America\n",
        "    'NGA', 'ZAF', 'EGY', 'KEN', # Africa\n",
        "    'CHN', 'IND', 'IDN', 'PAK', # Asia\n",
        "    'CSK', 'SUN' # Legacy entities for testing\n",
        "]\n",
        "\n",
        "# Generate the synthetic Barro-Lee data.\n",
        "barro_lee_data = []\n",
        "for country in country_list:\n",
        "    # Simulate lower female education rates, typical for 1960.\n",
        "    share_men = random.uniform(0.01, 0.15)\n",
        "    # Women's share is a fraction of men's, plus some noise.\n",
        "    share_women = max(0, share_men * random.uniform(0.2, 0.9) - random.uniform(0, 0.02))\n",
        "    \n",
        "    is_legacy = country in ['CSK', 'SUN']\n",
        "    notes = \"Assignment rule: Successor states mapped here.\" if is_legacy else None\n",
        "    \n",
        "    barro_lee_data.append({\n",
        "        'country_cob': country,\n",
        "        'country_name_1960': fake.country() if not is_legacy else {'CSK': 'Czechoslovakia', 'SUN': 'USSR'}[country],\n",
        "        'is_legacy_entity': is_legacy,\n",
        "        'share_tertiary_women_1960': share_women,\n",
        "        'share_tertiary_men_1960': share_men,\n",
        "        'reference_year': 1960,\n",
        "        'notes': notes\n",
        "    })\n",
        "\n",
        "# Create the DataFrame.\n",
        "barro_lee_raw = pd.DataFrame(barro_lee_data)\n",
        "\n",
        "print(\"Synthetic 'barro_lee_raw' DataFrame created:\")\n",
        "print(barro_lee_raw.head())\n",
        "```\n",
        "\n",
        "##### **Input 2: `student_parent_survey_raw` DataFrame**\n",
        "\n",
        "This is the main dataset. We will generate a sample of 10,000 observations, ensuring the data structure and distributions are plausible.\n",
        "\n",
        "```python\n",
        "# Set parameters for data generation.\n",
        "N_STUDENTS = 10000\n",
        "N_SCHOOLS = 200\n",
        "YEARS = [2016, 2017, 2018, 2019]\n",
        "GRADES = [3, 6, 10]\n",
        "\n",
        "# Generate the synthetic student and parent survey data.\n",
        "survey_data = []\n",
        "for i in range(N_STUDENTS):\n",
        "    # Assign basic identifiers.\n",
        "    student_id = 10000 + i\n",
        "    school_id = random.randint(1, N_SCHOOLS)\n",
        "    academic_year = random.choice(YEARS)\n",
        "    grade = random.choice(GRADES)\n",
        "    \n",
        "    # Enforce the structural exclusion rule from the study design.\n",
        "    if academic_year == 2016 and grade == 10:\n",
        "        continue # Skip this invalid combination.\n",
        "\n",
        "    # Simulate parental education with a realistic distribution (more higher-ed).\n",
        "    parent_edu_mother = random.choices(range(9), weights=[2, 2, 5, 10, 15, 25, 20, 15, 6])[0]\n",
        "    # Assortative mating: father's education is correlated with mother's.\n",
        "    parent_edu_father = max(0, min(8, parent_edu_mother + random.randint(-2, 2)))\n",
        "    \n",
        "    # Higher parental education correlates with better outcomes.\n",
        "    base_ability = (parent_edu_mother + parent_edu_father) / 16.0 # Normalize to [0, 1]\n",
        "    \n",
        "    # Generate scores around the mean of 500, influenced by base_ability.\n",
        "    # The multiplier (e.g., 150) controls the strength of the parental effect.\n",
        "    mu = 500 + 150 * (base_ability - 0.5)\n",
        "    score_math = np.random.normal(mu, 100)\n",
        "    score_lit = np.random.normal(mu, 100)\n",
        "    score_eng = np.random.normal(mu + 20, 100) # Slightly higher mean for English\n",
        "\n",
        "    # Simulate homework days.\n",
        "    days_hw_raw = random.randint(0, 7)\n",
        "    if days_hw_raw <= 1: days_hw_cat = 1\n",
        "    elif days_hw_raw <= 3: days_hw_cat = 2\n",
        "    elif days_hw_raw <= 5: days_hw_cat = 3\n",
        "    else: days_hw_cat = 4\n",
        "    \n",
        "    survey_data.append({\n",
        "        'student_id': student_id,\n",
        "        'school_id': school_id,\n",
        "        'academic_year': academic_year,\n",
        "        'grade': grade,\n",
        "        'parent_response_complete': random.choices([True, False], weights=[0.9, 0.1])[0],\n",
        "        'gender': random.randint(1, 2),\n",
        "        'student_cob': random.choices(['ESP', 'COL', 'MAR', 'ROU'], weights=[85, 5, 5, 5])[0],\n",
        "        'Score_Math': score_math,\n",
        "        'Score_Lit': score_lit,\n",
        "        'Score_Eng': score_eng,\n",
        "        'days_homework_raw': days_hw_raw,\n",
        "        'days_homework_cat': days_hw_cat,\n",
        "        'parent_edu_mother': parent_edu_mother,\n",
        "        'parent_edu_father': parent_edu_father,\n",
        "        'parent_cob_mother': random.choices(['ESP', 'COL', 'ECU', 'USA'], weights=[80, 8, 7, 5])[0],\n",
        "        'parent_cob_father': random.choices(['ESP', 'COL', 'ECU', 'USA'], weights=[80, 8, 7, 5])[0],\n",
        "        'employment_status_mother': random.randint(0, 3),\n",
        "        'employment_status_father': random.randint(0, 3),\n",
        "        'parent_talk_school': random.randint(1, 4),\n",
        "        'parent_teach_homework': random.randint(1, 4),\n",
        "        'parent_help_homework': random.randint(1, 4),\n",
        "        'parent_check_homework': random.randint(1, 4),\n",
        "        'books_at_home': random.randint(1, 5),\n",
        "        'freq_use_books_home': random.randint(1, 4),\n",
        "        'freq_use_computer_home': random.randint(1, 4),\n",
        "        'freq_use_internet_home': random.randint(1, 4),\n",
        "    })\n",
        "\n",
        "# Create the DataFrame.\n",
        "student_parent_survey_raw = pd.DataFrame(survey_data)\n",
        "\n",
        "# Convert types to match the exact specification.\n",
        "student_parent_survey_raw = student_parent_survey_raw.astype({\n",
        "    'student_id': 'int64', 'school_id': 'int64', 'academic_year': 'int64',\n",
        "    'grade': 'int64', 'parent_response_complete': 'bool', 'gender': 'int64',\n",
        "    'days_homework_raw': 'float64', 'days_homework_cat': 'int64',\n",
        "    'parent_edu_mother': 'int64', 'parent_edu_father': 'int64',\n",
        "    'employment_status_mother': 'int64', 'employment_status_father': 'int64',\n",
        "    'parent_talk_school': 'int64', 'parent_teach_homework': 'int64',\n",
        "    'parent_help_homework': 'int64', 'parent_check_homework': 'int64',\n",
        "    'books_at_home': 'int64', 'freq_use_books_home': 'int64',\n",
        "    'freq_use_computer_home': 'int64', 'freq_use_internet_home': 'int64'\n",
        "})\n",
        "\n",
        "\n",
        "print(\"\\nSynthetic 'student_parent_survey_raw' DataFrame created:\")\n",
        "print(student_parent_survey_raw.head())\n",
        "print(f\"\\nTotal rows generated: {len(student_parent_survey_raw)}\")\n",
        "```\n",
        "\n",
        "#### **Stage 3: Pipeline Execution and Output Inspection**\n",
        "\n",
        "With the configuration file on disk and the two synthetic DataFrames in memory, we can now execute the entire pipeline with a single function call.\n",
        "\n",
        "```python\n",
        "# Load the configuration from the YAML file.\n",
        "try:\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(\"\\nConfiguration loaded successfully from config.yaml.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nERROR: config.yaml not found. Please run Stage 1 first.\")\n",
        "    config = None\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Failed to load or parse config.yaml: {e}\")\n",
        "    config = None\n",
        "\n",
        "# Check if config was loaded before proceeding.\n",
        "if config:\n",
        "    # Execute the entire study pipeline using the top-level orchestrator.\n",
        "    # All results, diagnostics, and figures will be generated.\n",
        "    # The 'output_dir' will be created in the current directory.\n",
        "    full_results = execute_full_study(\n",
        "        student_parent_survey_raw=student_parent_survey_raw,\n",
        "        barro_lee_raw=barro_lee_raw,\n",
        "        config=config,\n",
        "        output_dir=\"study_replication_output\"\n",
        "    )\n",
        "\n",
        "    # --- Inspect the Outputs ---\n",
        "    # The 'full_results' object is a deeply nested dictionary containing everything.\n",
        "    # We can now inspect any part of the analysis.\n",
        "\n",
        "    print(\"\\n--- Example Output Inspection ---\")\n",
        "    \n",
        "    # 1. Inspect the final summary report.\n",
        "    print(\"\\nFinal Summary Report:\")\n",
        "    print(full_results['final_summary_report'])\n",
        "\n",
        "    # 2. Inspect a specific result table: Pooled IV for Mathematics.\n",
        "    print(\"\\nExample Result Table: Pooled IV for Score_Math\")\n",
        "    pooled_iv_math_results = full_results['main_analysis']['results']['pooled_iv']['Score_Math']\n",
        "    print(pooled_iv_math_results)\n",
        "\n",
        "    # 3. Inspect a diagnostic table: Sample inclusion flow.\n",
        "    print(\"\\nDiagnostic Table: Sample Inclusion Flow\")\n",
        "    sample_flow = full_results['main_analysis']['diagnostics']['sample_flow_table']\n",
        "    print(sample_flow)\n",
        "    \n",
        "    # 4. Inspect a robustness check: OLS control sensitivity.\n",
        "    print(\"\\nRobustness Check: OLS Control Sensitivity\")\n",
        "    sensitivity_check = full_results['robustness_checks']['ols_control_sensitivity']\n",
        "    print(sensitivity_check)\n",
        "    \n",
        "    # 5. List the generated figures.\n",
        "    print(\"\\nGenerated Figure Paths:\")\n",
        "    for fig_type, paths in full_results['main_analysis']['figures'].items():\n",
        "        print(f\"  - {fig_type}:\")\n",
        "        for path in paths:\n",
        "            print(f\"    - {path}\")\n",
        "```\n",
        "\n",
        "This concludes the example. It demonstrates a complete, professional workflow: generating realistic test data, executing a complex, multi-stage analytical pipeline with a single command, and finally, programmatically accessing the structured outputs for review.\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "sQc2wABqyU1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate configuration dictionary structure and parameter values\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate configuration dictionary structure and parameter values\n",
        "# ==============================================================================\n",
        "\n",
        "class ConfigurationError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised for errors during configuration validation.\n",
        "\n",
        "    This exception is specifically used to signal that the main configuration\n",
        "    dictionary, which governs the entire analysis pipeline, is malformed or\n",
        "    contains invalid parameters. It is raised when issues such as missing keys,\n",
        "    incorrect data types for values, or parameter values that violate the\n",
        "    study's replication standards are detected.\n",
        "\n",
        "    The primary purpose of this custom exception is to provide clear,\n",
        "    actionable error messages that pinpoint the exact location and nature of\n",
        "    the configuration problem, facilitating rapid debugging and correction.\n",
        "\n",
        "    Inherits:\n",
        "        ValueError: This class inherits from Python's built-in ValueError,\n",
        "                    as configuration problems represent a specific case of\n",
        "                    providing an argument (the config dictionary) with an\n",
        "                    inappropriate or invalid value.\n",
        "    \"\"\"\n",
        "    # The __init__ method is inherited from the base ValueError class.\n",
        "    # It is designed to be instantiated with a descriptive string message\n",
        "    # that details the specific configuration error encountered.\n",
        "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
        "        # This call initializes the base ValueError class with the provided\n",
        "        # error message and any other arguments.\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Validate the presence and type of top-level keys.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_config_structure(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the structural integrity of the configuration dictionary.\n",
        "\n",
        "    This function checks for the presence of all required top-level keys and\n",
        "    verifies that their corresponding values are dictionaries, ensuring the\n",
        "    expected nested structure for the pipeline's parameters.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary for the study.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If a required top-level key is missing or if its\n",
        "                            value is not a dictionary.\n",
        "    \"\"\"\n",
        "    # Define the set of all mandatory top-level keys for the configuration.\n",
        "    required_top_level_keys: List[str] = [\n",
        "        'data_sources', 'study_period', 'population_scope',\n",
        "        'variable_definitions', 'psychometric_model',\n",
        "        'parental_education_coding', 'instrument_construction',\n",
        "        'effort_coding', 'parental_investment_coding',\n",
        "        'books_at_home_coding', 'model_specifications',\n",
        "        'inference_parameters', 'iv_validation', 'sample_policies',\n",
        "        'reproducibility'\n",
        "    ]\n",
        "\n",
        "    # Iterate through each required key to ensure its presence and correct type.\n",
        "    for key in required_top_level_keys:\n",
        "        # Check if the key exists in the configuration dictionary.\n",
        "        if key not in config:\n",
        "            # If a key is missing, raise a specific error.\n",
        "            raise ConfigurationError(f\"Missing required top-level key in configuration: '{key}'\")\n",
        "\n",
        "        # Check if the value associated with the key is a dictionary.\n",
        "        if not isinstance(config[key], dict):\n",
        "            # If the type is incorrect, raise a specific error.\n",
        "            raise ConfigurationError(\n",
        "                f\"Key '{key}' in configuration must be a dictionary, but found type \"\n",
        "                f\"'{type(config[key]).__name__}'.\"\n",
        "            )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate study scope and period constraints.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_scope_and_period(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates critical parameters related to the study's scope and time period.\n",
        "\n",
        "    This function ensures that the configuration aligns with the specific\n",
        "    constraints of the replication study, such as the academic years, grades\n",
        "    of interest, and sample inclusion criteria.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary for the study.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If any scope or period parameter does not match\n",
        "                            the required value for the study replication.\n",
        "    \"\"\"\n",
        "    # A dictionary mapping the expected path, value, and a descriptive error message.\n",
        "    validations = {\n",
        "        ('study_period', 'start_year'): 2016,\n",
        "        ('study_period', 'end_year'): 2019,\n",
        "        ('study_period', 'grade10_assessed_in_2016'): False,\n",
        "        ('population_scope', 'grades_of_interest'): [3, 6, 10],\n",
        "        ('population_scope', 'require_parent_response'): True,\n",
        "    }\n",
        "\n",
        "    # Iterate through the validation rules.\n",
        "    for path, expected_value in validations.items():\n",
        "        try:\n",
        "            # Traverse the dictionary to get the actual value.\n",
        "            actual_value = config\n",
        "            for key in path:\n",
        "                actual_value = actual_value[key]\n",
        "\n",
        "            # Compare the actual value with the expected value.\n",
        "            if actual_value != expected_value:\n",
        "                # If they don't match, raise a detailed error.\n",
        "                raise ConfigurationError(\n",
        "                    f\"Configuration validation failed for path '{' -> '.join(path)}'. \"\n",
        "                    f\"Expected: {expected_value} (type: {type(expected_value).__name__}), \"\n",
        "                    f\"Found: {actual_value} (type: {type(actual_value).__name__}).\"\n",
        "                )\n",
        "        except KeyError:\n",
        "            # If any key in the path is missing, raise a detailed error.\n",
        "            raise ConfigurationError(f\"Missing required key path in configuration: {path}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate variable definitions and model specifications.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_variables_and_models(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates parameters defining variables and econometric models.\n",
        "\n",
        "    This function checks key definitions for dependent/endogenous variables,\n",
        "    fixed effects, interaction terms, and inference parameters to ensure\n",
        "    that the subsequent modeling steps are correctly specified.\n",
        "\n",
        "    Args:\n",
        "        config: The configuration dictionary for the study.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If any variable or model parameter does not match\n",
        "                            the required value for the study replication.\n",
        "    \"\"\"\n",
        "    # A dictionary mapping the expected path, value, and a descriptive error message.\n",
        "    # Using a list of tuples to handle potential float comparisons.\n",
        "    validations: List[Tuple[Tuple[str, ...], Any]] = [\n",
        "        (('variable_definitions', 'dependent_variables'), ['Score_Math', 'Score_Lit', 'Score_Eng']),\n",
        "        (('variable_definitions', 'endogenous_variable'), 'PHE'),\n",
        "        (('variable_definitions', 'instrumental_variable'), 'GG'),\n",
        "        (('model_specifications', 'fixed_effects'), ['school_id', 'academic_year']),\n",
        "        (('model_specifications', 'grade_pair_definitions'), [(3, 6), (6, 10), (3, 10)]),\n",
        "        (('inference_parameters', 'standard_error_clustering_level'), 'school_id'),\n",
        "        (('iv_validation', 'weak_instrument_threshold_f_stat'), 10.0),\n",
        "    ]\n",
        "\n",
        "    # Iterate through the validation rules.\n",
        "    for path, expected_value in validations:\n",
        "        try:\n",
        "            # Traverse the dictionary to get the actual value.\n",
        "            actual_value = config\n",
        "            for key in path:\n",
        "                actual_value = actual_value[key]\n",
        "\n",
        "            # Check for type and value equality, with special handling for floats.\n",
        "            is_equal = False\n",
        "            if isinstance(expected_value, float):\n",
        "                # Use math.isclose for robust floating-point comparison.\n",
        "                is_equal = isinstance(actual_value, (int, float)) and math.isclose(actual_value, expected_value)\n",
        "            else:\n",
        "                # Use direct comparison for other types (lists, strings, etc.).\n",
        "                is_equal = actual_value == expected_value\n",
        "\n",
        "            # If the values are not equal, raise a detailed error.\n",
        "            if not is_equal:\n",
        "                raise ConfigurationError(\n",
        "                    f\"Configuration validation failed for path '{' -> '.join(path)}'. \"\n",
        "                    f\"Expected: {expected_value} (type: {type(expected_value).__name__}), \"\n",
        "                    f\"Found: {actual_value} (type: {type(actual_value).__name__}).\"\n",
        "                )\n",
        "        except KeyError:\n",
        "            # If any key in the path is missing, raise a detailed error.\n",
        "            raise ConfigurationError(f\"Missing required key path in configuration: {path}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_configuration(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire study configuration dictionary.\n",
        "\n",
        "    This function serves as the main entry point for configuration validation.\n",
        "    It sequentially calls specialized validation functions to check the\n",
        "    structure, scope, period, variable definitions, and model specifications.\n",
        "    A successful execution of this function guarantees that the configuration\n",
        "    is valid and sufficient for running the replication pipeline.\n",
        "\n",
        "    Args:\n",
        "        config: The complete configuration dictionary for the study.\n",
        "\n",
        "    Returns:\n",
        "        True if the configuration is valid in all aspects.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If any part of the configuration is invalid,\n",
        "                            with a specific message indicating the failure.\n",
        "        TypeError: If the provided config is not a dictionary.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the provided config object is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"Configuration must be a dictionary, but got {type(config).__name__}.\")\n",
        "\n",
        "    # --- Step 1: Validate top-level structure ---\n",
        "    # This ensures all primary sections of the config are present.\n",
        "    _validate_config_structure(config=config)\n",
        "\n",
        "    # --- Step 2: Validate study scope and period constraints ---\n",
        "    # This confirms the study parameters match the replication target.\n",
        "    _validate_scope_and_period(config=config)\n",
        "\n",
        "    # --- Step 3: Validate variable definitions and model specifications ---\n",
        "    # This ensures the econometric models will be specified correctly.\n",
        "    _validate_variables_and_models(config=config)\n",
        "\n",
        "    # If all validations pass without raising an exception, return True.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "OsLw4359ytL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate student_parent_survey_raw DataFrame schema and content integrity\n",
        "\n",
        "# =================================================================================\n",
        "# Task 2: Validate student_parent_survey_raw DataFrame schema and content integrity\n",
        "# =================================================================================\n",
        "\n",
        "class SchemaValidationError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised for errors related to DataFrame schema violations.\n",
        "\n",
        "    This exception is specifically used to signal that a pandas DataFrame does\n",
        "    not conform to a predefined schema. It is typically raised when there are\n",
        "    discrepancies in column names, data types, or the presence/absence of\n",
        "    expected columns. Its error message is designed to be comprehensive,\n",
        "    detailing all identified schema issues to facilitate rapid debugging.\n",
        "\n",
        "    Inherits:\n",
        "        ValueError: The standard Python exception for inappropriate value types,\n",
        "                    making it a semantically appropriate base class for schema\n",
        "                    validation failures.\n",
        "    \"\"\"\n",
        "    # The __init__ method is inherited from the base ValueError class.\n",
        "    # It can be called with a string argument detailing the specific schema error.\n",
        "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
        "        # Initialize the base ValueError class with the provided arguments.\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class DataIntegrityError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised for errors in data content, integrity, or consistency.\n",
        "\n",
        "    This exception is used to indicate that while the data schema may be correct,\n",
        "    the values within the DataFrame violate predefined business rules, domain\n",
        "    constraints, or logical consistency checks. Examples include out-of-range\n",
        "    values, violation of structural rules (e.g., impossible data combinations),\n",
        "    or failure to meet normalization criteria.\n",
        "\n",
        "    Inherits:\n",
        "        ValueError: Chosen as the base class because data integrity failures\n",
        "                    represent cases where the data's content is invalid for\n",
        "                    the intended analytical purpose.\n",
        "    \"\"\"\n",
        "    # The __init__ method is inherited from the base ValueError class.\n",
        "    # It is intended to be instantiated with a detailed report of all\n",
        "    # data integrity violations found.\n",
        "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
        "        # Initialize the base ValueError class with the provided arguments.\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Verify presence, naming, and data types of all required columns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_dataframe_schema(\n",
        "    df: pd.DataFrame,\n",
        "    expected_schema: Dict[str, str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the schema of a DataFrame against an expected structure.\n",
        "\n",
        "    This function performs a rigorous check of column names and data types.\n",
        "    It identifies missing columns, extraneous columns, and columns with\n",
        "    incorrect data types, raising a detailed error if any discrepancies are found.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to validate.\n",
        "        expected_schema: A dictionary where keys are expected column names (str)\n",
        "                         and values are the expected dtype names (str).\n",
        "\n",
        "    Raises:\n",
        "        SchemaValidationError: If the DataFrame's schema does not match the\n",
        "                               expected schema.\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(expected_schema, dict):\n",
        "        raise TypeError(\"Input 'expected_schema' must be a dictionary.\")\n",
        "\n",
        "    # --- Schema Validation ---\n",
        "    # Get the actual column names from the DataFrame.\n",
        "    actual_columns: Set[str] = set(df.columns)\n",
        "    # Get the expected column names from the schema definition.\n",
        "    expected_columns: Set[str] = set(expected_schema.keys())\n",
        "\n",
        "    # Identify missing and extra columns using set operations for efficiency.\n",
        "    missing_columns: Set[str] = expected_columns - actual_columns\n",
        "    extra_columns: Set[str] = actual_columns - expected_columns\n",
        "\n",
        "    # Check for data type mismatches for columns that are present.\n",
        "    type_mismatches: Dict[str, Tuple[str, str]] = {}\n",
        "    for col_name, expected_type in expected_schema.items():\n",
        "        if col_name in actual_columns:\n",
        "            # Get the string name of the actual dtype.\n",
        "            actual_type: str = df[col_name].dtype.name\n",
        "            # Allow float for integer columns that might contain NaNs.\n",
        "            if 'int' in expected_type and 'float' in actual_type:\n",
        "                continue\n",
        "            if actual_type != expected_type:\n",
        "                type_mismatches[col_name] = (expected_type, actual_type)\n",
        "\n",
        "    # If any validation errors were found, compile a detailed error message.\n",
        "    if missing_columns or extra_columns or type_mismatches:\n",
        "        error_messages: List[str] = [\"DataFrame schema validation failed:\"]\n",
        "        if missing_columns:\n",
        "            error_messages.append(f\"  - Missing columns: {sorted(list(missing_columns))}\")\n",
        "        if extra_columns:\n",
        "            error_messages.append(f\"  - Extra columns found: {sorted(list(extra_columns))}\")\n",
        "        if type_mismatches:\n",
        "            mismatches_str = [f\"'{c}' (expected: {e}, found: {a})\" for c, (e, a) in type_mismatches.items()]\n",
        "            error_messages.append(f\"  - Dtype mismatches: {', '.join(mismatches_str)}\")\n",
        "\n",
        "        # Raise a single, comprehensive error with all findings.\n",
        "        raise SchemaValidationError(\"\\n\".join(error_messages))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate domain constraints and structural rules.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_content_integrity(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the content integrity of the student survey DataFrame.\n",
        "\n",
        "    This function checks for adherence to predefined domain constraints for\n",
        "    categorical variables and enforces critical structural rules, such as the\n",
        "    exclusion of Grade 10 data in the 2016 academic year. It collects all\n",
        "    violations before raising an error.\n",
        "\n",
        "    Args:\n",
        "        df: The schema-validated student survey DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        DataIntegrityError: If any content integrity checks fail, containing a\n",
        "                            comprehensive report of all violations.\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Content Validation ---\n",
        "    # Dictionary to store any validation failures.\n",
        "    violations: Dict[str, Any] = {}\n",
        "\n",
        "    # Define domain constraints for various columns.\n",
        "    domain_rules = {\n",
        "        'grade': {3, 6, 10},\n",
        "        'academic_year': {2016, 2017, 2018, 2019},\n",
        "        'gender': {1, 2},\n",
        "        'days_homework_cat': {1, 2, 3, 4},\n",
        "        'parent_edu_mother': set(range(9)), # {0, 1, ..., 8}\n",
        "        'parent_edu_father': set(range(9)),\n",
        "        'parent_talk_school': {1, 2, 3, 4},\n",
        "        'parent_teach_homework': {1, 2, 3, 4},\n",
        "        'parent_help_homework': {1, 2, 3, 4},\n",
        "        'parent_check_homework': {1, 2, 3, 4},\n",
        "        'books_at_home': {1, 2, 3, 4, 5},\n",
        "        'freq_use_books_home': {1, 2, 3, 4},\n",
        "        'freq_use_computer_home': {1, 2, 3, 4},\n",
        "        'freq_use_internet_home': {1, 2, 3, 4},\n",
        "        'employment_status_mother': {0, 1, 2, 3},\n",
        "        'employment_status_father': {0, 1, 2, 3},\n",
        "    }\n",
        "\n",
        "    # Check domain constraints for each specified column.\n",
        "    for col, valid_set in domain_rules.items():\n",
        "        # Isolate non-missing values for validation.\n",
        "        series_to_check = df[col].dropna()\n",
        "        # Find values that are not in the allowed set.\n",
        "        invalid_mask = ~series_to_check.isin(valid_set)\n",
        "        if invalid_mask.any():\n",
        "            # If violations exist, record them.\n",
        "            violating_values = series_to_check[invalid_mask].unique().tolist()\n",
        "            violations[col] = {\n",
        "                \"rule\": f\"Values must be in {valid_set}\",\n",
        "                \"count\": int(invalid_mask.sum()),\n",
        "                \"sample_invalid_values\": violating_values[:5]\n",
        "            }\n",
        "\n",
        "    # Check the structural rule: no Grade 10 data in 2016.\n",
        "    structural_violation_mask = (df['grade'] == 10) & (df['academic_year'] == 2016)\n",
        "    if structural_violation_mask.any():\n",
        "        # If violations exist, record them.\n",
        "        violations['structural_rule_grade10_2016'] = {\n",
        "            \"rule\": \"No records should have grade=10 and academic_year=2016\",\n",
        "            \"count\": int(structural_violation_mask.sum())\n",
        "        }\n",
        "\n",
        "    # If any violations were collected, raise a single, comprehensive error.\n",
        "    if violations:\n",
        "        error_message = \"Data content integrity validation failed:\"\n",
        "        for key, details in violations.items():\n",
        "            error_message += f\"\\n  - Check failed for '{key}':\"\n",
        "            error_message += f\"\\n    Rule: {details['rule']}\"\n",
        "            error_message += f\"\\n    Violation count: {details['count']}\"\n",
        "            if \"sample_invalid_values\" in details:\n",
        "                error_message += f\"\\n    Sample invalid values: {details['sample_invalid_values']}\"\n",
        "        raise DataIntegrityError(error_message)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate score normalization within (subject, grade, year) cells.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_score_normalization(\n",
        "    df: pd.DataFrame,\n",
        "    score_cols: List[str],\n",
        "    mean_target: float,\n",
        "    std_target: float,\n",
        "    tolerance: float,\n",
        "    min_sample_size: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the normalization of score columns within specified groups.\n",
        "\n",
        "    This function checks if the mean and standard deviation of score columns\n",
        "    are within a given tolerance of their target values for each\n",
        "    (grade, academic_year) cell.\n",
        "\n",
        "    Args:\n",
        "        df: The schema- and content-validated student survey DataFrame.\n",
        "        score_cols: A list of score column names to validate.\n",
        "        mean_target: The expected mean of the scores (e.g., 500).\n",
        "        std_target: The expected standard deviation of the scores (e.g., 100).\n",
        "        tolerance: The acceptable deviation from the target values.\n",
        "        min_sample_size: The minimum number of observations in a cell to\n",
        "                         perform the validation check.\n",
        "\n",
        "    Raises:\n",
        "        DataIntegrityError: If any cell's statistics fall outside the tolerance.\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Normalization Validation ---\n",
        "    # Group the DataFrame by grade and academic year.\n",
        "    grouped = df.groupby(['grade', 'academic_year'])\n",
        "\n",
        "    # Dictionary to store any normalization failures.\n",
        "    violations: List[str] = []\n",
        "\n",
        "    # Iterate through each score column to validate its normalization.\n",
        "    for score_col in score_cols:\n",
        "        # Aggregate to get mean, std, and count for each group.\n",
        "        stats = grouped[score_col].agg(['mean', 'std', 'count']).reset_index()\n",
        "\n",
        "        # Filter out groups with insufficient sample size.\n",
        "        stats_to_check = stats[stats['count'] >= min_sample_size]\n",
        "\n",
        "        # Check for mean violations.\n",
        "        mean_violations = stats_to_check[~np.isclose(stats_to_check['mean'], mean_target, atol=tolerance)]\n",
        "        for _, row in mean_violations.iterrows():\n",
        "            violations.append(\n",
        "                f\"Mean violation for '{score_col}' in (grade={row['grade']}, year={row['academic_year']}): \"\n",
        "                f\"Expected ~{mean_target}, Found {row['mean']:.2f} (Count: {row['count']})\"\n",
        "            )\n",
        "\n",
        "        # Check for standard deviation violations.\n",
        "        std_violations = stats_to_check[~np.isclose(stats_to_check['std'], std_target, atol=tolerance)]\n",
        "        for _, row in std_violations.iterrows():\n",
        "            violations.append(\n",
        "                f\"Std Dev violation for '{score_col}' in (grade={row['grade']}, year={row['academic_year']}): \"\n",
        "                f\"Expected ~{std_target}, Found {row['std']:.2f} (Count: {row['count']})\"\n",
        "            )\n",
        "\n",
        "    # If any violations were found, raise a single, comprehensive error.\n",
        "    if violations:\n",
        "        error_message = \"Score normalization validation failed:\\n\" + \"\\n\".join(violations)\n",
        "        raise DataIntegrityError(error_message)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_student_survey_data(\n",
        "    student_parent_survey_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the student_parent_survey_raw DataFrame.\n",
        "\n",
        "    This function executes a sequence of validation steps to ensure the input\n",
        "    DataFrame has the correct schema, adheres to all content integrity rules,\n",
        "    and meets the score normalization criteria specified in the study design.\n",
        "\n",
        "    Args:\n",
        "        student_parent_survey_raw: The raw student survey DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        True if all validation steps pass successfully.\n",
        "\n",
        "    Raises:\n",
        "        SchemaValidationError: If the DataFrame's schema is invalid.\n",
        "        DataIntegrityError: If the DataFrame's content or normalization is invalid.\n",
        "        TypeError: If input types are incorrect.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(student_parent_survey_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_parent_survey_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Validate DataFrame Schema ---\n",
        "    # Define the expected schema based on the provided data dictionary.\n",
        "    # Note: Nullable integer columns are represented as float64 by pandas.\n",
        "    expected_schema = {\n",
        "        'student_id': 'int64', 'school_id': 'int64', 'academic_year': 'int64',\n",
        "        'grade': 'int64', 'parent_response_complete': 'bool', 'gender': 'int64',\n",
        "        'student_cob': 'object', 'Score_Math': 'float64', 'Score_Lit': 'float64',\n",
        "        'Score_Eng': 'float64', 'days_homework_raw': 'float64', # Nullable int\n",
        "        'days_homework_cat': 'int64', 'parent_edu_mother': 'int64',\n",
        "        'parent_edu_father': 'int64', 'parent_cob_mother': 'object',\n",
        "        'parent_cob_father': 'object', 'employment_status_mother': 'int64',\n",
        "        'employment_status_father': 'int64', 'parent_talk_school': 'int64',\n",
        "        'parent_teach_homework': 'int64', 'parent_help_homework': 'int64',\n",
        "        'parent_check_homework': 'int64', 'books_at_home': 'int64',\n",
        "        'freq_use_books_home': 'int64', 'freq_use_computer_home': 'int64',\n",
        "        'freq_use_internet_home': 'int64'\n",
        "    }\n",
        "    _validate_dataframe_schema(df=student_parent_survey_raw, expected_schema=expected_schema)\n",
        "\n",
        "    # --- Step 2: Validate Content Integrity ---\n",
        "    # This checks domain constraints and structural rules.\n",
        "    _validate_content_integrity(df=student_parent_survey_raw)\n",
        "\n",
        "    # --- Step 3: Validate Score Normalization ---\n",
        "    # Extract parameters from the validated config.\n",
        "    score_cols = config['variable_definitions']['dependent_variables']\n",
        "    mean_target = config['psychometric_model']['standardization_mean']\n",
        "    std_target = config['psychometric_model']['standardization_std_dev']\n",
        "\n",
        "    _validate_score_normalization(\n",
        "        df=student_parent_survey_raw,\n",
        "        score_cols=score_cols,\n",
        "        mean_target=float(mean_target),\n",
        "        std_target=float(std_target),\n",
        "        tolerance=3.0,  # As specified in the task discussion\n",
        "        min_sample_size=30 # A reasonable threshold for stable stats\n",
        "    )\n",
        "\n",
        "    # If all validations pass, return True.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "BwbJq3Ltz3If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Validate barro_lee_raw DataFrame schema and instrument construction readiness\n",
        "\n",
        "# =====================================================================================\n",
        "# Task 3: Validate barro_lee_raw DataFrame schema and instrument construction readiness\n",
        "# =====================================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Verify presence, naming, and data types of all required columns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_barro_lee_schema(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the schema of the Barro-Lee DataFrame.\n",
        "\n",
        "    This function ensures the DataFrame containing the instrumental variable data\n",
        "    has the exact column structure and data types required for the analysis.\n",
        "    It checks for column presence, names, and dtypes against a canonical schema.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame containing the Barro-Lee (1960) data.\n",
        "\n",
        "    Raises:\n",
        "        SchemaValidationError: If the DataFrame's schema does not match the\n",
        "                               expected structure.\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Schema Definition ---\n",
        "    # Define the canonical schema for the barro_lee_raw DataFrame.\n",
        "    # The 'notes' column is often object type and can be nullable.\n",
        "    expected_schema: Dict[str, str] = {\n",
        "        'country_cob': 'object',\n",
        "        'country_name_1960': 'object',\n",
        "        'is_legacy_entity': 'bool',\n",
        "        'share_tertiary_women_1960': 'float64',\n",
        "        'share_tertiary_men_1960': 'float64',\n",
        "        'reference_year': 'int64',\n",
        "        'notes': 'object'\n",
        "    }\n",
        "\n",
        "    # --- Schema Validation ---\n",
        "    # Re-use the robust, generic schema validation function defined in Task 2.\n",
        "    # This promotes modularity and code reuse.\n",
        "    _validate_dataframe_schema(df=df, expected_schema=expected_schema)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Validate instrument data constraints and reference year.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_barro_lee_content(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the content integrity of the Barro-Lee DataFrame.\n",
        "\n",
        "    This function performs critical checks on the data values to ensure they\n",
        "    are consistent and valid for constructing the instrumental variable. It\n",
        "    verifies the reference year, the valid range of proportion data, and the\n",
        "    format of country codes.\n",
        "\n",
        "    Args:\n",
        "        df: The schema-validated Barro-Lee DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        DataIntegrityError: If any content integrity checks fail, containing a\n",
        "                            comprehensive report of all violations.\n",
        "        TypeError: If df is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Content Validation ---\n",
        "    # Dictionary to store any validation failures for a comprehensive report.\n",
        "    violations: Dict[str, Any] = {}\n",
        "\n",
        "    # 1. Validate that all entries in 'reference_year' are exactly 1960.\n",
        "    if not df['reference_year'].eq(1960).all():\n",
        "        # Find and report the count of non-compliant rows.\n",
        "        non_compliant_years = df.loc[~df['reference_year'].eq(1960), 'reference_year'].unique().tolist()\n",
        "        violations['reference_year'] = {\n",
        "            \"rule\": \"All values must be exactly 1960.\",\n",
        "            \"count\": df['reference_year'].ne(1960).sum(),\n",
        "            \"sample_invalid_values\": non_compliant_years\n",
        "        }\n",
        "\n",
        "    # 2. Validate that share variables are valid proportions [0.0, 1.0].\n",
        "    for col in ['share_tertiary_women_1960', 'share_tertiary_men_1960']:\n",
        "        # Isolate non-missing values for the range check.\n",
        "        series_to_check = df[col].dropna()\n",
        "        # Create a boolean mask for values outside the valid [0, 1] range.\n",
        "        invalid_mask = (series_to_check < 0.0) | (series_to_check > 1.0)\n",
        "        if invalid_mask.any():\n",
        "            # If violations exist, record them.\n",
        "            violating_values = series_to_check[invalid_mask].unique().tolist()\n",
        "            violations[col] = {\n",
        "                \"rule\": \"Values must be valid proportions in the range [0.0, 1.0].\",\n",
        "                \"count\": int(invalid_mask.sum()),\n",
        "                \"sample_invalid_values\": violating_values[:5]\n",
        "            }\n",
        "\n",
        "    # 3. Validate the format of 'country_cob' (ISO 3166-1 alpha-3).\n",
        "    # Isolate non-missing country codes for format validation.\n",
        "    cob_series = df['country_cob'].dropna()\n",
        "    # Check for length not equal to 3 or non-alphabetic characters.\n",
        "    invalid_format_mask = (cob_series.str.len() != 3) | (~cob_series.str.isalpha()) | (cob_series.str.upper() != cob_series)\n",
        "    if invalid_format_mask.any():\n",
        "        # If format violations exist, record them.\n",
        "        violating_values = cob_series[invalid_format_mask].unique().tolist()\n",
        "        violations['country_cob'] = {\n",
        "            \"rule\": \"Values must be 3-character, uppercase alphabetic ISO codes.\",\n",
        "            \"count\": int(invalid_format_mask.sum()),\n",
        "            \"sample_invalid_values\": violating_values[:5]\n",
        "        }\n",
        "\n",
        "    # If any violations were collected, raise a single, comprehensive error.\n",
        "    if violations:\n",
        "        error_message = \"Barro-Lee data content integrity validation failed:\"\n",
        "        for key, details in violations.items():\n",
        "            error_message += f\"\\n  - Check failed for '{key}':\"\n",
        "            error_message += f\"\\n    Rule: {details['rule']}\"\n",
        "            error_message += f\"\\n    Violation count: {details['count']}\"\n",
        "            if \"sample_invalid_values\" in details:\n",
        "                error_message += f\"\\n    Sample invalid values: {details['sample_invalid_values']}\"\n",
        "        raise DataIntegrityError(error_message)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate crosswalk coverage and legacy entity handling.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_instrument_coverage(\n",
        "    barro_lee_df: pd.DataFrame,\n",
        "    student_survey_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the coverage of the instrument data against the student data.\n",
        "\n",
        "    This function checks which parent countries of birth from the student survey\n",
        "    are present in the Barro-Lee dataset. It reports any coverage gaps and\n",
        "    validates that a policy for handling them is defined in the configuration.\n",
        "    It also checks that legacy entities are properly documented.\n",
        "\n",
        "    Args:\n",
        "        barro_lee_df: The schema- and content-validated Barro-Lee DataFrame.\n",
        "        student_survey_df: The student survey DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        DataIntegrityError: If there are unmapped countries and no valid policy\n",
        "                            is defined, or if legacy entities lack notes.\n",
        "        TypeError: If inputs are not of the expected types.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(barro_lee_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'barro_lee_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(student_survey_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_survey_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Coverage Validation ---\n",
        "    # 1. Extract unique, non-null country codes from the Barro-Lee data.\n",
        "    # Ensure codes are cleaned (uppercase, stripped) for fair comparison.\n",
        "    available_codes: Set[str] = set(barro_lee_df['country_cob'].str.upper().str.strip().dropna().unique())\n",
        "\n",
        "    # 2. Extract unique, non-null parent country codes from the student survey data.\n",
        "    mother_cobs = student_survey_df['parent_cob_mother'].str.upper().str.strip().dropna()\n",
        "    father_cobs = student_survey_df['parent_cob_father'].str.upper().str.strip().dropna()\n",
        "    required_codes: Set[str] = set(pd.concat([mother_cobs, father_cobs]).unique())\n",
        "\n",
        "    # 3. Compute the set of unmapped country codes.\n",
        "    unmapped_codes: Set[str] = required_codes - available_codes\n",
        "\n",
        "    # 4. Validate the policy for handling unmapped codes.\n",
        "    if unmapped_codes:\n",
        "        # Check if the configuration specifies a valid policy ('drop').\n",
        "        policy = config.get('instrument_construction', {}).get('instrument_missing_policy')\n",
        "        if policy != 'drop':\n",
        "            raise DataIntegrityError(\n",
        "                f\"Found {len(unmapped_codes)} unmapped parent countries of birth, but the \"\n",
        "                f\"policy in config['instrument_construction']['instrument_missing_policy'] \"\n",
        "                f\"is '{policy}', not 'drop'. Unmapped codes: {sorted(list(unmapped_codes))}\"\n",
        "            )\n",
        "        # If the policy is valid, this is a warning, not an error. The pipeline can proceed.\n",
        "        # In a real-world scenario, this would be logged as a warning.\n",
        "        print(f\"INFO: Found {len(unmapped_codes)} parent COBs not in Barro-Lee data. \"\n",
        "              f\"Observations from these countries will be dropped from IV analysis as per policy.\")\n",
        "\n",
        "    # 5. Validate handling of legacy entities.\n",
        "    # Check that rows marked as legacy entities have non-null notes.\n",
        "    legacy_rows = barro_lee_df[barro_lee_df['is_legacy_entity']]\n",
        "    legacy_without_notes = legacy_rows[legacy_rows['notes'].isnull()]\n",
        "    if not legacy_without_notes.empty:\n",
        "        # If any legacy entities lack documentation, raise an error.\n",
        "        countries_missing_notes = legacy_without_notes['country_name_1960'].tolist()\n",
        "        raise DataIntegrityError(\n",
        "            f\"Found {len(countries_missing_notes)} legacy entities with missing crosswalk \"\n",
        "            f\"notes: {countries_missing_notes}\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_barro_lee_data(\n",
        "    barro_lee_raw: pd.DataFrame,\n",
        "    student_parent_survey_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the Barro-Lee instrument DataFrame.\n",
        "\n",
        "    This function serves as the main entry point for validating the external\n",
        "    instrument data. It executes a sequence of checks for schema, content\n",
        "    integrity, and coverage against the main student survey data, ensuring it\n",
        "    is ready for use in the IV analysis.\n",
        "\n",
        "    Args:\n",
        "        barro_lee_raw: The raw pandas DataFrame with Barro-Lee (1960) data.\n",
        "        student_parent_survey_raw: The raw student survey DataFrame, used for\n",
        "                                   coverage checks.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        True if all validation steps pass successfully.\n",
        "\n",
        "    Raises:\n",
        "        SchemaValidationError: If the Barro-Lee DataFrame's schema is invalid.\n",
        "        DataIntegrityError: If the content, integrity, or coverage is invalid.\n",
        "        TypeError: If input types are incorrect.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # This is implicitly handled by the helper functions, but good practice.\n",
        "    if not isinstance(barro_lee_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'barro_lee_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(student_parent_survey_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_parent_survey_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Validate DataFrame Schema ---\n",
        "    # Ensures the structure of the instrument data is correct.\n",
        "    _validate_barro_lee_schema(df=barro_lee_raw)\n",
        "\n",
        "    # --- Step 2: Validate Content Integrity ---\n",
        "    # Ensures the values within the instrument data are valid.\n",
        "    _validate_barro_lee_content(df=barro_lee_raw)\n",
        "\n",
        "    # --- Step 3: Validate Instrument Coverage and Legacy Handling ---\n",
        "    # Ensures the instrument data can be successfully merged with the main data.\n",
        "    _validate_instrument_coverage(\n",
        "        barro_lee_df=barro_lee_raw,\n",
        "        student_survey_df=student_parent_survey_raw,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # If all validations pass without raising an exception, return True.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "qtcsHJ0Z2XSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Cleanse student_parent_survey_raw by enforcing inclusion criteria and removing invalid records\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse student_parent_survey_raw by enforcing inclusion criteria\n",
        "#         and removing invalid records\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Enforce primary sample inclusion criteria.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_inclusion_criteria(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Applies primary inclusion criteria to the student survey DataFrame.\n",
        "\n",
        "    This function filters the DataFrame based on two core criteria defined in\n",
        "    the configuration: the requirement for a complete parent response and the\n",
        "    restriction to valid grade levels. It meticulously tracks and returns the\n",
        "    number of records dropped at each stage.\n",
        "\n",
        "    Args:\n",
        "        df: The raw or partially processed student survey DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The filtered DataFrame.\n",
        "        - Dict[str, int]: A log detailing the sample size at each step.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Filtering and Logging ---\n",
        "    # Initialize a log to track sample attrition.\n",
        "    log = {'initial_rows': len(df)}\n",
        "\n",
        "    # 1. Filter for records with a complete parent response.\n",
        "    # This is a fundamental requirement for the analysis sample.\n",
        "    require_parent_response = config['population_scope']['require_parent_response']\n",
        "    if require_parent_response:\n",
        "        df_filtered = df[df['parent_response_complete']].copy()\n",
        "        log['after_parent_response_filter'] = len(df_filtered)\n",
        "    else:\n",
        "        df_filtered = df.copy()\n",
        "        log['after_parent_response_filter'] = len(df_filtered) # No change\n",
        "\n",
        "    # 2. Filter for records within the valid grade levels for the study.\n",
        "    # This ensures the sample is restricted to the grades of interest.\n",
        "    valid_grades = config['sample_policies']['valid_grades']\n",
        "    df_filtered = df_filtered[df_filtered['grade'].isin(valid_grades)]\n",
        "    log['after_grade_filter'] = len(df_filtered)\n",
        "\n",
        "    return df_filtered, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Remove structurally invalid records and enforce exclusions.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _remove_invalid_records(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Removes structurally invalid records and applies study-specific exclusions.\n",
        "\n",
        "    This function cleans the DataFrame by removing records that are impossible\n",
        "    by study design (e.g., Grade 10 in 2016) and records with missing critical\n",
        "    identifiers (student, school, year). This revised implementation provides a\n",
        "    highly granular log of records dropped due to missing identifiers.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame after primary inclusion criteria have been applied.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The further cleansed DataFrame.\n",
        "        - Dict[str, int]: A detailed log of the number of rows dropped at each\n",
        "                          sub-step.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input df is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    # Ensure the input config is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Filtering and Logging ---\n",
        "    # Record the number of rows before any filtering in this function.\n",
        "    initial_rows = len(df)\n",
        "    # Initialize a dictionary to log the counts of dropped rows.\n",
        "    log: Dict[str, int] = {}\n",
        "\n",
        "    # Create a working copy of the DataFrame to avoid modifying the original input.\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # 1. Apply study-specific exclusions from the configuration.\n",
        "    # This loop makes the exclusion logic generalizable to multiple rules.\n",
        "    for exclusion in config['sample_policies']['exclude_year_grade']:\n",
        "        # Create a boolean mask for rows that match the exclusion criteria.\n",
        "        mask = (df_filtered['academic_year'] == exclusion['academic_year']) & \\\n",
        "               (df_filtered['grade'] == exclusion['grade'])\n",
        "        # Apply the filter by keeping only the rows that do NOT match the mask.\n",
        "        df_filtered = df_filtered[~mask]\n",
        "\n",
        "    # Log the number of rows dropped by this specific exclusion rule.\n",
        "    rows_after_exclusion = len(df_filtered)\n",
        "    log['dropped_by_study_exclusion'] = initial_rows - rows_after_exclusion\n",
        "\n",
        "    # 2. Sequentially remove records with missing critical identifiers.\n",
        "    # This provides a granular audit trail of data attrition.\n",
        "    id_cols = ['student_id', 'school_id', 'academic_year']\n",
        "\n",
        "    # Keep track of the row count before starting the sequential check.\n",
        "    rows_before_id_check = len(df_filtered)\n",
        "\n",
        "    # Iterate through each identifier column to check for and remove nulls.\n",
        "    for id_col in id_cols:\n",
        "        # Record the number of rows before filtering on the current identifier.\n",
        "        rows_before_current_check = len(df_filtered)\n",
        "        # Create a mask for non-null values in the current identifier column.\n",
        "        non_null_mask = df_filtered[id_col].notna()\n",
        "        # Apply the filter.\n",
        "        df_filtered = df_filtered[non_null_mask]\n",
        "        # Count the number of rows dropped in this specific sub-step.\n",
        "        rows_dropped_current = rows_before_current_check - len(df_filtered)\n",
        "        # If any rows were dropped, log the count with a descriptive key.\n",
        "        if rows_dropped_current > 0:\n",
        "            log[f'dropped_missing_{id_col}'] = rows_dropped_current\n",
        "\n",
        "    # Calculate the total number of rows dropped due to any missing identifier.\n",
        "    rows_after_id_check = len(df_filtered)\n",
        "    log['total_dropped_by_missing_identifiers'] = rows_before_id_check - rows_after_id_check\n",
        "\n",
        "    # Return the fully cleansed DataFrame and the detailed log.\n",
        "    return df_filtered, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Identify and resolve duplicate records.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _resolve_duplicates(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Identifies and resolves duplicate records based on a composite key.\n",
        "\n",
        "    This function ensures that each observation, defined by the unique key\n",
        "    (student_id, academic_year, grade), is unique. It uses a deterministic\n",
        "    strategy to resolve duplicates, prioritizing records with more complete\n",
        "    data.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame after filtering and exclusions.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The deduplicated DataFrame.\n",
        "        - int: The number of duplicate rows that were dropped.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Deduplication Logic ---\n",
        "    # Define the composite key that should be unique for each observation.\n",
        "    key_cols = ['student_id', 'academic_year', 'grade']\n",
        "\n",
        "    # Find all rows that are part of any duplicate set.\n",
        "    duplicates_mask = df.duplicated(subset=key_cols, keep=False)\n",
        "\n",
        "    # If no duplicates exist, return the original DataFrame.\n",
        "    if not duplicates_mask.any():\n",
        "        return df, 0\n",
        "\n",
        "    # Separate unique records from duplicate sets for processing.\n",
        "    df_unique = df[~duplicates_mask]\n",
        "    df_duplicates = df[duplicates_mask]\n",
        "\n",
        "    # --- Deterministic Resolution Strategy ---\n",
        "    # 1. Calculate the number of non-missing values for each duplicate row.\n",
        "    df_duplicates['completeness'] = df_duplicates.notna().sum(axis=1)\n",
        "\n",
        "    # 2. Sort duplicates to bring the \"best\" record to the top of each group.\n",
        "    # Sort by completeness (descending), then by original index (ascending) for tie-breaking.\n",
        "    df_sorted_duplicates = df_duplicates.sort_values(\n",
        "        by=['student_id', 'academic_year', 'grade', 'completeness', df_duplicates.index],\n",
        "        ascending=[True, True, True, False, True]\n",
        "    )\n",
        "\n",
        "    # 3. Keep only the first (i.e., the \"best\") record for each duplicate group.\n",
        "    df_resolved = df_sorted_duplicates.drop_duplicates(subset=key_cols, keep='first')\n",
        "\n",
        "    # Drop the temporary 'completeness' column.\n",
        "    df_resolved = df_resolved.drop(columns=['completeness'])\n",
        "\n",
        "    # 4. Combine the unique records with the resolved duplicate records.\n",
        "    df_final = pd.concat([df_unique, df_resolved], ignore_index=True)\n",
        "\n",
        "    # --- Final Assertion and Logging ---\n",
        "    # Verify that the deduplication was successful.\n",
        "    if df_final.duplicated(subset=key_cols).any():\n",
        "        raise RuntimeError(\"Deduplication process failed. Duplicates still exist.\")\n",
        "\n",
        "    # Calculate the number of rows dropped.\n",
        "    rows_dropped = len(df) - len(df_final)\n",
        "\n",
        "    return df_final, rows_dropped\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_student_survey_data(\n",
        "    student_parent_survey_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of the student survey DataFrame.\n",
        "\n",
        "    This function applies a multi-step cleansing process to the raw data,\n",
        "    enforcing inclusion criteria, removing invalid records, and resolving\n",
        "    duplicates. It produces a clean DataFrame ready for feature engineering\n",
        "    and a detailed log of the sample attrition at each step.\n",
        "\n",
        "    Args:\n",
        "        student_parent_survey_raw: The raw student survey DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The fully cleansed DataFrame.\n",
        "        - Dict[str, Any]: A comprehensive log of the cleansing process and\n",
        "                          sample attrition.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(student_parent_survey_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_parent_survey_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Apply primary inclusion criteria ---\n",
        "    # Filter based on parent response and valid grades.\n",
        "    df_step1, log_step1 = _apply_inclusion_criteria(\n",
        "        df=student_parent_survey_raw,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Remove structurally invalid records ---\n",
        "    # Apply study-specific exclusions and remove rows with missing IDs.\n",
        "    df_step2, log_step2 = _remove_invalid_records(\n",
        "        df=df_step1,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Resolve duplicate records ---\n",
        "    # Ensure each observation (student-year-grade) is unique.\n",
        "    df_step3, dropped_duplicates = _resolve_duplicates(df=df_step2)\n",
        "\n",
        "    # --- Compile Final Log ---\n",
        "    # Combine logs from all steps into a single, comprehensive audit trail.\n",
        "    final_log = {\n",
        "        \"initial_rows\": log_step1['initial_rows'],\n",
        "        \"rows_after_parent_response_filter\": log_step1['after_parent_response_filter'],\n",
        "        \"rows_after_grade_filter\": log_step1['after_grade_filter'],\n",
        "        \"rows_after_exclusions_and_id_check\": len(df_step2) - log_step2['dropped_by_missing_identifiers'],\n",
        "        \"final_rows_after_deduplication\": len(df_step3),\n",
        "        \"summary_dropped\": {\n",
        "            \"dropped_no_parent_response\": log_step1['initial_rows'] - log_step1['after_parent_response_filter'],\n",
        "            \"dropped_invalid_grade\": log_step1['after_parent_response_filter'] - log_step1['after_grade_filter'],\n",
        "            \"dropped_by_study_exclusion\": log_step2['dropped_by_study_exclusion'],\n",
        "            \"dropped_by_missing_identifiers\": log_step2['dropped_by_missing_identifiers'],\n",
        "            \"dropped_as_duplicates\": dropped_duplicates\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Return the final cleansed DataFrame and the detailed log.\n",
        "    return df_step3, final_log\n"
      ],
      "metadata": {
        "id": "C2Eujppz4PD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Harmonize and standardize categorical codings and country codes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Harmonize and standardize categorical codings and country codes\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Standardize ISO country codes for student and parents.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _standardize_country_codes(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Standardizes and validates ISO 3166-1 alpha-3 country codes.\n",
        "\n",
        "    This function processes specified country code columns to ensure they conform\n",
        "    to a standard format: 3-character, uppercase, alphabetic strings. It cleans\n",
        "    the data by stripping whitespace and converting to uppercase. Any codes that\n",
        "    do not meet the format requirements after cleaning are set to NaN.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing country code columns to be standardized.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame with standardized country code columns.\n",
        "        - Dict[str, int]: A log detailing the number of invalid codes nullified\n",
        "                          for each column.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Standardization and Validation ---\n",
        "    # Define the columns containing country codes that require standardization.\n",
        "    cob_cols = ['student_cob', 'parent_cob_mother', 'parent_cob_father']\n",
        "    # Initialize a log to track the number of nullified invalid entries.\n",
        "    log = {f\"nullified_{col}\": 0 for col in cob_cols}\n",
        "\n",
        "    # Create a copy to avoid SettingWithCopyWarning.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Iterate through each country code column to apply standardization.\n",
        "    for col in cob_cols:\n",
        "        # Ensure the column exists before processing.\n",
        "        if col not in df_processed.columns:\n",
        "            raise SchemaValidationError(f\"Missing expected country code column: '{col}'\")\n",
        "\n",
        "        # Use the .str accessor for robust, NaN-safe string operations.\n",
        "        # 1. Convert to uppercase and strip leading/trailing whitespace.\n",
        "        cleaned_series = df_processed[col].str.upper().str.strip()\n",
        "\n",
        "        # 2. Identify invalid formats (not 3 chars, not alphabetic) among non-null entries.\n",
        "        # This mask will be True for any value that is correctly formatted.\n",
        "        valid_mask = (\n",
        "            (cleaned_series.str.len() == 3) &\n",
        "            (cleaned_series.str.isalpha())\n",
        "        )\n",
        "\n",
        "        # The invalid mask applies to entries that are not null AND not valid.\n",
        "        invalid_mask = cleaned_series.notna() & ~valid_mask\n",
        "\n",
        "        # If any invalid entries are found, log the count and nullify them.\n",
        "        if invalid_mask.any():\n",
        "            log[f\"nullified_{col}\"] = int(invalid_mask.sum())\n",
        "            # Use .loc to safely modify the DataFrame slice.\n",
        "            df_processed.loc[invalid_mask, col] = np.nan\n",
        "        else:\n",
        "            # If no invalid entries, update the column with the cleaned version.\n",
        "            df_processed[col] = cleaned_series\n",
        "\n",
        "    return df_processed, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Validate and clip/nullify numeric categorical variables.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_numeric_categoricals(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Validates and cleans numeric categorical variables based on defined domains.\n",
        "\n",
        "    This function enforces domain integrity for various numeric columns.\n",
        "    Depending on the variable, values outside the valid domain are either\n",
        "    clipped to the nearest boundary or set to NaN.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame with numeric categorical columns to validate.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame with cleaned numeric categorical columns.\n",
        "        - Dict[str, int]: A log detailing the number of values clipped or\n",
        "                          nullified for each column.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Domain Rules Definition ---\n",
        "    # Define rules for each variable: domain and action ('clip' or 'nullify').\n",
        "    rules = {\n",
        "        'days_homework_raw': {'domain': (0, 7), 'action': 'clip'},\n",
        "        'days_homework_cat': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'parent_edu_mother': {'domain': set(range(9)), 'action': 'nullify'},\n",
        "        'parent_edu_father': {'domain': set(range(9)), 'action': 'nullify'},\n",
        "        'parent_talk_school': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'parent_teach_homework': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'parent_help_homework': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'parent_check_homework': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'freq_use_books_home': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'freq_use_computer_home': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'freq_use_internet_home': {'domain': {1, 2, 3, 4}, 'action': 'nullify'},\n",
        "        'books_at_home': {'domain': {1, 2, 3, 4, 5}, 'action': 'nullify'},\n",
        "    }\n",
        "\n",
        "    # --- Validation and Cleaning ---\n",
        "    # Initialize a log for actions taken.\n",
        "    log = {}\n",
        "    # Create a copy to ensure modifications are safe.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Iterate through the rules and apply them.\n",
        "    for col, rule in rules.items():\n",
        "        if col not in df_processed.columns:\n",
        "            raise SchemaValidationError(f\"Missing expected numeric categorical column: '{col}'\")\n",
        "\n",
        "        original_series = df_processed[col].copy()\n",
        "\n",
        "        if rule['action'] == 'clip':\n",
        "            # Apply clipping for range-bound variables.\n",
        "            lower, upper = rule['domain']\n",
        "            df_processed[col] = df_processed[col].clip(lower, upper)\n",
        "            # Count how many values were changed.\n",
        "            num_clipped = (original_series != df_processed[col]).sum()\n",
        "            if num_clipped > 0:\n",
        "                log[f\"clipped_{col}\"] = int(num_clipped)\n",
        "\n",
        "        elif rule['action'] == 'nullify':\n",
        "            # Nullify values outside the valid set for discrete variables.\n",
        "            valid_set = rule['domain']\n",
        "            invalid_mask = ~df_processed[col].isin(valid_set) & df_processed[col].notna()\n",
        "            if invalid_mask.any():\n",
        "                num_nullified = int(invalid_mask.sum())\n",
        "                log[f\"nullified_{col}\"] = num_nullified\n",
        "                df_processed.loc[invalid_mask, col] = np.nan\n",
        "\n",
        "    return df_processed, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Document reference categories for all categorical variables.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _document_reference_categories(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts and documents reference categories for modeling.\n",
        "\n",
        "    This function creates a metadata dictionary that serves as a single source\n",
        "    of truth for the reference (omitted) category of each categorical variable\n",
        "    used in the regression models. This ensures consistency in dummy variable\n",
        "    creation and model interpretation.\n",
        "\n",
        "    Args:\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping variable names to their reference categories.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If a required reference category is not defined\n",
        "                            in the configuration.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Metadata Extraction ---\n",
        "    # Initialize the metadata dictionary.\n",
        "    reference_categories = {}\n",
        "\n",
        "    # Define paths in the config to find reference category definitions.\n",
        "    paths = {\n",
        "        'days_homework_cat': ('effort_coding', 'reference_category'),\n",
        "        'parent_talk_school': ('parental_investment_coding', 'reference_category'),\n",
        "        'parent_teach_homework': ('parental_investment_coding', 'reference_category'),\n",
        "        'parent_help_homework': ('parental_investment_coding', 'reference_category'),\n",
        "        'parent_check_homework': ('parental_investment_coding', 'reference_category'),\n",
        "        'books_at_home': ('books_at_home_coding', 'reference_category'),\n",
        "        'freq_use_books_home': ('parental_investment_coding', 'reference_category'), # Assumes same scale\n",
        "        'freq_use_computer_home': ('parental_investment_coding', 'reference_category'),\n",
        "        'freq_use_internet_home': ('parental_investment_coding', 'reference_category'),\n",
        "    }\n",
        "\n",
        "    # Extract reference categories from the config using the defined paths.\n",
        "    for var, path in paths.items():\n",
        "        try:\n",
        "            ref_cat = config[path[0]][path[1]]\n",
        "            reference_categories[var] = ref_cat\n",
        "        except KeyError:\n",
        "            raise ConfigurationError(f\"Missing reference category definition in config for path: {path}\")\n",
        "\n",
        "    # Manually add documented reference categories not in paths.\n",
        "    # For gender, we consistently use Female (coded as 1) as the reference.\n",
        "    reference_categories['gender'] = 1\n",
        "    # For employment status, we use Not employed/Inactive (coded as 0) as the reference.\n",
        "    reference_categories['employment_status_mother'] = 0\n",
        "    reference_categories['employment_status_father'] = 0\n",
        "\n",
        "    return reference_categories\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def harmonize_and_standardize_data(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the harmonization and standardization of categorical data.\n",
        "\n",
        "    This function executes a sequence of cleaning and documentation steps:\n",
        "    1. Standardizes country codes to a consistent format.\n",
        "    2. Validates and cleans numeric categorical variables based on their domains.\n",
        "    3. Extracts and documents reference categories for all model variables.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The cleansed DataFrame from the previous task.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The harmonized and standardized DataFrame.\n",
        "        - Dict[str, Any]: A log detailing all cleaning actions taken.\n",
        "        - Dict[str, Any]: A metadata dictionary of reference categories.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(cleansed_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'cleansed_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Standardize ISO country codes ---\n",
        "    # Ensures country codes are clean and consistently formatted for merging.\n",
        "    df_step1, log_step1 = _standardize_country_codes(df=cleansed_df)\n",
        "\n",
        "    # --- Step 2: Validate and clean numeric categoricals ---\n",
        "    # Enforces domain integrity for survey response variables.\n",
        "    df_step2, log_step2 = _validate_numeric_categoricals(df=df_step1)\n",
        "\n",
        "    # --- Step 3: Document reference categories ---\n",
        "    # Creates an authoritative source for model reference levels.\n",
        "    reference_categories_metadata = _document_reference_categories(config=config)\n",
        "\n",
        "    # --- Compile Final Log ---\n",
        "    # Combine logs from all steps into a single, comprehensive audit trail.\n",
        "    final_log = {\n",
        "        \"country_code_standardization\": log_step1,\n",
        "        \"numeric_categorical_validation\": log_step2\n",
        "    }\n",
        "\n",
        "    # Return the processed DataFrame, the log, and the metadata.\n",
        "    return df_step2, final_log, reference_categories_metadata\n"
      ],
      "metadata": {
        "id": "Uq1HOiM05omT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Construct the parental highest education variable (PHE) and associated dummies\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Construct the parental highest education variable (PHE) and\n",
        "#         associated dummies\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Compute raw parental highest education (PHE_raw).\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_phe_raw(df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Computes the raw parental highest education level (PHE_raw).\n",
        "\n",
        "    This function determines the highest ISCED level of education between the\n",
        "    two parents for each student. The result is stored in a new column, 'PHE_raw'.\n",
        "    The logic correctly handles missing values for one or both parents.\n",
        "\n",
        "    Equation implemented: PHE_raw_i = max(parent_edu_mother_i, parent_edu_father_i)\n",
        "\n",
        "    Args:\n",
        "        df: The harmonized student survey DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the 'PHE_raw' column.\n",
        "        - int: The number of rows where 'PHE_raw' could not be computed\n",
        "               (i.e., is NaN).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    required_cols = ['parent_edu_mother', 'parent_edu_father']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise SchemaValidationError(f\"Missing one or more required columns: {required_cols}\")\n",
        "\n",
        "    # --- Computation ---\n",
        "    # Create a copy to avoid modifying the original DataFrame in place.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Compute the element-wise maximum of the two parent education columns.\n",
        "    # The .max(axis=1) method on a DataFrame selection is NaN-aware:\n",
        "    # - max(5, 3) -> 5\n",
        "    # - max(5, NaN) -> 5\n",
        "    # - max(NaN, NaN) -> NaN\n",
        "    # This behavior is exactly what is required for the logic.\n",
        "    df_processed['PHE_raw'] = df_processed[required_cols].max(axis=1)\n",
        "\n",
        "    # --- Logging ---\n",
        "    # Count the number of observations where PHE_raw is missing.\n",
        "    # This occurs if and only if both parent education levels are missing.\n",
        "    num_missing_phe_raw = df_processed['PHE_raw'].isna().sum()\n",
        "\n",
        "    return df_processed, int(num_missing_phe_raw)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Bin PHE_raw into three ISCED-level groups (PHE).\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _bin_phe_raw_to_phe(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Bins the raw parental education level into three analysis categories (PHE).\n",
        "\n",
        "    This function maps the continuous 'PHE_raw' variable (ISCED levels 0-8)\n",
        "    into the three-level categorical variable 'PHE' (0, 1, 2) as defined in\n",
        "    the study's configuration.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the 'PHE_raw' column.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the 'PHE' column.\n",
        "        - pd.Series: The value counts of the newly created 'PHE' column.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    if 'PHE_raw' not in df.columns:\n",
        "        raise SchemaValidationError(\"Missing required input column: 'PHE_raw'\")\n",
        "\n",
        "    # --- Binning Logic ---\n",
        "    # Create a copy for safe modification.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Extract the binning definitions from the configuration.\n",
        "    phe_coding = config['parental_education_coding']\n",
        "    level_0_bins = phe_coding['level_0_raw']\n",
        "    level_1_bins = phe_coding['level_1_raw']\n",
        "    level_2_bins = phe_coding['level_2_raw']\n",
        "\n",
        "    # Define the conditions for each category based on the config.\n",
        "    conditions = [\n",
        "        df_processed['PHE_raw'].isin(level_0_bins),\n",
        "        df_processed['PHE_raw'].isin(level_1_bins),\n",
        "        df_processed['PHE_raw'].isin(level_2_bins),\n",
        "    ]\n",
        "    # Define the corresponding values for each condition.\n",
        "    choices = [0, 1, 2]\n",
        "\n",
        "    # Use np.select for efficient, vectorized conditional assignment.\n",
        "    # The default value is np.nan, which correctly handles missing PHE_raw.\n",
        "    df_processed['PHE'] = np.select(conditions, choices, default=np.nan)\n",
        "\n",
        "    # --- Final Validation and Logging ---\n",
        "    # Assert that all non-null PHE_raw values were successfully binned.\n",
        "    unmapped_mask = df_processed['PHE_raw'].notna() & df_processed['PHE'].isna()\n",
        "    if unmapped_mask.any():\n",
        "        unmapped_values = df_processed.loc[unmapped_mask, 'PHE_raw'].unique()\n",
        "        raise DataIntegrityError(\n",
        "            f\"PHE binning failed. The following 'PHE_raw' values were not mapped \"\n",
        "            f\"to a category: {unmapped_values}\"\n",
        "        )\n",
        "\n",
        "    # Get the distribution of the new 'PHE' variable for logging.\n",
        "    phe_distribution = df_processed['PHE'].value_counts().sort_index()\n",
        "\n",
        "    return df_processed, phe_distribution\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Create dummy variables for PHE.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_phe_dummies(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates dummy variables for the categorical PHE variable.\n",
        "\n",
        "    This function generates two binary indicator variables, 'p1' and 'p2',\n",
        "    representing PHE levels 1 and 2, respectively. The base/reference\n",
        "    category is PHE level 0. These dummies are essential for the OLS model\n",
        "    specifications.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame containing the 'PHE' column.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The DataFrame augmented with the 'p1' and 'p2' dummy columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    if 'PHE' not in df.columns:\n",
        "        raise SchemaValidationError(\"Missing required input column: 'PHE'\")\n",
        "\n",
        "    # --- Dummy Creation ---\n",
        "    # Create a copy for safe modification.\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Extract the specified names for the dummy variables from the config.\n",
        "    dummy_names = config['variable_definitions']['phe_dummy_names']\n",
        "    if len(dummy_names) != 2:\n",
        "        raise ConfigurationError(\"Expected two names in 'phe_dummy_names'.\")\n",
        "    p1_name, p2_name = dummy_names\n",
        "\n",
        "    # Create the dummy for PHE level 1.\n",
        "    # The result of the boolean comparison is cast to an integer (0 or 1).\n",
        "    df_processed[p1_name] = (df_processed['PHE'] == 1).astype(float)\n",
        "\n",
        "    # Create the dummy for PHE level 2.\n",
        "    df_processed[p2_name] = (df_processed['PHE'] == 2).astype(float)\n",
        "\n",
        "    # Explicitly set dummies to NaN where the original PHE is NaN.\n",
        "    # This ensures correct handling during listwise deletion in modeling.\n",
        "    phe_is_na = df_processed['PHE'].isna()\n",
        "    df_processed.loc[phe_is_na, [p1_name, p2_name]] = np.nan\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    # Check that for any row, at most one dummy is active.\n",
        "    if (df_processed[p1_name] + df_processed[p2_name] > 1).any():\n",
        "        raise DataIntegrityError(\"PHE dummy creation failed: a row has multiple dummies active.\")\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_parental_education_variable(\n",
        "    harmonized_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the Parental Highest Education (PHE) variable.\n",
        "\n",
        "    This function executes the full feature engineering pipeline for the study's\n",
        "    primary endogenous variable. It computes the raw maximum education level,\n",
        "    bins it into the three analytical categories, and creates the corresponding\n",
        "    dummy variables for regression analysis.\n",
        "\n",
        "    Args:\n",
        "        harmonized_df: The harmonized and standardized DataFrame from Task 5.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with 'PHE_raw', 'PHE', 'p1',\n",
        "                        and 'p2' columns.\n",
        "        - Dict[str, Any]: A log detailing the construction process, including\n",
        "                          missing value counts and distributional summaries.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(harmonized_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'harmonized_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Compute raw parental highest education (PHE_raw) ---\n",
        "    # This determines the maximum ISCED level between both parents.\n",
        "    df_step1, num_missing_phe_raw = _compute_phe_raw(df=harmonized_df)\n",
        "\n",
        "    # --- Step 2: Bin PHE_raw into the three 'PHE' categories ---\n",
        "    # This maps the raw ISCED levels to the 0, 1, 2 analytical groups.\n",
        "    df_step2, phe_distribution = _bin_phe_raw_to_phe(df=df_step1, config=config)\n",
        "\n",
        "    # --- Step 3: Create dummy variables for 'PHE' ---\n",
        "    # This generates the 'p1' and 'p2' regressors for the OLS models.\n",
        "    df_final = _create_phe_dummies(df=df_step2, config=config)\n",
        "\n",
        "    # --- Compile Final Log ---\n",
        "    # Create a comprehensive log of the entire feature construction process.\n",
        "    final_log = {\n",
        "        \"missing_phe_raw_count\": num_missing_phe_raw,\n",
        "        \"phe_category_distribution\": phe_distribution.to_dict()\n",
        "    }\n",
        "\n",
        "    # Return the final DataFrame and the detailed log.\n",
        "    return df_final, final_log\n"
      ],
      "metadata": {
        "id": "sYLcx3JS6uSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Construct student effort and parental investment control variables\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Construct student effort and parental investment control variables\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Generic Helper Function for Dummy Variable Creation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_dummies_from_categorical(\n",
        "    df: pd.DataFrame,\n",
        "    column_name: str,\n",
        "    reference_category: Any\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Creates dummy variables for a single categorical column.\n",
        "\n",
        "    This generic helper function takes a categorical column, identifies its unique\n",
        "    non-null values, and creates a binary indicator (dummy) column for each\n",
        "    category except for the specified reference category. The new columns are\n",
        "    named systematically as '{column_name}_{category}'.\n",
        "\n",
        "    Args:\n",
        "        df: The DataFrame to be modified.\n",
        "        column_name: The name of the source categorical column.\n",
        "        reference_category: The category to be omitted (the base level).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the new dummy columns.\n",
        "        - List[str]: A list of the names of the newly created dummy columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if column_name not in df.columns:\n",
        "        raise SchemaValidationError(f\"Source column '{column_name}' not found in DataFrame.\")\n",
        "\n",
        "    # --- Dummy Creation Logic ---\n",
        "    df_processed = df.copy()\n",
        "    created_dummies = []\n",
        "\n",
        "    # Find all unique categories in the column, excluding NaNs.\n",
        "    categories = df_processed[column_name].dropna().unique()\n",
        "\n",
        "    # Iterate over each category to create a corresponding dummy variable.\n",
        "    for category in categories:\n",
        "        # Skip the creation of a dummy for the reference category.\n",
        "        if category == reference_category:\n",
        "            continue\n",
        "\n",
        "        # Define a systematic and descriptive name for the new dummy column.\n",
        "        dummy_name = f\"{column_name}_{int(category)}\"\n",
        "\n",
        "        # Create the binary indicator column.\n",
        "        df_processed[dummy_name] = (df_processed[column_name] == category).astype(float)\n",
        "\n",
        "        # Ensure NaNs in the source propagate to the dummy variables.\n",
        "        df_processed.loc[df_processed[column_name].isna(), dummy_name] = np.nan\n",
        "\n",
        "        # Add the new dummy's name to the list of created columns.\n",
        "        created_dummies.append(dummy_name)\n",
        "\n",
        "    return df_processed, created_dummies\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_control_variables(\n",
        "    df_with_phe: pd.DataFrame,\n",
        "    reference_categories: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of all control variables for regression models.\n",
        "\n",
        "    This function systematically converts all categorical student and household\n",
        "    characteristics into dummy variables, preparing the DataFrame for the\n",
        "    modeling stage. It uses a generic helper to ensure consistent and accurate\n",
        "    dummy creation based on the provided reference category metadata.\n",
        "\n",
        "    Args:\n",
        "        df_with_phe: The DataFrame from Task 6, containing the PHE variables.\n",
        "        reference_categories: The metadata dictionary from Task 5, specifying\n",
        "                              the reference category for each variable.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The fully-featured DataFrame ready for modeling.\n",
        "        - List[str]: A comprehensive list of all newly created control\n",
        "                     variable names (the dummy variables).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df_with_phe, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df_with_phe' must be a pandas DataFrame.\")\n",
        "    if not isinstance(reference_categories, dict):\n",
        "        raise TypeError(\"Input 'reference_categories' must be a dictionary.\")\n",
        "\n",
        "    # --- Feature Engineering ---\n",
        "    df_processed = df_with_phe.copy()\n",
        "    all_created_dummies = []\n",
        "\n",
        "    # --- Step 1 & 2 & 3 (Combined): Create dummies for all multi-category variables ---\n",
        "    # Define the list of all categorical variables that require dummification.\n",
        "    # This list is the single source of truth for this operation.\n",
        "    vars_to_dummify = [\n",
        "        'days_homework_cat',\n",
        "        'parent_talk_school', 'parent_teach_homework', 'parent_help_homework', 'parent_check_homework',\n",
        "        'freq_use_books_home', 'freq_use_computer_home', 'freq_use_internet_home',\n",
        "        'books_at_home',\n",
        "        'employment_status_mother', 'employment_status_father'\n",
        "    ]\n",
        "\n",
        "    # Loop through the list and apply the generic dummy creation function.\n",
        "    for var in vars_to_dummify:\n",
        "        if var not in reference_categories:\n",
        "            raise ConfigurationError(f\"Reference category for '{var}' not found in metadata.\")\n",
        "\n",
        "        # Call the helper to create dummies for the current variable.\n",
        "        df_processed, new_dummies = _create_dummies_from_categorical(\n",
        "            df=df_processed,\n",
        "            column_name=var,\n",
        "            reference_category=reference_categories[var]\n",
        "        )\n",
        "        # Aggregate the names of the newly created dummies.\n",
        "        all_created_dummies.extend(new_dummies)\n",
        "\n",
        "    # --- Step 3 (Continued): Create binary indicator variables ---\n",
        "\n",
        "    # 1. Gender dummy (Male indicator).\n",
        "    # Reference category is Female (coded as 1).\n",
        "    gender_ref = reference_categories.get('gender')\n",
        "    if gender_ref is None:\n",
        "        raise ConfigurationError(\"Reference category for 'gender' not found.\")\n",
        "\n",
        "    # Create a dummy for the non-reference gender category (Male, coded as 2).\n",
        "    df_processed['gender_male'] = (df_processed['gender'] != gender_ref).astype(float)\n",
        "    df_processed.loc[df_processed['gender'].isna(), 'gender_male'] = np.nan\n",
        "    all_created_dummies.append('gender_male')\n",
        "\n",
        "    # 2. Foreign-born status indicators.\n",
        "    # The reference is being born in Spain ('ESP').\n",
        "    # Student foreign-born indicator.\n",
        "    df_processed['student_foreign_born'] = (df_processed['student_cob'] != 'ESP').astype(float)\n",
        "    df_processed.loc[df_processed['student_cob'].isna(), 'student_foreign_born'] = np.nan\n",
        "    all_created_dummies.append('student_foreign_born')\n",
        "\n",
        "    # Mother foreign-born indicator.\n",
        "    df_processed['mother_foreign_born'] = (df_processed['parent_cob_mother'] != 'ESP').astype(float)\n",
        "    df_processed.loc[df_processed['parent_cob_mother'].isna(), 'mother_foreign_born'] = np.nan\n",
        "    all_created_dummies.append('mother_foreign_born')\n",
        "\n",
        "    # Father foreign-born indicator.\n",
        "    df_processed['father_foreign_born'] = (df_processed['parent_cob_father'] != 'ESP').astype(float)\n",
        "    df_processed.loc[df_processed['parent_cob_father'].isna(), 'father_foreign_born'] = np.nan\n",
        "    all_created_dummies.append('father_foreign_born')\n",
        "\n",
        "    # Return the final DataFrame and the list of all new control variables.\n",
        "    return df_processed, all_created_dummies\n"
      ],
      "metadata": {
        "id": "PmIzzACa70m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Construct the instrumental variable (GG) by selecting the more-educated parent's country and merging with Barro-Lee\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Construct the instrumental variable (GG) by selecting the\n",
        "#         more-educated parent's country and merging with Barro-Lee\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Identify the more-educated parent's country of birth.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _select_instrument_country_of_birth(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Identifies and selects the country of birth of the more-educated parent.\n",
        "\n",
        "    This function implements the logic to determine which parent's country of\n",
        "    birth to use for the instrumental variable merge. The selection is based on\n",
        "    the parent with the higher ISCED education level, with a deterministic\n",
        "    tie-breaking rule specified in the configuration.\n",
        "\n",
        "    Args:\n",
        "        df: The feature-engineered student survey DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the column\n",
        "                        'more_educated_parent_cob'.\n",
        "        - int: The number of rows where the instrument country could not be\n",
        "               determined.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    required_cols = [\n",
        "        'parent_edu_mother', 'parent_edu_father',\n",
        "        'parent_cob_mother', 'parent_cob_father'\n",
        "    ]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise SchemaValidationError(f\"Missing one or more required columns: {required_cols}\")\n",
        "\n",
        "    # --- Selection Logic ---\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Define boolean conditions for parent education comparison.\n",
        "    # These masks handle NaN values correctly (comparisons with NaN yield False).\n",
        "    cond_mother_gt = df_processed['parent_edu_mother'] > df_processed['parent_edu_father']\n",
        "    cond_father_gt = df_processed['parent_edu_father'] > df_processed['parent_edu_mother']\n",
        "    cond_equal = df_processed['parent_edu_mother'] == df_processed['parent_edu_father']\n",
        "\n",
        "    # Handle cases where only one parent's education is available.\n",
        "    cond_mother_only = df_processed['parent_edu_mother'].notna() & df_processed['parent_edu_father'].isna()\n",
        "    cond_father_only = df_processed['parent_edu_father'].notna() & df_processed['parent_edu_mother'].isna()\n",
        "\n",
        "    # Get the tie-breaking rule from the configuration.\n",
        "    tie_break_rule = config['parental_education_coding']['tie_break_rule']\n",
        "    if tie_break_rule == \"mother_if_equal\":\n",
        "        # Define the sequence of conditions for np.select.\n",
        "        conditions = [\n",
        "            cond_mother_gt,\n",
        "            cond_father_gt,\n",
        "            cond_equal,\n",
        "            cond_mother_only,\n",
        "            cond_father_only\n",
        "        ]\n",
        "        # Define the corresponding choices (which parent's COB to select).\n",
        "        choices = [\n",
        "            df_processed['parent_cob_mother'],\n",
        "            df_processed['parent_cob_father'],\n",
        "            df_processed['parent_cob_mother'], # Tie-break rule\n",
        "            df_processed['parent_cob_mother'],\n",
        "            df_processed['parent_cob_father']\n",
        "        ]\n",
        "    else:\n",
        "        # Future-proofing for other potential tie-break rules.\n",
        "        raise NotImplementedError(f\"Tie-break rule '{tie_break_rule}' is not implemented.\")\n",
        "\n",
        "    # Use np.select for efficient, vectorized conditional assignment.\n",
        "    # The default is np.nan, which is assigned if no condition is met.\n",
        "    df_processed['more_educated_parent_cob'] = np.select(conditions, choices, default=np.nan)\n",
        "\n",
        "    # --- Logging ---\n",
        "    # Count the number of rows where the COB could not be determined.\n",
        "    num_missing = df_processed['more_educated_parent_cob'].isna().sum()\n",
        "\n",
        "    return df_processed, int(num_missing)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Merge student data with Barro-Lee data.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _merge_with_instrument_data(\n",
        "    student_df: pd.DataFrame,\n",
        "    barro_lee_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Merges the student dataset with the Barro-Lee instrument data.\n",
        "\n",
        "    This function performs a left join, attaching the historical gender gap\n",
        "    data from the Barro-Lee dataset to each student record based on the\n",
        "    previously identified country of birth of the more-educated parent.\n",
        "\n",
        "    Args:\n",
        "        student_df: The student DataFrame, augmented with\n",
        "                    'more_educated_parent_cob'.\n",
        "        barro_lee_df: The validated Barro-Lee DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The merged DataFrame.\n",
        "        - Dict[str, int]: A log detailing the merge success rate.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(student_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(barro_lee_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'barro_lee_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Merge Preparation ---\n",
        "    # Defensive cleaning of join keys to prevent mismatches due to formatting.\n",
        "    student_df_copy = student_df.copy()\n",
        "    barro_lee_df_copy = barro_lee_df.copy()\n",
        "    student_df_copy['merge_key'] = student_df_copy['more_educated_parent_cob'].str.upper().str.strip()\n",
        "    barro_lee_df_copy['merge_key'] = barro_lee_df_copy['country_cob'].str.upper().str.strip()\n",
        "\n",
        "    # --- Merge Operation ---\n",
        "    # Perform a left merge to keep all students.\n",
        "    # The 'validate' parameter ensures the merge does not create duplicate rows.\n",
        "    merged_df = pd.merge(\n",
        "        student_df_copy,\n",
        "        barro_lee_df_copy[['merge_key', 'share_tertiary_women_1960', 'share_tertiary_men_1960']],\n",
        "        on='merge_key',\n",
        "        how='left',\n",
        "        validate='many_to_one'\n",
        "    )\n",
        "\n",
        "    # Drop the temporary merge key.\n",
        "    merged_df = merged_df.drop(columns=['merge_key'])\n",
        "\n",
        "    # --- Logging ---\n",
        "    # Calculate the number of students who successfully matched.\n",
        "    num_matched = merged_df['share_tertiary_women_1960'].notna().sum()\n",
        "    num_total = len(merged_df)\n",
        "    log = {\n",
        "        \"total_students_for_merge\": num_total,\n",
        "        \"students_matched_to_instrument\": int(num_matched),\n",
        "        \"students_unmatched\": num_total - int(num_matched)\n",
        "    }\n",
        "\n",
        "    return merged_df, log\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Compute the gender gap instrument (GG).\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_gender_gap_instrument(\n",
        "    merged_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Computes the gender gap (GG) instrumental variable.\n",
        "\n",
        "    This function calculates the difference between the share of women and men\n",
        "    with tertiary education in 1960, based on the merged Barro-Lee data.\n",
        "\n",
        "    Equation implemented: GG_i = share_tertiary_women_1960_i - share_tertiary_men_1960_i\n",
        "\n",
        "    Args:\n",
        "        merged_df: The DataFrame after merging with the Barro-Lee data.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the 'GG' column.\n",
        "        - int: The number of rows with a valid, non-null 'GG' value.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(merged_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'merged_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Computation ---\n",
        "    df_processed = merged_df.copy()\n",
        "\n",
        "    # Get the name for the instrument variable from the config.\n",
        "    gg_col_name = config['variable_definitions']['instrumental_variable']\n",
        "\n",
        "    # Calculate the gender gap. Pandas' vectorized subtraction handles NaNs correctly.\n",
        "    df_processed[gg_col_name] = (\n",
        "        df_processed['share_tertiary_women_1960'] - df_processed['share_tertiary_men_1960']\n",
        "    )\n",
        "\n",
        "    # --- Final Validation and Logging ---\n",
        "    # Verify that all computed GG values are within the logical range [-1.0, 1.0].\n",
        "    valid_gg = df_processed[gg_col_name].dropna()\n",
        "    if not valid_gg.between(-1.0, 1.0).all():\n",
        "        raise DataIntegrityError(\"Computed 'GG' values fall outside the logical range of [-1.0, 1.0].\")\n",
        "\n",
        "    # Count the number of valid, non-null GG values.\n",
        "    num_valid_gg = int(valid_gg.notna().sum())\n",
        "\n",
        "    return df_processed, num_valid_gg\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_instrumental_variable(\n",
        "    featured_df: pd.DataFrame,\n",
        "    barro_lee_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the instrumental variable (GG).\n",
        "\n",
        "    This function executes the full pipeline to create the gender gap instrument:\n",
        "    1. Selects the appropriate parent's country of birth.\n",
        "    2. Merges the student data with the external Barro-Lee dataset.\n",
        "    3. Computes the final 'GG' instrumental variable.\n",
        "\n",
        "    Args:\n",
        "        featured_df: The fully-featured student DataFrame from Task 7.\n",
        "        barro_lee_df: The validated Barro-Lee DataFrame from Task 3.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: The DataFrame augmented with the 'GG' column.\n",
        "        - Dict[str, Any]: A comprehensive log of the instrument construction\n",
        "                          process, including merge rates and final sample size.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(featured_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'featured_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(barro_lee_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'barro_lee_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Step 1: Select the more-educated parent's country of birth ---\n",
        "    df_step1, num_missing_cob = _select_instrument_country_of_birth(\n",
        "        df=featured_df,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Merge with the Barro-Lee instrument data ---\n",
        "    df_step2, merge_log = _merge_with_instrument_data(\n",
        "        student_df=df_step1,\n",
        "        barro_lee_df=barro_lee_df\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Compute the gender gap instrument (GG) ---\n",
        "    df_final, num_valid_gg = _compute_gender_gap_instrument(\n",
        "        merged_df=df_step2,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Compile Final Log ---\n",
        "    final_log = {\n",
        "        \"cob_selection\": {\n",
        "            \"rows_with_missing_instrument_cob\": num_missing_cob\n",
        "        },\n",
        "        \"merge_statistics\": merge_log,\n",
        "        \"instrument_computation\": {\n",
        "            \"final_rows_with_valid_gg\": num_valid_gg\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return df_final, final_log\n"
      ],
      "metadata": {
        "id": "2l0Ky-pI8g3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Define model-specific analysis samples with listwise deletion\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Define model-specific analysis samples with listwise deletion\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Generic Helper Function for Sample Creation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_analysis_sample(\n",
        "    df: pd.DataFrame,\n",
        "    required_cols: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a model-specific analysis sample by performing listwise deletion.\n",
        "\n",
        "    This function takes a DataFrame and a list of columns required for a\n",
        "    specific model. It removes all rows that have a missing value in any of\n",
        "    the specified columns, resulting in a complete-case analysis sample.\n",
        "\n",
        "    Args:\n",
        "        df: The source DataFrame from which to create the sample.\n",
        "        required_cols: A list of column names that must be non-missing for a\n",
        "                       row to be included in the final sample.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame containing only the complete cases for the specified\n",
        "        columns.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Check if all required columns exist in the DataFrame.\n",
        "    missing_in_df = set(required_cols) - set(df.columns)\n",
        "    if missing_in_df:\n",
        "        raise SchemaValidationError(\n",
        "            f\"Cannot create sample. The following required columns are missing \"\n",
        "            f\"from the DataFrame: {sorted(list(missing_in_df))}\"\n",
        "        )\n",
        "\n",
        "    # --- Listwise Deletion ---\n",
        "    # Use .dropna() with the specified subset of columns.\n",
        "    # .copy() is used to ensure the output is a new DataFrame, not a view.\n",
        "    analysis_sample = df.dropna(subset=required_cols).copy()\n",
        "\n",
        "    return analysis_sample\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def define_analysis_samples(\n",
        "    final_df: pd.DataFrame,\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of all model-specific analysis samples.\n",
        "\n",
        "    This function systematically generates all datasets required for the study's\n",
        "    various regression models (OLS, IV, pooled, by-grade, interaction). It\n",
        "    applies listwise deletion based on the specific variables needed for each\n",
        "    model, ensuring each analysis is run on the appropriate complete-case sample.\n",
        "\n",
        "    Args:\n",
        "        final_df: The fully feature-engineered DataFrame from Task 8.\n",
        "        control_vars_list: The list of all created dummy/control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - Dict[str, Any]: A nested dictionary holding all the generated\n",
        "                          analysis sample DataFrames.\n",
        "        - Dict[str, Any]: A nested dictionary logging the final sample size (N)\n",
        "                          for each generated sample.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(final_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'final_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(control_vars_list, list):\n",
        "        raise TypeError(\"Input 'control_vars_list' must be a list.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize dictionaries to store the output samples and their sizes.\n",
        "    samples: Dict[str, Any] = {\n",
        "        'ols': {'pooled': {}, 'interaction': {}},\n",
        "        'iv': {'pooled': {}, 'by_grade': {}, 'interaction': {}}\n",
        "    }\n",
        "    sample_logs: Dict[str, Any] = {\n",
        "        'ols': {'pooled': {}, 'interaction': {}},\n",
        "        'iv': {'pooled': {}, 'by_grade': {}, 'interaction': {}}\n",
        "    }\n",
        "\n",
        "    # Extract key variable names from the configuration for convenience.\n",
        "    dep_vars = config['variable_definitions']['dependent_variables']\n",
        "    phe_dummies = config['variable_definitions']['phe_dummy_names']\n",
        "    phe_var = config['variable_definitions']['endogenous_variable']\n",
        "    iv_var = config['variable_definitions']['instrumental_variable']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "\n",
        "    # --- Step 1: Define Pooled OLS Samples ---\n",
        "    base_ols_cols = phe_dummies + control_vars_list\n",
        "    for y_var in dep_vars:\n",
        "        required_cols = [y_var] + base_ols_cols\n",
        "        sample = _create_analysis_sample(final_df, required_cols)\n",
        "        samples['ols']['pooled'][y_var] = sample\n",
        "        sample_logs['ols']['pooled'][y_var] = len(sample)\n",
        "\n",
        "    # --- Step 2: Define Grade-Interaction OLS Samples ---\n",
        "    for y_var in dep_vars:\n",
        "        samples['ols']['interaction'][y_var] = {}\n",
        "        sample_logs['ols']['interaction'][y_var] = {}\n",
        "        for u, v in grade_pairs:\n",
        "            # Filter data to the relevant pair of grades.\n",
        "            grade_pair_df = final_df[final_df['grade'].isin([u, v])].copy()\n",
        "\n",
        "            # Create the interaction indicator and terms.\n",
        "            indicator = f\"I_{u}_{v}\"\n",
        "            grade_pair_df[indicator] = (grade_pair_df['grade'] == v).astype(float)\n",
        "            interaction_terms = []\n",
        "            for dummy in phe_dummies:\n",
        "                inter_term = f\"{dummy}_x_{indicator}\"\n",
        "                grade_pair_df[inter_term] = grade_pair_df[dummy] * grade_pair_df[indicator]\n",
        "                interaction_terms.append(inter_term)\n",
        "\n",
        "            # Define required columns and create the sample.\n",
        "            required_cols = [y_var] + base_ols_cols + [indicator] + interaction_terms\n",
        "            sample = _create_analysis_sample(grade_pair_df, required_cols)\n",
        "            samples['ols']['interaction'][y_var][(u, v)] = sample\n",
        "            sample_logs['ols']['interaction'][y_var][(u, v)] = len(sample)\n",
        "\n",
        "    # --- Step 3: Define IV Samples ---\n",
        "    base_iv_cols = [phe_var, iv_var] + control_vars_list\n",
        "\n",
        "    # Pooled IV Samples\n",
        "    for y_var in dep_vars:\n",
        "        required_cols = [y_var] + base_iv_cols\n",
        "        sample = _create_analysis_sample(final_df, required_cols)\n",
        "        samples['iv']['pooled'][y_var] = sample\n",
        "        sample_logs['iv']['pooled'][y_var] = len(sample)\n",
        "\n",
        "    # By-Grade IV Samples\n",
        "    for y_var in dep_vars:\n",
        "        samples['iv']['by_grade'][y_var] = {}\n",
        "        sample_logs['iv']['by_grade'][y_var] = {}\n",
        "        for grade in grades:\n",
        "            grade_df = final_df[final_df['grade'] == grade]\n",
        "            required_cols = [y_var] + base_iv_cols\n",
        "            sample = _create_analysis_sample(grade_df, required_cols)\n",
        "            samples['iv']['by_grade'][y_var][grade] = sample\n",
        "            sample_logs['iv']['by_grade'][y_var][grade] = len(sample)\n",
        "\n",
        "    # Interaction IV Samples\n",
        "    for y_var in dep_vars:\n",
        "        samples['iv']['interaction'][y_var] = {}\n",
        "        sample_logs['iv']['interaction'][y_var] = {}\n",
        "        for u, v in grade_pairs:\n",
        "            # Filter data to the relevant pair of grades.\n",
        "            grade_pair_df = final_df[final_df['grade'].isin([u, v])].copy()\n",
        "\n",
        "            # Create the interaction indicator and term for the endogenous variable.\n",
        "            indicator = f\"I_{u}_{v}\"\n",
        "            inter_term = f\"{phe_var}_x_{indicator}\"\n",
        "            grade_pair_df[indicator] = (grade_pair_df['grade'] == v).astype(float)\n",
        "            grade_pair_df[inter_term] = grade_pair_df[phe_var] * grade_pair_df[indicator]\n",
        "\n",
        "            # Define required columns and create the sample.\n",
        "            required_cols = [y_var] + base_iv_cols + [indicator, inter_term]\n",
        "            sample = _create_analysis_sample(grade_pair_df, required_cols)\n",
        "            samples['iv']['interaction'][y_var][(u, v)] = sample\n",
        "            sample_logs['iv']['interaction'][y_var][(u, v)] = len(sample)\n",
        "\n",
        "    return samples, sample_logs\n"
      ],
      "metadata": {
        "id": "hYZJXlF29g8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Specify and document the OLS model equations and fixed-effects structure\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Specify and document the OLS model equations and fixed-effects\n",
        "#          structure\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Specify the pooled OLS model.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _specify_pooled_ols_model(\n",
        "    dependent_var: str,\n",
        "    control_vars: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines the specification for a pooled OLS regression model.\n",
        "\n",
        "    This function programmatically constructs a dictionary that contains all the\n",
        "    necessary components to define the pooled OLS model from Equation (1) of\n",
        "    the study. This includes the dependent variable, exogenous variables (PHE\n",
        "    dummies and all controls), fixed effects, and clustering variable.\n",
        "\n",
        "    Equation (1): y_its = α₀ + Σ αⱼpⱼᵢ + Controls + aₜ + bₛ + εᵢₜₛ\n",
        "\n",
        "    Args:\n",
        "        dependent_var: The name of the dependent variable (e.g., 'Score_Math').\n",
        "        control_vars: A list of all control variable names (dummies).\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the complete model specification.\n",
        "    \"\"\"\n",
        "    # --- Specification Construction ---\n",
        "    # Extract the names of the PHE dummy variables from the config.\n",
        "    phe_dummies = config['variable_definitions']['phe_dummy_names']\n",
        "\n",
        "    # Combine PHE dummies and all other control variables into one list of regressors.\n",
        "    exog_vars = phe_dummies + control_vars\n",
        "\n",
        "    # Assemble the full model specification into a structured dictionary.\n",
        "    specification = {\n",
        "        \"dependent\": dependent_var,\n",
        "        \"exog\": exog_vars,\n",
        "        \"fixed_effects\": config['model_specifications']['fixed_effects'],\n",
        "        \"cluster_by\": config['inference_parameters']['standard_error_clustering_level'],\n",
        "        \"model_type\": \"Pooled OLS (Persistent Effect)\"\n",
        "    }\n",
        "\n",
        "    return specification\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Specify the OLS model with grade-by-PHE interactions.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _specify_interaction_ols_model(\n",
        "    dependent_var: str,\n",
        "    grade_pair: Tuple[int, int],\n",
        "    control_vars: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines the specification for an OLS regression with interaction terms.\n",
        "\n",
        "    This function constructs the specification for the Matthew Effect test model\n",
        "    from Equation (2) of the study. It includes main effects, a grade indicator,\n",
        "    and interaction terms between the grade indicator and PHE dummies.\n",
        "\n",
        "    Equation (2): yᵢgₜₛ = α₀ + Σ αⱼpⱼᵢ + ηI(u,v) + Σ θⱼ(pⱼᵢ·I(u,v)) + ...\n",
        "\n",
        "    Args:\n",
        "        dependent_var: The name of the dependent variable.\n",
        "        grade_pair: A tuple (u, v) representing the reference and comparison grades.\n",
        "        control_vars: A list of all control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the complete interaction model specification.\n",
        "    \"\"\"\n",
        "    # --- Specification Construction ---\n",
        "    # Unpack the grade pair.\n",
        "    u, v = grade_pair\n",
        "\n",
        "    # Get the base list of PHE dummies.\n",
        "    phe_dummies = config['variable_definitions']['phe_dummy_names']\n",
        "\n",
        "    # Programmatically define the names for the indicator and interaction terms.\n",
        "    # This matches the naming convention used in Task 9.\n",
        "    indicator = f\"I_{u}_{v}\"\n",
        "    interaction_terms = [f\"{dummy}_x_{indicator}\" for dummy in phe_dummies]\n",
        "\n",
        "    # Combine all regressors: PHE dummies, controls, indicator, and interactions.\n",
        "    exog_vars = phe_dummies + control_vars + [indicator] + interaction_terms\n",
        "\n",
        "    # Assemble the full model specification.\n",
        "    specification = {\n",
        "        \"dependent\": dependent_var,\n",
        "        \"exog\": exog_vars,\n",
        "        \"fixed_effects\": config['model_specifications']['fixed_effects'],\n",
        "        \"cluster_by\": config['inference_parameters']['standard_error_clustering_level'],\n",
        "        \"model_type\": f\"Interaction OLS (Matthew Effect Test, Grades {u} vs {v})\"\n",
        "    }\n",
        "\n",
        "    return specification\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Specify OLS models for effort and parental investment.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _specify_effort_investment_ols_model(\n",
        "    dependent_var: str,\n",
        "    grade: int,\n",
        "    control_vars: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines the specification for by-grade effort and investment models.\n",
        "\n",
        "    This function constructs the specification for the models that analyze the\n",
        "    impact of student effort and parental investment within a specific subject\n",
        "    and grade. The model includes PHE dummies and the full set of controls.\n",
        "\n",
        "    Args:\n",
        "        dependent_var: The name of the dependent variable.\n",
        "        grade: The specific grade for this model (3, 6, or 10).\n",
        "        control_vars: A list of all control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the complete model specification.\n",
        "    \"\"\"\n",
        "    # --- Specification Construction ---\n",
        "    # This model uses the same regressor structure as the pooled OLS.\n",
        "    phe_dummies = config['variable_definitions']['phe_dummy_names']\n",
        "    exog_vars = phe_dummies + control_vars\n",
        "\n",
        "    # Assemble the full model specification.\n",
        "    specification = {\n",
        "        \"dependent\": dependent_var,\n",
        "        \"exog\": exog_vars,\n",
        "        \"fixed_effects\": config['model_specifications']['fixed_effects'],\n",
        "        \"cluster_by\": config['inference_parameters']['standard_error_clustering_level'],\n",
        "        \"model_type\": f\"Effort/Investment OLS (Grade {grade})\"\n",
        "    }\n",
        "\n",
        "    return specification\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def specify_ols_models(\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the specification of all OLS models for the study.\n",
        "\n",
        "    This function serves as a factory for creating the precise definitions of\n",
        "    every OLS regression to be estimated. It does not run the models but\n",
        "    produces a structured dictionary of specifications that will drive the\n",
        "    estimation tasks, ensuring consistency and fidelity to the research design.\n",
        "\n",
        "    Args:\n",
        "        control_vars_list: The comprehensive list of all created control\n",
        "                           variable names from Task 7.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the complete specifications for all\n",
        "        pooled, interaction, and by-grade effort/investment OLS models.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(control_vars_list, list):\n",
        "        raise TypeError(\"Input 'control_vars_list' must be a list.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize the main dictionary to store all model specifications.\n",
        "    specifications: Dict[str, Any] = {\n",
        "        'pooled': {},\n",
        "        'interaction': {},\n",
        "        'effort_investment': {}\n",
        "    }\n",
        "\n",
        "    # Extract key metadata from the configuration.\n",
        "    dep_vars = config['variable_definitions']['dependent_variables']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "\n",
        "    # --- Step 1: Generate Pooled OLS Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['pooled'][y_var] = _specify_pooled_ols_model(\n",
        "            dependent_var=y_var,\n",
        "            control_vars=control_vars_list,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Generate Interaction OLS Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['interaction'][y_var] = {}\n",
        "        for pair in grade_pairs:\n",
        "            specifications['interaction'][y_var][pair] = _specify_interaction_ols_model(\n",
        "                dependent_var=y_var,\n",
        "                grade_pair=pair,\n",
        "                control_vars=control_vars_list,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "    # --- Step 3: Generate Effort/Investment OLS Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['effort_investment'][y_var] = {}\n",
        "        for grade in grades:\n",
        "            # The study design does not assess Grade 10 in 2016, but the model\n",
        "            # specification is still valid for other years. The sample creation\n",
        "            # in Task 9 handles the data filtering.\n",
        "            specifications['effort_investment'][y_var][grade] = _specify_effort_investment_ols_model(\n",
        "                dependent_var=y_var,\n",
        "                grade=grade,\n",
        "                control_vars=control_vars_list,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "    return specifications\n"
      ],
      "metadata": {
        "id": "01r-Noib-X1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Specify and document the IV (2SLS) model equations and diagnostics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Specify and document the IV (2SLS) model equations and diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Generic Helper Function for IV Model Specification\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _specify_iv_model_components(\n",
        "    dependent_var: str,\n",
        "    control_vars: List[str],\n",
        "    config: Dict[str, Any],\n",
        "    grade_pair: Tuple[int, int] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines the specifications for all components of a 2SLS model.\n",
        "\n",
        "    This function programmatically constructs a dictionary containing the precise\n",
        "    definitions for the first-stage, reduced-form, and second-stage regressions\n",
        "    of a complete IV analysis, including handling for interaction terms.\n",
        "\n",
        "    Args:\n",
        "        dependent_var: The name of the final outcome variable (for the 2nd stage).\n",
        "        control_vars: A list of all exogenous control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "        grade_pair: Optional tuple (u, v) for interaction models.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the specifications for 'first_stage',\n",
        "        'reduced_form', and 'second_stage'.\n",
        "    \"\"\"\n",
        "    # --- Variable Definitions from Config ---\n",
        "    # Extract names for endogenous, instrumental, and fixed effect variables.\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']  # 'PHE'\n",
        "    instrument_var = config['variable_definitions']['instrumental_variable']  # 'GG'\n",
        "    fixed_effects = config['model_specifications']['fixed_effects']\n",
        "    cluster_by = config['inference_parameters']['standard_error_clustering_level']\n",
        "\n",
        "    # --- Base Specification Components ---\n",
        "    # Exogenous regressors are the controls. The instrument is handled separately.\n",
        "    exog_controls = control_vars\n",
        "\n",
        "    # --- Handle Interaction Terms (if applicable) ---\n",
        "    if grade_pair:\n",
        "        u, v = grade_pair\n",
        "        indicator = f\"I_{u}_{v}\"\n",
        "\n",
        "        # For interaction models, the indicator is an additional exogenous control.\n",
        "        exog_controls_with_indicator = exog_controls + [indicator]\n",
        "\n",
        "        # The endogenous variable and its interaction.\n",
        "        endog_vars_interaction = [endog_var, f\"{endog_var}_x_{indicator}\"]\n",
        "\n",
        "        # The instrument and its interaction with the indicator.\n",
        "        instruments_interaction = [instrument_var, f\"{instrument_var}_x_{indicator}\"]\n",
        "    else:\n",
        "        # For non-interaction models, the lists are simpler.\n",
        "        exog_controls_with_indicator = exog_controls\n",
        "        endog_vars_interaction = [endog_var]\n",
        "        instruments_interaction = [instrument_var]\n",
        "\n",
        "    # --- Step 1: Specify the First-Stage Regression ---\n",
        "    # Regresses the endogenous variable(s) on instruments and all exogenous controls.\n",
        "    # Equation (3): PHEᵢ = π₀ + π₁GGᵢ + Controls + aₜ + bₛ + νᵢₜₛ\n",
        "    first_stage_spec = {\n",
        "        \"dependent\": endog_vars_interaction, # Dependent var(s) of the first stage\n",
        "        \"exog\": instruments_interaction + exog_controls_with_indicator,\n",
        "        \"fixed_effects\": fixed_effects,\n",
        "        \"cluster_by\": cluster_by,\n",
        "        \"model_type\": \"IV First Stage\"\n",
        "    }\n",
        "\n",
        "    # --- Step 2: Specify the Reduced-Form Regression ---\n",
        "    # Regresses the final outcome on the instrument(s) and all exogenous controls.\n",
        "    # Equation: yᵢₜₛ = ρ₀ + ρ₁GGᵢ + Controls + aₜ + bₛ + ωᵢₜₛ\n",
        "    reduced_form_spec = {\n",
        "        \"dependent\": dependent_var,\n",
        "        \"exog\": instruments_interaction + exog_controls_with_indicator,\n",
        "        \"fixed_effects\": fixed_effects,\n",
        "        \"cluster_by\": cluster_by,\n",
        "        \"model_type\": \"IV Reduced Form (Diagnostic)\"\n",
        "    }\n",
        "\n",
        "    # --- Step 3: Specify the Second-Stage Regression ---\n",
        "    # Defines the main causal model for a 2SLS estimator.\n",
        "    # Equation (4): yᵢₜₛ = α₀ + α₁P̂HEᵢ + Controls + aₜ + bₛ + εᵢₜₛ\n",
        "    # Equation (5): yᵢgₜₛ = α₀ + α₁P̂HEᵢ + ηI(u,v) + α₂(P̂HEᵢ·I(u,v)) + ...\n",
        "    second_stage_spec = {\n",
        "        \"dependent\": dependent_var,\n",
        "        \"exog\": exog_controls_with_indicator, # Exogenous controls only\n",
        "        \"endog\": endog_vars_interaction,     # Endogenous variable(s)\n",
        "        \"instruments\": instruments_interaction, # Instrumental variable(s)\n",
        "        \"fixed_effects\": fixed_effects,\n",
        "        \"cluster_by\": cluster_by,\n",
        "        \"model_type\": \"IV Second Stage (Causal Effect)\"\n",
        "    }\n",
        "\n",
        "    # --- Assemble Final Specification ---\n",
        "    # Combine all three components into a single dictionary for the IV model.\n",
        "    iv_model_specification = {\n",
        "        \"first_stage\": first_stage_spec,\n",
        "        \"reduced_form\": reduced_form_spec,\n",
        "        \"second_stage\": second_stage_spec\n",
        "    }\n",
        "\n",
        "    return iv_model_specification\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def specify_iv_models(\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the specification of all IV (2SLS) models for the study.\n",
        "\n",
        "    This function acts as a factory for creating the precise definitions of\n",
        "    every IV regression and its associated diagnostics (first-stage and\n",
        "    reduced-form). It produces a structured dictionary of specifications that\n",
        "    will drive the causal estimation tasks.\n",
        "\n",
        "    Args:\n",
        "        control_vars_list: The comprehensive list of all created control\n",
        "                           variable names from Task 7.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary containing the complete specifications for all\n",
        "        pooled, by-grade, and interaction IV models.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(control_vars_list, list):\n",
        "        raise TypeError(\"Input 'control_vars_list' must be a list.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize the main dictionary to store all IV model specifications.\n",
        "    specifications: Dict[str, Any] = {\n",
        "        'pooled': {},\n",
        "        'by_grade': {},\n",
        "        'interaction': {}\n",
        "    }\n",
        "\n",
        "    # Extract key metadata from the configuration.\n",
        "    dep_vars = config['variable_definitions']['dependent_variables']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "\n",
        "    # --- Generate Pooled IV Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['pooled'][y_var] = _specify_iv_model_components(\n",
        "            dependent_var=y_var,\n",
        "            control_vars=control_vars_list,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "    # --- Generate By-Grade IV Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['by_grade'][y_var] = {}\n",
        "        for grade in grades:\n",
        "            specifications['by_grade'][y_var][grade] = _specify_iv_model_components(\n",
        "                dependent_var=y_var,\n",
        "                control_vars=control_vars_list,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "    # --- Generate Interaction IV Specifications ---\n",
        "    for y_var in dep_vars:\n",
        "        specifications['interaction'][y_var] = {}\n",
        "        for pair in grade_pairs:\n",
        "            specifications['interaction'][y_var][pair] = _specify_iv_model_components(\n",
        "                dependent_var=y_var,\n",
        "                control_vars=control_vars_list,\n",
        "                config=config,\n",
        "                grade_pair=pair\n",
        "            )\n",
        "\n",
        "    return specifications\n"
      ],
      "metadata": {
        "id": "RTsAQsdv_hyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Implement cluster-robust standard error computation at the school level\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Implement the generic estimation engine for models with fixed\n",
        "#          effects and cluster-robust standard errors\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator and Implementation Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_model(\n",
        "    analysis_df: pd.DataFrame,\n",
        "    model_spec: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Union[PanelEffectsResults, IVPanelEffectsResults]:\n",
        "    \"\"\"\n",
        "    Estimates a specified regression model with fixed effects and clustered SEs.\n",
        "\n",
        "    This function serves as a generic estimation engine for the entire study.\n",
        "    It takes a model specification and a corresponding analysis sample, then\n",
        "    estimates the appropriate model (PanelOLS or IV2SLS) using the `linearmodels`\n",
        "    library. It automatically handles high-dimensional fixed effects (school and\n",
        "    year) and computes standard errors clustered at the school level, precisely\n",
        "    implementing the study's econometric methodology.\n",
        "\n",
        "    Methodology:\n",
        "    1.  **Panel Data Setup**: The input DataFrame is indexed by the entity\n",
        "        (school_id) and time (academic_year) variables to create a panel structure.\n",
        "    2.  **Fixed Effects**: The `entity_effects=True` and `time_effects=True`\n",
        "        arguments instruct the estimator to absorb the school and year fixed\n",
        "        effects using a within-transformation (de-meaning), which is\n",
        "        computationally efficient and numerically stable (Frisch-Waugh-Lovell).\n",
        "    3.  **Cluster-Robust SE**: The `.fit()` method is configured with\n",
        "        `cov_type='clustered'` and `cluster_entity=True` to compute the\n",
        "        cluster-robust variance-covariance matrix, which is robust to arbitrary\n",
        "        heteroskedasticity and within-school correlation of errors.\n",
        "        The formula is: V_cluster = (X'X)⁻¹(Σ X_g'û_gû_g'X_g)(X'X)⁻¹\n",
        "\n",
        "    Args:\n",
        "        analysis_df: The model-specific, complete-case analysis sample DataFrame.\n",
        "        model_spec: A dictionary defining the model (dependent, exog, endog, etc.).\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The fitted model result object from the `linearmodels` library, which\n",
        "        contains all regression outputs (coefficients, SEs, p-values, etc.).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model specification is invalid or incomplete.\n",
        "        TypeError: If input types are incorrect.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'analysis_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(model_spec, dict):\n",
        "        raise TypeError(\"Input 'model_spec' must be a dictionary.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Data Preparation for Panel Model ---\n",
        "    # Create a copy to avoid modifying the original sample DataFrame.\n",
        "    df = analysis_df.copy()\n",
        "\n",
        "    # Extract fixed effect and cluster variable names from the specification.\n",
        "    entity_col, time_col = model_spec['fixed_effects']\n",
        "    cluster_col = model_spec['cluster_by']\n",
        "\n",
        "    # The linearmodels library requires a MultiIndex of (entity, time).\n",
        "    # Verify the cluster variable is the same as the entity variable.\n",
        "    if cluster_col != entity_col:\n",
        "        raise ConfigurationError(\n",
        "            f\"Clustering variable '{cluster_col}' must be the same as the \"\n",
        "            f\"entity variable '{entity_col}' for this implementation.\"\n",
        "        )\n",
        "\n",
        "    # Set the panel index.\n",
        "    df = df.set_index([entity_col, time_col])\n",
        "\n",
        "    # --- Model Specification Parsing ---\n",
        "    # Extract dependent and exogenous variables from the specification.\n",
        "    dependent = df[[model_spec['dependent']]] if isinstance(model_spec['dependent'], str) else df[model_spec['dependent']]\n",
        "    exog = df[model_spec['exog']]\n",
        "\n",
        "    # Add a constant to the exogenous variables, which is required by linearmodels.\n",
        "    # The fixed effects absorption will correctly handle the intercept.\n",
        "    exog['const'] = 1.0\n",
        "\n",
        "    # --- Model Estimation ---\n",
        "    # Determine whether to run an IV or OLS model based on the specification keys.\n",
        "    is_iv_model = 'endog' in model_spec and 'instruments' in model_spec\n",
        "\n",
        "    if is_iv_model:\n",
        "        # This is a 2SLS model.\n",
        "        endog = df[model_spec['endog']]\n",
        "        instruments = df[model_spec['instruments']]\n",
        "\n",
        "        # Instantiate the IV2SLS model with fixed effects.\n",
        "        model = IV2SLS(\n",
        "            dependent=dependent,\n",
        "            exog=exog,\n",
        "            endog=endog,\n",
        "            instruments=instruments,\n",
        "            entity_effects=True,\n",
        "            time_effects=True\n",
        "        )\n",
        "    else:\n",
        "        # This is a standard OLS model.\n",
        "        # Instantiate the PanelOLS model with fixed effects.\n",
        "        model = PanelOLS(\n",
        "            dependent=dependent,\n",
        "            exog=exog,\n",
        "            entity_effects=True,\n",
        "            time_effects=True\n",
        "        )\n",
        "\n",
        "    # --- Fit the Model with Cluster-Robust Standard Errors ---\n",
        "    # The configuration specifies the covariance type and clustering.\n",
        "    # `cov_type='clustered'` enables robust standard errors.\n",
        "    # `cluster_entity=True` specifies clustering on the entity index (school_id).\n",
        "    # `debiased=True` corresponds to the HC1 small-sample adjustment.\n",
        "    results = model.fit(\n",
        "        cov_type='clustered',\n",
        "        cluster_entity=True,\n",
        "        debiased=config['inference_parameters']['robust_se_type'] == 'HC1'\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "DnaVqkwtAnHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Estimate pooled OLS models with FE and extract persistent-effect coefficients\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Estimate pooled OLS models with FE and extract persistent-effect\n",
        "#          coefficients\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Helper function to extract and format model results.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_significance_stars(p_value: float, config: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Assigns significance stars to a p-value based on configured thresholds.\n",
        "\n",
        "    Args:\n",
        "        p_value: The p-value of a coefficient estimate.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the appropriate significance stars ('***', '**', '*').\n",
        "    \"\"\"\n",
        "    # Extract significance levels from the configuration.\n",
        "    levels = config['inference_parameters']['significance_levels']\n",
        "    # Determine the star rating based on the p-value.\n",
        "    if p_value < levels['star_3']:\n",
        "        return '***'\n",
        "    elif p_value < levels['star_2']:\n",
        "        return '**'\n",
        "    elif p_value < levels['star_1']:\n",
        "        return '*'\n",
        "    return ''\n",
        "\n",
        "def _extract_result_summary(\n",
        "    results: PanelEffectsResults,\n",
        "    target_coeffs: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts, formats, and summarizes key statistics from a fitted model.\n",
        "\n",
        "    This function processes a `linearmodels` result object to create a clean,\n",
        "    publication-ready summary table. It focuses on specified target\n",
        "    coefficients and includes essential model statistics.\n",
        "\n",
        "    Args:\n",
        "        results: The fitted model object from `linearmodels`.\n",
        "        target_coeffs: A list of coefficient names to include in the summary.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the regression results.\n",
        "    \"\"\"\n",
        "    # --- Result Extraction ---\n",
        "    # Initialize a list to hold dictionaries of results for each coefficient.\n",
        "    summary_data = []\n",
        "\n",
        "    # Iterate through the target coefficients to extract their statistics.\n",
        "    for coeff in target_coeffs:\n",
        "        # Extract core statistics directly from the results object attributes.\n",
        "        estimate = results.params[coeff]\n",
        "        std_err = results.std_errors[coeff]\n",
        "        p_value = results.pvalues[coeff]\n",
        "        t_stat = results.tstats[coeff]\n",
        "\n",
        "        # Calculate the 95% confidence interval.\n",
        "        ci_lower = estimate - 1.96 * std_err\n",
        "        ci_upper = estimate + 1.96 * std_err\n",
        "\n",
        "        # Get significance stars.\n",
        "        stars = _get_significance_stars(p_value, config)\n",
        "\n",
        "        # Append the formatted results to the list.\n",
        "        summary_data.append({\n",
        "            'Coefficient': coeff,\n",
        "            'Estimate': estimate,\n",
        "            'Std. Error': std_err,\n",
        "            'T-stat': t_stat,\n",
        "            'P-value': p_value,\n",
        "            'CI Lower': ci_lower,\n",
        "            'CI Upper': ci_upper,\n",
        "            'Significance': stars\n",
        "        })\n",
        "\n",
        "    # --- DataFrame Creation and Formatting ---\n",
        "    # Convert the list of dictionaries to a pandas DataFrame.\n",
        "    summary_df = pd.DataFrame(summary_data).set_index('Coefficient')\n",
        "\n",
        "    # --- Add Model-Level Statistics ---\n",
        "    # Add key model statistics for context and reporting.\n",
        "    summary_df['N'] = int(results.nobs)\n",
        "    summary_df['Num_Clusters'] = int(results.n_clusters['cluster_entity'])\n",
        "    summary_df['R2_Within'] = results.rsquared_within\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_pooled_ols_estimation(\n",
        "    analysis_samples: Dict[str, pd.DataFrame],\n",
        "    model_specifications: Dict[str, Dict[str, Any]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of pooled OLS models for all subjects.\n",
        "\n",
        "    This function iterates through each subject (dependent variable), retrieves\n",
        "    the appropriate analysis sample and model specification, runs the\n",
        "    regression using the generic estimation engine, extracts the results, and\n",
        "    performs validation checks.\n",
        "\n",
        "    Args:\n",
        "        analysis_samples: A dictionary of the pooled OLS analysis samples,\n",
        "                          keyed by dependent variable name.\n",
        "        model_specifications: A dictionary of the pooled OLS model\n",
        "                              specifications, keyed by dependent variable name.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are dependent variable names and values are\n",
        "        pandas DataFrames containing the formatted regression results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(analysis_samples, dict):\n",
        "        raise TypeError(\"Input 'analysis_samples' must be a dictionary.\")\n",
        "    if not isinstance(model_specifications, dict):\n",
        "        raise TypeError(\"Input 'model_specifications' must be a dictionary.\")\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    # Initialize a dictionary to store the final results tables.\n",
        "    all_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # Get the list of dependent variables to iterate over.\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "\n",
        "    # Get the names of the key coefficients of interest.\n",
        "    target_coeffs = config['variable_definitions']['phe_dummy_names']\n",
        "\n",
        "    # Loop through each subject (e.g., 'Score_Math', 'Score_Lit', 'Score_Eng').\n",
        "    for y_var in dependent_vars:\n",
        "        print(f\"--- Estimating Pooled OLS for: {y_var} ---\")\n",
        "\n",
        "        # --- Step 1: Fit the pooled OLS model ---\n",
        "        # Retrieve the correct sample and specification for the current subject.\n",
        "        sample = analysis_samples[y_var]\n",
        "        spec = model_specifications[y_var]\n",
        "\n",
        "        # Estimate the model using the generic estimation engine from Task 12.\n",
        "        try:\n",
        "            results = estimate_model(\n",
        "                analysis_df=sample,\n",
        "                model_spec=spec,\n",
        "                config=config\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Estimation failed for Pooled OLS on {y_var}.\") from e\n",
        "\n",
        "        # --- Step 2: Extract and format results ---\n",
        "        # Process the results object into a clean summary table.\n",
        "        summary_table = _extract_result_summary(\n",
        "            results=results,\n",
        "            target_coeffs=target_coeffs,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # Store the final summary table.\n",
        "        all_results[y_var] = summary_table\n",
        "        print(f\"Estimation successful for {y_var}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected sign and magnitude patterns ---\n",
        "    # This cross-model validation is performed after all models are estimated.\n",
        "    print(\"\\n--- Validating Pooled OLS Results Patterns ---\")\n",
        "\n",
        "    # Check that coefficients are positive for all subjects.\n",
        "    for y_var, res_df in all_results.items():\n",
        "        for coeff in target_coeffs:\n",
        "            estimate = res_df.loc[coeff, 'Estimate']\n",
        "            if estimate <= 0:\n",
        "                warnings.warn(\n",
        "                    f\"ValidationWarning: Coefficient '{coeff}' for subject '{y_var}' \"\n",
        "                    f\"is not positive as expected (Estimate: {estimate:.3f}).\",\n",
        "                    UserWarning\n",
        "                )\n",
        "\n",
        "    # Check the expected ordering of effect sizes (Eng > Lit > Math).\n",
        "    try:\n",
        "        p2_coeff_name = target_coeffs[1] # Assumes 'p2' is the second dummy\n",
        "        eng_eff = all_results['Score_Eng'].loc[p2_coeff_name, 'Estimate']\n",
        "        lit_eff = all_results['Score_Lit'].loc[p2_coeff_name, 'Estimate']\n",
        "        math_eff = all_results['Score_Math'].loc[p2_coeff_name, 'Estimate']\n",
        "\n",
        "        if not (eng_eff > lit_eff > math_eff):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: The expected ordering of effect sizes \"\n",
        "                f\"(Eng > Lit > Math) for '{p2_coeff_name}' was not observed. \"\n",
        "                f\"Found: Eng={eng_eff:.2f}, Lit={lit_eff:.2f}, Math={math_eff:.2f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "        else:\n",
        "            print(\"Result patterns validated successfully.\")\n",
        "    except (KeyError, IndexError):\n",
        "        warnings.warn(\"Could not perform pattern validation due to missing results.\", UserWarning)\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "V0krQzKfB44F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Estimate OLS models with grade-by-PHE interactions and extract Matthew-effect coefficients\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Estimate OLS models with grade-by-PHE interactions and extract\n",
        "#          Matthew-effect coefficients\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_interaction_ols_estimation(\n",
        "    interaction_samples: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    interaction_specifications: Dict[str, Dict[Tuple[int, int], Dict[str, Any]]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[Tuple[int, int], pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of OLS models with grade-by-PHE interactions.\n",
        "\n",
        "    This function systematically estimates the 9 models required to test for the\n",
        "    Matthew Effect (one for each combination of 3 subjects and 3 grade-pair\n",
        "    transitions). It uses the generic estimation engine and results extractor\n",
        "    and concludes by validating the results against the study's key findings.\n",
        "\n",
        "    Args:\n",
        "        interaction_samples: A nested dictionary of the interaction analysis\n",
        "                             samples from Task 9.\n",
        "        interaction_specifications: A nested dictionary of the interaction model\n",
        "                                    specifications from Task 10.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary where keys are subject and grade-pair, and values\n",
        "        are pandas DataFrames with the formatted regression results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(interaction_samples, dict):\n",
        "        raise TypeError(\"Input 'interaction_samples' must be a dictionary.\")\n",
        "    if not isinstance(interaction_specifications, dict):\n",
        "        raise TypeError(\"Input 'interaction_specifications' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Initialize a nested dictionary to store the final results tables.\n",
        "    all_results: Dict[str, Dict[Tuple[int, int], pd.DataFrame]] = {}\n",
        "\n",
        "    # Extract key metadata from the configuration.\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    phe_dummies = config['variable_definitions']['phe_dummy_names']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    # Use nested loops to iterate through every subject and grade-pair combination.\n",
        "    for y_var in dependent_vars:\n",
        "        # Initialize the inner dictionary for the current subject.\n",
        "        all_results[y_var] = {}\n",
        "\n",
        "        for pair in grade_pairs:\n",
        "            u, v = pair\n",
        "            print(f\"--- Estimating Interaction OLS for: {y_var}, Grades {u} vs {v} ---\")\n",
        "\n",
        "            # --- Step 1: Fit the grade-interaction OLS model ---\n",
        "            # Retrieve the correct sample and specification for the current model.\n",
        "            try:\n",
        "                sample = interaction_samples[y_var][pair]\n",
        "                spec = interaction_specifications[y_var][pair]\n",
        "            except KeyError:\n",
        "                raise ConfigurationError(\n",
        "                    f\"Missing sample or specification for subject '{y_var}' \"\n",
        "                    f\"and grade pair {pair}.\"\n",
        "                )\n",
        "\n",
        "            # Estimate the model using the generic estimation engine.\n",
        "            try:\n",
        "                results = estimate_model(\n",
        "                    analysis_df=sample,\n",
        "                    model_spec=spec,\n",
        "                    config=config\n",
        "                )\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(\n",
        "                    f\"Estimation failed for Interaction OLS on {y_var}, \"\n",
        "                    f\"Grades {u} vs {v}.\"\n",
        "                ) from e\n",
        "\n",
        "            # --- Step 2: Extract and format results for interaction terms ---\n",
        "            # Programmatically generate the names of the interaction coefficients.\n",
        "            indicator = f\"I_{u}_{v}\"\n",
        "            interaction_coeffs = [f\"{dummy}_x_{indicator}\" for dummy in phe_dummies]\n",
        "\n",
        "            # The target coefficients are the base effects and the interactions.\n",
        "            target_coeffs = phe_dummies + interaction_coeffs\n",
        "\n",
        "            # Process the results object into a clean summary table.\n",
        "            summary_table = _extract_result_summary(\n",
        "                results=results,\n",
        "                target_coeffs=target_coeffs,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Store the final summary table in the nested dictionary.\n",
        "            all_results[y_var][pair] = summary_table\n",
        "            print(f\"Estimation successful for {y_var}, Grades {u} vs {v}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected Matthew-effect patterns by subject ---\n",
        "    print(\"\\n--- Validating Interaction OLS Results Patterns (Matthew Effect) ---\")\n",
        "\n",
        "    # Define the significance threshold for validation checks.\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1'] # e.g., 0.10\n",
        "\n",
        "    # Get the name of the interaction term for the highest education group (PHE=2).\n",
        "    p2_dummy_name = phe_dummies[1] # Assumes 'p2' is the second dummy.\n",
        "\n",
        "    try:\n",
        "        # Check Mathematics: Expect negative effect for 6 vs 10.\n",
        "        math_inter_6_10 = all_results['Score_Math'][(6, 10)]\n",
        "        math_coeff_name = f\"{p2_dummy_name}_x_I_6_10\"\n",
        "        math_est = math_inter_6_10.loc[math_coeff_name, 'Estimate']\n",
        "        math_pval = math_inter_6_10.loc[math_coeff_name, 'P-value']\n",
        "        if not (math_est < 0 and math_pval < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Mathematics (6 vs 10) did not show the expected \"\n",
        "                f\"significant negative interaction. Found Est={math_est:.3f}, P-val={math_pval:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # Check Literature: Expect positive for 3 vs 6, negative for 6 vs 10.\n",
        "        lit_inter_3_6 = all_results['Score_Lit'][(3, 6)]\n",
        "        lit_coeff_name_3_6 = f\"{p2_dummy_name}_x_I_3_6\"\n",
        "        lit_est_3_6 = lit_inter_3_6.loc[lit_coeff_name_3_6, 'Estimate']\n",
        "        if not (lit_est_3_6 > 0): # Paper finds it positive, may not be significant\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Literature (3 vs 6) did not show the expected \"\n",
        "                f\"positive interaction. Found Est={lit_est_3_6:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        lit_inter_6_10 = all_results['Score_Lit'][(6, 10)]\n",
        "        lit_coeff_name_6_10 = f\"{p2_dummy_name}_x_I_6_10\"\n",
        "        lit_est_6_10 = lit_inter_6_10.loc[lit_coeff_name_6_10, 'Estimate']\n",
        "        lit_pval_6_10 = lit_inter_6_10.loc[lit_coeff_name_6_10, 'P-value']\n",
        "        if not (lit_est_6_10 < 0 and lit_pval_6_10 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Literature (6 vs 10) did not show the expected \"\n",
        "                f\"significant negative interaction. Found Est={lit_est_6_10:.3f}, P-val={lit_pval_6_10:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # Check English: Expect positive effects for both transitions.\n",
        "        eng_inter_3_6 = all_results['Score_Eng'][(3, 6)]\n",
        "        eng_coeff_name_3_6 = f\"{p2_dummy_name}_x_I_3_6\"\n",
        "        eng_est_3_6 = eng_inter_3_6.loc[eng_coeff_name_3_6, 'Estimate']\n",
        "        eng_pval_3_6 = eng_inter_3_6.loc[eng_coeff_name_3_6, 'P-value']\n",
        "        if not (eng_est_3_6 > 0 and eng_pval_3_6 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: English (3 vs 6) did not show the expected \"\n",
        "                f\"significant positive interaction. Found Est={eng_est_3_6:.3f}, P-val={eng_pval_3_6:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        eng_inter_6_10 = all_results['Score_Eng'][(6, 10)]\n",
        "        eng_coeff_name_6_10 = f\"{p2_dummy_name}_x_I_6_10\"\n",
        "        eng_est_6_10 = eng_inter_6_10.loc[eng_coeff_name_6_10, 'Estimate']\n",
        "        eng_pval_6_10 = eng_inter_6_10.loc[eng_coeff_name_6_10, 'P-value']\n",
        "        if not (eng_est_6_10 > 0 and eng_pval_6_10 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: English (6 vs 10) did not show the expected \"\n",
        "                f\"significant positive interaction. Found Est={eng_est_6_10:.3f}, P-val={eng_pval_6_10:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        print(\"Result patterns for interaction models validated.\")\n",
        "    except KeyError as e:\n",
        "        warnings.warn(f\"Could not perform pattern validation due to missing coefficient: {e}\", UserWarning)\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "3NT9YWJsIXqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Estimate OLS effort and parental-investment models by subject × grade\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Estimate OLS effort and parental-investment models by subject × grade\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Helper function to validate result patterns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_effort_investment_patterns(\n",
        "    results: Dict[str, Dict[int, pd.DataFrame]],\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the results of effort/investment models against expected patterns.\n",
        "\n",
        "    This function checks for key developmental trends reported in the study, such\n",
        "    as the changing effect of homework and parental investment across grades and\n",
        "    subjects. It issues warnings for any significant deviations.\n",
        "\n",
        "    Args:\n",
        "        results: A nested dictionary of the regression summary tables.\n",
        "        config: The validated study configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Define the significance threshold for validation checks.\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1']\n",
        "\n",
        "    # --- Validate Homework Effort Patterns ---\n",
        "    try:\n",
        "        # Grade 3: Expect negative or weak effects.\n",
        "        g3_math_eff = results['Score_Math'][3].loc['days_homework_cat_4', 'Estimate']\n",
        "        if g3_math_eff > 0:\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Homework effort in Grade 3 (Math) was expected to be \"\n",
        "                f\"negative or weak, but found a positive estimate: {g3_math_eff:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # Grade 10: Expect monotonically positive and significant effects.\n",
        "        g10_math_eff_4 = results['Score_Math'][10].loc['days_homework_cat_4', 'Estimate']\n",
        "        g10_math_pval_4 = results['Score_Math'][10].loc['days_homework_cat_4', 'P-value']\n",
        "        if not (g10_math_eff_4 > 0 and g10_math_pval_4 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Homework effort in Grade 10 (Math, cat 4) was expected \"\n",
        "                f\"to be positive and significant. Found Est={g10_math_eff_4:.3f}, P-val={g10_math_pval_4:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "    except KeyError as e:\n",
        "        warnings.warn(f\"Could not perform homework pattern validation due to missing coefficient: {e}\", UserWarning)\n",
        "\n",
        "    # --- Validate Parental Investment (Talk Frequency) Patterns ---\n",
        "    try:\n",
        "        # English: Expect effect to increase with grade.\n",
        "        eng_g3_talk = results['Score_Eng'][3].loc['parent_talk_school_4', 'Estimate']\n",
        "        eng_g10_talk = results['Score_Eng'][10].loc['parent_talk_school_4', 'Estimate']\n",
        "        if not eng_g10_talk > eng_g3_talk:\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Parental talk effect for English was expected to increase \"\n",
        "                f\"from G3 to G10. Found G3={eng_g3_talk:.3f}, G10={eng_g10_talk:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # Literature: Expect effect to decrease with grade.\n",
        "        lit_g3_talk = results['Score_Lit'][3].loc['parent_talk_school_4', 'Estimate']\n",
        "        lit_g10_talk = results['Score_Lit'][10].loc['parent_talk_school_4', 'Estimate']\n",
        "        if not lit_g10_talk < lit_g3_talk:\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Parental talk effect for Literature was expected to decrease \"\n",
        "                f\"from G3 to G10. Found G3={lit_g3_talk:.3f}, G10={lit_g10_talk:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "    except KeyError as e:\n",
        "        warnings.warn(f\"Could not perform parental talk pattern validation due to missing coefficient: {e}\", UserWarning)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_effort_investment_ols_estimation(\n",
        "    full_df: pd.DataFrame,\n",
        "    model_specifications: Dict[str, Dict[int, Dict[str, Any]]],\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[int, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of by-grade effort and investment OLS models.\n",
        "\n",
        "    This function systematically estimates 9 separate OLS models, one for each\n",
        "    combination of subject and grade. It creates the specific analysis sample\n",
        "    for each cell on-the-fly, runs the regression, extracts key results for\n",
        "    effort and investment variables, and validates the findings against the\n",
        "    developmental patterns reported in the study.\n",
        "\n",
        "    Args:\n",
        "        full_df: The complete, feature-engineered DataFrame from Task 8.\n",
        "        model_specifications: A nested dictionary of the by-grade model\n",
        "                              specifications from Task 10.\n",
        "        control_vars_list: The comprehensive list of all control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary where keys are subject and grade, and values are\n",
        "        pandas DataFrames with the formatted regression results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(full_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'full_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(model_specifications, dict):\n",
        "        raise TypeError(\"Input 'model_specifications' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results: Dict[str, Dict[int, pd.DataFrame]] = {}\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "\n",
        "    # Define the key effort and investment coefficients to extract.\n",
        "    target_coeffs = [\n",
        "        'days_homework_cat_2', 'days_homework_cat_3', 'days_homework_cat_4',\n",
        "        'parent_talk_school_2', 'parent_talk_school_3', 'parent_talk_school_4'\n",
        "    ]\n",
        "    # Also include the main PHE dummies for context.\n",
        "    target_coeffs.extend(config['variable_definitions']['phe_dummy_names'])\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    for y_var in dependent_vars:\n",
        "        all_results[y_var] = {}\n",
        "        for grade in grades:\n",
        "            print(f\"--- Estimating Effort/Investment OLS for: {y_var}, Grade {grade} ---\")\n",
        "\n",
        "            # --- Step 1: Create sample and fit the model ---\n",
        "            # Retrieve the correct model specification.\n",
        "            try:\n",
        "                spec = model_specifications[y_var][grade]\n",
        "            except KeyError:\n",
        "                raise ConfigurationError(f\"Missing spec for subject '{y_var}', grade {grade}.\")\n",
        "\n",
        "            # Create the analysis sample for this specific cell on-the-fly.\n",
        "            # 1. Filter by grade.\n",
        "            grade_df = full_df[full_df['grade'] == grade]\n",
        "            # 2. Perform listwise deletion using all required variables.\n",
        "            required_cols = [spec['dependent']] + spec['exog']\n",
        "            sample = _create_analysis_sample(grade_df, required_cols)\n",
        "\n",
        "            if sample.empty:\n",
        "                warnings.warn(f\"Sample for {y_var}, Grade {grade} is empty after listwise deletion. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            # Estimate the model.\n",
        "            try:\n",
        "                results = estimate_model(\n",
        "                    analysis_df=sample,\n",
        "                    model_spec=spec,\n",
        "                    config=config\n",
        "                )\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(\n",
        "                    f\"Estimation failed for Effort/Investment OLS on {y_var}, Grade {grade}.\"\n",
        "                ) from e\n",
        "\n",
        "            # --- Step 2: Extract and store results ---\n",
        "            # Extract results for the coefficients of interest.\n",
        "            summary_table = _extract_result_summary(\n",
        "                results=results,\n",
        "                target_coeffs=[c for c in target_coeffs if c in results.params.index],\n",
        "                config=config\n",
        "            )\n",
        "            all_results[y_var][grade] = summary_table\n",
        "            print(f\"Estimation successful for {y_var}, Grade {grade}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected patterns across grades ---\n",
        "    print(\"\\n--- Validating Effort/Investment Results Patterns ---\")\n",
        "    _validate_effort_investment_patterns(results=all_results, config=config)\n",
        "    print(\"Pattern validation for effort/investment models complete.\")\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "FNWSvekbJTYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Estimate IV first-stage regressions and validate instrument relevance\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Estimate IV first-stage regressions and validate instrument relevance\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Helper to extract and validate first-stage results.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _extract_and_validate_first_stage(\n",
        "    results: PanelEffectsResults,\n",
        "    instrument_var: str,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts key statistics from a first-stage regression and validates them.\n",
        "\n",
        "    This function processes a fitted first-stage model to extract the\n",
        "    instrument's coefficient and the weak instrument F-statistic. It then\n",
        "    validates these against the study's theoretical expectations (sign) and\n",
        "    methodological requirements (strength).\n",
        "\n",
        "    Args:\n",
        "        results: The fitted first-stage model object from `linearmodels`.\n",
        "        instrument_var: The name of the instrumental variable (e.g., 'GG').\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the extracted statistics and validation flags.\n",
        "    \"\"\"\n",
        "    # --- Statistic Extraction ---\n",
        "    # Extract the coefficient for the instrumental variable.\n",
        "    pi1_estimate = results.params[instrument_var]\n",
        "    # Extract the standard error, t-statistic, and p-value.\n",
        "    std_err = results.std_errors[instrument_var]\n",
        "    t_stat = results.tstats[instrument_var]\n",
        "    p_value = results.pvalues[instrument_var]\n",
        "\n",
        "    # The F-statistic for a single excluded instrument is the square of its t-statistic.\n",
        "    # This is the correct statistic to use for the weak instrument test in this context.\n",
        "    f_statistic = t_stat ** 2\n",
        "\n",
        "    # --- Validation ---\n",
        "    # 1. Validate instrument strength against the Staiger-Stock rule-of-thumb.\n",
        "    f_stat_threshold = config['iv_validation']['weak_instrument_threshold_f_stat']\n",
        "    is_strong = f_statistic >= f_stat_threshold\n",
        "    if not is_strong:\n",
        "        warnings.warn(\n",
        "            f\"WeakInstrumentWarning: Instrument '{instrument_var}' may be weak. \"\n",
        "            f\"F-statistic ({f_statistic:.2f}) is below the threshold of {f_stat_threshold}.\",\n",
        "            UserWarning\n",
        "        )\n",
        "\n",
        "    # 2. Validate the sign of the instrument's coefficient against theory.\n",
        "    expected_sign = config['instrument_construction']['expected_sign_pi1']\n",
        "    sign_is_correct = (pi1_estimate < 0) if expected_sign == 'negative' else (pi1_estimate > 0)\n",
        "    if not sign_is_correct:\n",
        "        warnings.warn(\n",
        "            f\"UnexpectedSignWarning: Instrument '{instrument_var}' has an unexpected sign. \"\n",
        "            f\"Expected '{expected_sign}', but found estimate of {pi1_estimate:.3f}.\",\n",
        "            UserWarning\n",
        "        )\n",
        "\n",
        "    # --- Assemble Summary ---\n",
        "    # Compile all extracted and validated information into a dictionary.\n",
        "    summary = {\n",
        "        'pi1_estimate': pi1_estimate,\n",
        "        'std_error': std_err,\n",
        "        'p_value': p_value,\n",
        "        'f_statistic': f_statistic,\n",
        "        'is_strong': is_strong,\n",
        "        'sign_is_correct': sign_is_correct\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_iv_first_stage_estimation(\n",
        "    iv_samples: Dict[str, Any],\n",
        "    iv_specifications: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation and validation of all first-stage IV models.\n",
        "\n",
        "    This function systematically runs the first-stage regression for all pooled\n",
        "    and by-grade IV specifications. It extracts key diagnostic statistics,\n",
        "    validates instrument relevance (strength and sign), and compiles the\n",
        "    results into a single, comprehensive summary table.\n",
        "\n",
        "    Args:\n",
        "        iv_samples: The nested dictionary of all IV analysis samples from Task 9.\n",
        "        iv_specifications: The nested dictionary of all IV model specifications\n",
        "                           from Task 11.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the results of all first-stage regressions.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(iv_samples, dict) or not isinstance(iv_specifications, dict):\n",
        "        raise TypeError(\"Inputs 'iv_samples' and 'iv_specifications' must be dictionaries.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results_data = []\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "    instrument_var = config['variable_definitions']['instrumental_variable']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    # Define the model types to iterate over.\n",
        "    model_types_to_run = {'pooled': dependent_vars, 'by_grade': grades}\n",
        "\n",
        "    for model_type, items in model_types_to_run.items():\n",
        "        if model_type == 'pooled':\n",
        "            # Loop through subjects for pooled models.\n",
        "            for y_var in items:\n",
        "                print(f\"--- Estimating IV First Stage for: Pooled, {y_var} ---\")\n",
        "                sample = iv_samples[model_type][y_var]\n",
        "                spec = iv_specifications[model_type][y_var]['first_stage']\n",
        "\n",
        "                # --- Step 1: Fit the first-stage regression ---\n",
        "                results = estimate_model(sample, spec, config)\n",
        "\n",
        "                # --- Step 2: Extract and validate results ---\n",
        "                summary = _extract_and_validate_first_stage(results, instrument_var, config)\n",
        "                summary['subject'] = y_var\n",
        "                summary['grade'] = 'Pooled'\n",
        "                all_results_data.append(summary)\n",
        "\n",
        "        elif model_type == 'by_grade':\n",
        "            # Use nested loops for by-grade models.\n",
        "            for y_var in dependent_vars:\n",
        "                for grade in items:\n",
        "                    print(f\"--- Estimating IV First Stage for: {y_var}, Grade {grade} ---\")\n",
        "                    sample = iv_samples[model_type][y_var][grade]\n",
        "                    spec = iv_specifications[model_type][y_var][grade]['first_stage']\n",
        "\n",
        "                    if sample.empty:\n",
        "                        warnings.warn(f\"Sample for {y_var}, Grade {grade} is empty. Skipping first stage.\", UserWarning)\n",
        "                        continue\n",
        "\n",
        "                    # --- Step 1 & 2 ---\n",
        "                    results = estimate_model(sample, spec, config)\n",
        "                    summary = _extract_and_validate_first_stage(results, instrument_var, config)\n",
        "                    summary['subject'] = y_var\n",
        "                    summary['grade'] = grade\n",
        "                    all_results_data.append(summary)\n",
        "\n",
        "    # --- Step 3: Store and report results ---\n",
        "    # Convert the list of dictionaries into a final, well-structured DataFrame.\n",
        "    results_df = pd.DataFrame(all_results_data)\n",
        "\n",
        "    # Set a MultiIndex for clear organization and easy access.\n",
        "    results_df = results_df.set_index(['subject', 'grade'])\n",
        "\n",
        "    # Reorder columns for a clean presentation.\n",
        "    column_order = [\n",
        "        'pi1_estimate', 'std_error', 'p_value', 'f_statistic',\n",
        "        'is_strong', 'sign_is_correct'\n",
        "    ]\n",
        "    results_df = results_df[column_order]\n",
        "\n",
        "    print(\"\\n--- IV First-Stage Estimation and Validation Complete ---\")\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "deibs3mjKNlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Estimate IV reduced-form regressions (diagnostic)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Estimate IV reduced-form regressions (diagnostic)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Helper to extract and validate reduced-form results.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _extract_and_validate_reduced_form(\n",
        "    results: PanelEffectsResults,\n",
        "    instrument_var: str,\n",
        "    first_stage_pi1: float,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts key statistics from a reduced-form regression and validates them.\n",
        "\n",
        "    This function processes a fitted reduced-form model to extract the\n",
        "    instrument's direct effect on the outcome. It validates the sign of this\n",
        "    effect for consistency with the overall IV framework and computes the\n",
        "    implied Wald estimator as a diagnostic.\n",
        "\n",
        "    Args:\n",
        "        results: The fitted reduced-form model object from `linearmodels`.\n",
        "        instrument_var: The name of the instrumental variable (e.g., 'GG').\n",
        "        first_stage_pi1: The estimated first-stage coefficient (π̂₁) for the\n",
        "                         corresponding model.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the extracted statistics and diagnostics.\n",
        "    \"\"\"\n",
        "    # --- Statistic Extraction ---\n",
        "    # Extract the reduced-form coefficient (ρ̂₁), its standard error, and p-value.\n",
        "    rho1_estimate = results.params[instrument_var]\n",
        "    std_err = results.std_errors[instrument_var]\n",
        "    p_value = results.pvalues[instrument_var]\n",
        "\n",
        "    # --- Validation and Diagnostics ---\n",
        "    # 1. Validate the sign of the reduced-form coefficient.\n",
        "    # Given π̂₁ < 0 and an expected causal effect α̂₁ > 0, we expect ρ̂₁ < 0.\n",
        "    sign_is_consistent = rho1_estimate < 0\n",
        "    if not sign_is_consistent:\n",
        "        warnings.warn(\n",
        "            f\"InconsistentSignWarning: Reduced-form coefficient for '{instrument_var}' \"\n",
        "            f\"has an unexpected sign. Expected negative, but found {rho1_estimate:.3f}.\",\n",
        "            UserWarning\n",
        "        )\n",
        "\n",
        "    # 2. Compute the implied Wald estimator as a diagnostic.\n",
        "    # α̂₁_wald = ρ̂₁ / π̂₁\n",
        "    # Avoid division by zero, though π̂₁ should be non-zero if the instrument is relevant.\n",
        "    if first_stage_pi1 != 0:\n",
        "        wald_estimate = rho1_estimate / first_stage_pi1\n",
        "    else:\n",
        "        wald_estimate = float('nan')\n",
        "\n",
        "    # --- Assemble Summary ---\n",
        "    # Compile all information into a summary dictionary.\n",
        "    summary = {\n",
        "        'rho1_estimate': rho1_estimate,\n",
        "        'std_error': std_err,\n",
        "        'p_value': p_value,\n",
        "        'sign_is_consistent': sign_is_consistent,\n",
        "        'implied_wald_estimate': wald_estimate\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_iv_reduced_form_estimation(\n",
        "    iv_samples: Dict[str, Any],\n",
        "    iv_specifications: Dict[str, Any],\n",
        "    first_stage_results: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of all reduced-form IV diagnostic models.\n",
        "\n",
        "    This function systematically runs the reduced-form regression for all pooled\n",
        "    and by-grade IV specifications. It serves as a diagnostic check on the\n",
        "    instrument's effect on the final outcome. The function is conditional on the\n",
        "    'report_reduced_form' flag in the configuration.\n",
        "\n",
        "    Args:\n",
        "        iv_samples: The nested dictionary of all IV analysis samples.\n",
        "        iv_specifications: The nested dictionary of all IV model specifications.\n",
        "        first_stage_results: The DataFrame of results from the first-stage\n",
        "                             estimations (Task 16).\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the reduced-form results, or None if\n",
        "        this diagnostic step is disabled in the configuration.\n",
        "    \"\"\"\n",
        "    # --- Check Configuration ---\n",
        "    # This entire diagnostic step is optional, controlled by the config.\n",
        "    if not config['inference_parameters'].get('report_reduced_form', False):\n",
        "        print(\"--- Skipping IV Reduced-Form Estimation (disabled in config) ---\")\n",
        "        return None\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(iv_samples, dict) or not isinstance(iv_specifications, dict):\n",
        "        raise TypeError(\"Inputs 'iv_samples' and 'iv_specifications' must be dictionaries.\")\n",
        "    if not isinstance(first_stage_results, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'first_stage_results' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results_data = []\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "    instrument_var = config['variable_definitions']['instrumental_variable']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    # Iterate through the same models as the first-stage estimation.\n",
        "    for y_var in dependent_vars:\n",
        "        # Pooled Model\n",
        "        print(f\"--- Estimating IV Reduced-Form for: Pooled, {y_var} ---\")\n",
        "        sample = iv_samples['pooled'][y_var]\n",
        "        spec = iv_specifications['pooled'][y_var]['reduced_form']\n",
        "        pi1 = first_stage_results.loc[(y_var, 'Pooled'), 'pi1_estimate']\n",
        "\n",
        "        results = estimate_model(sample, spec, config)\n",
        "        summary = _extract_and_validate_reduced_form(results, instrument_var, pi1, config)\n",
        "        summary['subject'] = y_var\n",
        "        summary['grade'] = 'Pooled'\n",
        "        all_results_data.append(summary)\n",
        "\n",
        "        # By-Grade Models\n",
        "        for grade in grades:\n",
        "            print(f\"--- Estimating IV Reduced-Form for: {y_var}, Grade {grade} ---\")\n",
        "            try:\n",
        "                sample = iv_samples['by_grade'][y_var][grade]\n",
        "                spec = iv_specifications['by_grade'][y_var][grade]['reduced_form']\n",
        "                pi1 = first_stage_results.loc[(y_var, grade), 'pi1_estimate']\n",
        "            except KeyError:\n",
        "                warnings.warn(f\"Missing sample, spec, or first-stage result for {y_var}, Grade {grade}. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            if sample.empty:\n",
        "                warnings.warn(f\"Sample for {y_var}, Grade {grade} is empty. Skipping reduced form.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            results = estimate_model(sample, spec, config)\n",
        "            summary = _extract_and_validate_reduced_form(results, instrument_var, pi1, config)\n",
        "            summary['subject'] = y_var\n",
        "            summary['grade'] = grade\n",
        "            all_results_data.append(summary)\n",
        "\n",
        "    # --- Step 3: Store and report results ---\n",
        "    # Convert the list of dictionaries into a final DataFrame.\n",
        "    results_df = pd.DataFrame(all_results_data).set_index(['subject', 'grade'])\n",
        "\n",
        "    # Define a clean column order for the final report.\n",
        "    column_order = [\n",
        "        'rho1_estimate', 'std_error', 'p_value',\n",
        "        'sign_is_consistent', 'implied_wald_estimate'\n",
        "    ]\n",
        "    results_df = results_df[column_order]\n",
        "\n",
        "    print(\"\\n--- IV Reduced-Form Estimation and Validation Complete ---\")\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "xx5wBtlzLVdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Estimate IV second-stage regressions and extract causal persistent-effect coefficients\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Estimate IV second-stage regressions and extract causal\n",
        "#          persistent-effect coefficients\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_pooled_iv_estimation(\n",
        "    iv_samples: Dict[str, Any],\n",
        "    iv_specifications: Dict[str, Any],\n",
        "    ols_results: Dict[str, pd.DataFrame],\n",
        "    first_stage_results: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of pooled IV second-stage models.\n",
        "\n",
        "    This function estimates the main causal effect of parental education on\n",
        "    student achievement for each subject using a 2SLS approach. It leverages\n",
        "    the generic estimation engine, extracts and formats the results, and\n",
        "    performs validation checks against theoretical expectations and OLS estimates.\n",
        "\n",
        "    Args:\n",
        "        iv_samples: The nested dictionary of IV analysis samples.\n",
        "        iv_specifications: The nested dictionary of IV model specifications.\n",
        "        ols_results: The dictionary of pooled OLS results from Task 13 for comparison.\n",
        "        first_stage_results: The DataFrame of first-stage results from Task 16\n",
        "                             for diagnostic checks.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are subject names and values are pandas\n",
        "        DataFrames containing the formatted 2SLS regression results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(iv_samples, dict) or not isinstance(iv_specifications, dict):\n",
        "        raise TypeError(\"Inputs 'iv_samples' and 'iv_specifications' must be dictionaries.\")\n",
        "    if not isinstance(ols_results, dict):\n",
        "        raise TypeError(\"Input 'ols_results' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results: Dict[str, pd.DataFrame] = {}\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    for y_var in dependent_vars:\n",
        "        print(f\"--- Estimating Pooled IV Second Stage for: {y_var} ---\")\n",
        "\n",
        "        # --- Step 1: Fit the pooled IV second-stage model ---\n",
        "        # Retrieve the correct sample and the 'second_stage' specification.\n",
        "        try:\n",
        "            sample = iv_samples['pooled'][y_var]\n",
        "            spec = iv_specifications['pooled'][y_var]['second_stage']\n",
        "        except KeyError:\n",
        "            raise ConfigurationError(f\"Missing sample or spec for Pooled IV on {y_var}.\")\n",
        "\n",
        "        # Check if the instrument for this model was strong.\n",
        "        is_strong = first_stage_results.loc[(y_var, 'Pooled'), 'is_strong']\n",
        "        if not is_strong:\n",
        "            warnings.warn(\n",
        "                f\"Estimating 2SLS for {y_var} with a weak instrument. \"\n",
        "                \"Results may be unreliable.\", UserWarning\n",
        "            )\n",
        "\n",
        "        # Estimate the 2SLS model using the generic estimation engine.\n",
        "        try:\n",
        "            results = estimate_model(\n",
        "                analysis_df=sample,\n",
        "                model_spec=spec,\n",
        "                config=config\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Estimation failed for Pooled IV on {y_var}.\") from e\n",
        "\n",
        "        # --- Step 2: Extract and format results ---\n",
        "        # Process the results object into a clean summary table.\n",
        "        # The target coefficient is the endogenous variable itself.\n",
        "        summary_table = _extract_result_summary(\n",
        "            results=results,\n",
        "            target_coeffs=[endog_var],\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        all_results[y_var] = summary_table\n",
        "        print(f\"Estimation successful for {y_var}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected causal effects ---\n",
        "    print(\"\\n--- Validating Pooled IV Results Patterns ---\")\n",
        "\n",
        "    # Check for positive sign and compare magnitudes.\n",
        "    for y_var, res_df in all_results.items():\n",
        "        # 1. Check for positive sign.\n",
        "        estimate = res_df.loc[endog_var, 'Estimate']\n",
        "        if estimate <= 0:\n",
        "            warnings.warn(\n",
        "                f\"ValidationWarning: Causal estimate for '{y_var}' is not positive \"\n",
        "                f\"as expected (Estimate: {estimate:.3f}).\", UserWarning\n",
        "            )\n",
        "\n",
        "        # 2. Compare IV estimate to the (scaled) OLS estimate.\n",
        "        try:\n",
        "            p2_coeff_name = config['variable_definitions']['phe_dummy_names'][1]\n",
        "            ols_est_p2 = ols_results[y_var].loc[p2_coeff_name, 'Estimate']\n",
        "            # The IV estimate is per-level; the OLS p2 estimate is for two levels.\n",
        "            # We compare the IV estimate to half the OLS p2 estimate.\n",
        "            scaled_ols_est = ols_est_p2 / 2.0\n",
        "            if estimate < scaled_ols_est:\n",
        "                warnings.warn(\n",
        "                    f\"ValidationWarning: IV estimate for '{y_var}' ({estimate:.2f}) is smaller \"\n",
        "                    f\"than the scaled OLS estimate ({scaled_ols_est:.2f}). This may suggest \"\n",
        "                    \"a different direction of endogeneity bias than expected.\", UserWarning\n",
        "                )\n",
        "        except (KeyError, IndexError):\n",
        "            warnings.warn(f\"Could not perform OLS comparison for {y_var}.\", UserWarning)\n",
        "\n",
        "    # 3. Check the expected ordering of effect sizes (Eng > Lit > Math).\n",
        "    try:\n",
        "        eng_eff = all_results['Score_Eng'].loc[endog_var, 'Estimate']\n",
        "        lit_eff = all_results['Score_Lit'].loc[endog_var, 'Estimate']\n",
        "        math_eff = all_results['Score_Math'].loc[endog_var, 'Estimate']\n",
        "\n",
        "        if not (eng_eff > lit_eff > math_eff):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: The expected ordering of IV causal effect sizes \"\n",
        "                f\"(Eng > Lit > Math) was not observed. \"\n",
        "                f\"Found: Eng={eng_eff:.2f}, Lit={lit_eff:.2f}, Math={math_eff:.2f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "        else:\n",
        "            print(\"Result patterns for pooled IV models validated successfully.\")\n",
        "    except KeyError:\n",
        "        warnings.warn(\"Could not perform pattern validation due to missing results.\", UserWarning)\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "oJeJQRD5Ma7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Estimate IV second-stage by-grade regressions\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Estimate IV second-stage by-grade regressions\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Helper function to validate result patterns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_by_grade_iv_patterns(\n",
        "    results: Dict[str, Dict[int, pd.DataFrame]],\n",
        "    endog_var: str,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the results of by-grade IV models against expected age-profiles.\n",
        "\n",
        "    This function checks for the key dynamic patterns in the causal effect of\n",
        "    parental education across grades, as reported in the study. It issues\n",
        "    warnings for significant deviations from these expected trends.\n",
        "\n",
        "    Args:\n",
        "        results: A nested dictionary of the regression summary tables.\n",
        "        endog_var: The name of the endogenous variable ('PHE').\n",
        "        config: The validated study configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Define the significance threshold for validation checks.\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1']\n",
        "\n",
        "    try:\n",
        "        # --- Validate Mathematics & Literature: Declining Influence ---\n",
        "        for subject in ['Score_Math', 'Score_Lit']:\n",
        "            # Check that the effect in Grade 10 is smaller and/or insignificant.\n",
        "            est_g6 = results[subject][6].loc[endog_var, 'Estimate']\n",
        "            est_g10 = results[subject][10].loc[endog_var, 'Estimate']\n",
        "            pval_g10 = results[subject][10].loc[endog_var, 'P-value']\n",
        "\n",
        "            if not (est_g10 < est_g6):\n",
        "                warnings.warn(\n",
        "                    f\"ValidationWarning: Causal effect for '{subject}' was expected to \"\n",
        "                    f\"decrease from G6 to G10. Found G6={est_g6:.2f}, G10={est_g10:.2f}\",\n",
        "                    UserWarning\n",
        "                )\n",
        "            if pval_g10 < alpha:\n",
        "                warnings.warn(\n",
        "                    f\"ValidationWarning: Causal effect for '{subject}' in G10 was \"\n",
        "                    f\"expected to be insignificant. Found P-val={pval_g10:.3f}\",\n",
        "                    UserWarning\n",
        "                )\n",
        "\n",
        "        # --- Validate English: Persistent Influence ---\n",
        "        # Check that the effect remains positive and significant in all grades.\n",
        "        for grade in [3, 6, 10]:\n",
        "            est_eng = results['Score_Eng'][grade].loc[endog_var, 'Estimate']\n",
        "            pval_eng = results['Score_Eng'][grade].loc[endog_var, 'P-value']\n",
        "            if not (est_eng > 0 and pval_eng < alpha):\n",
        "                warnings.warn(\n",
        "                    f\"ValidationWarning: Causal effect for 'Score_Eng' in G{grade} was \"\n",
        "                    f\"expected to be positive and significant. Found Est={est_eng:.2f}, \"\n",
        "                    f\"P-val={pval_eng:.3f}\",\n",
        "                    UserWarning\n",
        "                )\n",
        "    except KeyError as e:\n",
        "        warnings.warn(f\"Could not perform by-grade pattern validation due to missing result: {e}\", UserWarning)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_by_grade_iv_estimation(\n",
        "    iv_samples: Dict[str, Any],\n",
        "    iv_specifications: Dict[str, Any],\n",
        "    first_stage_results: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[int, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of by-grade IV second-stage models.\n",
        "\n",
        "    This function systematically estimates the 9 models required to analyze the\n",
        "    causal effect of parental education at each grade level (3, 6, 10) for each\n",
        "    subject. It validates the results against the expected age-profile patterns.\n",
        "\n",
        "    Args:\n",
        "        iv_samples: The nested dictionary of all IV analysis samples.\n",
        "        iv_specifications: The nested dictionary of all IV model specifications.\n",
        "        first_stage_results: The DataFrame of results from the first-stage\n",
        "                             estimations (Task 16).\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary where keys are subject and grade, and values are\n",
        "        pandas DataFrames with the formatted 2SLS regression results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(iv_samples, dict) or not isinstance(iv_specifications, dict):\n",
        "        raise TypeError(\"Inputs 'iv_samples' and 'iv_specifications' must be dictionaries.\")\n",
        "    if not isinstance(first_stage_results, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'first_stage_results' must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results: Dict[str, Dict[int, pd.DataFrame]] = {}\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grades = config['population_scope']['grades_of_interest']\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    for y_var in dependent_vars:\n",
        "        all_results[y_var] = {}\n",
        "        for grade in grades:\n",
        "            print(f\"--- Estimating By-Grade IV Second Stage for: {y_var}, Grade {grade} ---\")\n",
        "\n",
        "            # --- Step 1: Fit the by-grade IV second-stage model ---\n",
        "            try:\n",
        "                sample = iv_samples['by_grade'][y_var][grade]\n",
        "                spec = iv_specifications['by_grade'][y_var][grade]['second_stage']\n",
        "                is_strong = first_stage_results.loc[(y_var, grade), 'is_strong']\n",
        "            except KeyError:\n",
        "                warnings.warn(f\"Missing sample, spec, or first-stage result for {y_var}, Grade {grade}. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            if sample.empty:\n",
        "                warnings.warn(f\"Sample for {y_var}, Grade {grade} is empty. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            if not is_strong:\n",
        "                warnings.warn(\n",
        "                    f\"Estimating 2SLS for {y_var}, Grade {grade} with a weak instrument. \"\n",
        "                    \"Results may be unreliable.\", UserWarning\n",
        "                )\n",
        "\n",
        "            try:\n",
        "                results = estimate_model(sample, spec, config)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Estimation failed for By-Grade IV on {y_var}, Grade {grade}.\") from e\n",
        "\n",
        "            # --- Step 2: Extract and store results ---\n",
        "            summary_table = _extract_result_summary(\n",
        "                results=results,\n",
        "                target_coeffs=[endog_var],\n",
        "                config=config\n",
        "            )\n",
        "            all_results[y_var][grade] = summary_table\n",
        "            print(f\"Estimation successful for {y_var}, Grade {grade}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected age-profile patterns ---\n",
        "    print(\"\\n--- Validating By-Grade IV Results Patterns ---\")\n",
        "    _validate_by_grade_iv_patterns(results=all_results, endog_var=endog_var, config=config)\n",
        "    print(\"Pattern validation for by-grade IV models complete.\")\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "J7RbUsvCNfLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Estimate IV second-stage with grade-by-PHE interactions and extract causal Matthew-effect coefficients\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Estimate IV second-stage with grade-by-PHE interactions and\n",
        "#          extract causal Matthew-effect coefficients\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Helper function to validate result patterns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_interaction_iv_patterns(\n",
        "    results: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    endog_var: str,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates causal Matthew-effect patterns from IV interaction models.\n",
        "\n",
        "    This function checks the sign and significance of the key interaction\n",
        "    coefficients (α̂₂) against the study's main causal findings for each\n",
        "    subject, issuing warnings for any deviations.\n",
        "\n",
        "    Args:\n",
        "        results: A nested dictionary of the regression summary tables.\n",
        "        endog_var: The name of the endogenous variable ('PHE').\n",
        "        config: The validated study configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Define the significance threshold for validation checks.\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1']\n",
        "\n",
        "    try:\n",
        "        # --- Validate Mathematics: Declining Causal Influence ---\n",
        "        math_res_6_10 = results['Score_Math'][(6, 10)]\n",
        "        math_coeff_name = f\"{endog_var}_x_I_6_10\"\n",
        "        math_est = math_res_6_10.loc[math_coeff_name, 'Estimate']\n",
        "        math_pval = math_res_6_10.loc[math_coeff_name, 'P-value']\n",
        "        if not (math_est < 0 and math_pval < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Causal interaction for Mathematics (6 vs 10) did not show \"\n",
        "                f\"the expected significant negative effect. Found Est={math_est:.3f}, P-val={math_pval:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # --- Validate Literature: Bell-Shaped Causal Influence ---\n",
        "        lit_res_3_6 = results['Score_Lit'][(3, 6)]\n",
        "        lit_coeff_name_3_6 = f\"{endog_var}_x_I_3_6\"\n",
        "        lit_est_3_6 = lit_res_3_6.loc[lit_coeff_name_3_6, 'Estimate']\n",
        "        if not (lit_est_3_6 > 0):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Causal interaction for Literature (3 vs 6) did not show \"\n",
        "                f\"the expected positive effect. Found Est={lit_est_3_6:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        lit_res_6_10 = results['Score_Lit'][(6, 10)]\n",
        "        lit_coeff_name_6_10 = f\"{endog_var}_x_I_6_10\"\n",
        "        lit_est_6_10 = lit_res_6_10.loc[lit_coeff_name_6_10, 'Estimate']\n",
        "        lit_pval_6_10 = lit_res_6_10.loc[lit_coeff_name_6_10, 'P-value']\n",
        "        if not (lit_est_6_10 < 0 and lit_pval_6_10 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Causal interaction for Literature (6 vs 10) did not show \"\n",
        "                f\"the expected significant negative effect. Found Est={lit_est_6_10:.3f}, P-val={lit_pval_6_10:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # --- Validate English: Increasing Causal Influence ---\n",
        "        eng_res_3_6 = results['Score_Eng'][(3, 6)]\n",
        "        eng_coeff_name_3_6 = f\"{endog_var}_x_I_3_6\"\n",
        "        eng_est_3_6 = eng_res_3_6.loc[eng_coeff_name_3_6, 'Estimate']\n",
        "        eng_pval_3_6 = eng_res_3_6.loc[eng_coeff_name_3_6, 'P-value']\n",
        "        if not (eng_est_3_6 > 0 and eng_pval_3_6 < alpha):\n",
        "            warnings.warn(\n",
        "                \"ValidationWarning: Causal interaction for English (3 vs 6) did not show \"\n",
        "                f\"the expected significant positive effect. Found Est={eng_est_3_6:.3f}, P-val={eng_pval_3_6:.3f}\",\n",
        "                UserWarning\n",
        "            )\n",
        "    except KeyError as e:\n",
        "        warnings.warn(f\"Could not perform IV interaction pattern validation due to missing coefficient: {e}\", UserWarning)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_interaction_iv_estimation(\n",
        "    iv_samples: Dict[str, Any],\n",
        "    iv_specifications: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[Tuple[int, int], pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of IV models with grade-by-PHE interactions.\n",
        "\n",
        "    This function estimates the 9 complex 2SLS models required to derive the\n",
        "    causal Matthew Effect. It correctly specifies models with two endogenous\n",
        "    variables (PHE and its interaction) and two instruments, extracts the key\n",
        "    causal coefficients, and validates them against the study's main findings.\n",
        "\n",
        "    Args:\n",
        "        iv_samples: The nested dictionary of IV interaction analysis samples.\n",
        "        iv_specifications: The nested dictionary of IV interaction model specs.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A nested dictionary with the formatted 2SLS interaction model results.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(iv_samples, dict) or not isinstance(iv_specifications, dict):\n",
        "        raise TypeError(\"Inputs 'iv_samples' and 'iv_specifications' must be dictionaries.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    all_results: Dict[str, Dict[Tuple[int, int], pd.DataFrame]] = {}\n",
        "    dependent_vars = config['variable_definitions']['dependent_variables']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "\n",
        "    # --- Estimation Loop ---\n",
        "    for y_var in dependent_vars:\n",
        "        all_results[y_var] = {}\n",
        "        for pair in grade_pairs:\n",
        "            u, v = pair\n",
        "            print(f\"--- Estimating Interaction IV for: {y_var}, Grades {u} vs {v} ---\")\n",
        "\n",
        "            # --- Step 1: Fit the IV interaction model ---\n",
        "            try:\n",
        "                sample = iv_samples['interaction'][y_var][pair]\n",
        "                spec = iv_specifications['interaction'][y_var][pair]['second_stage']\n",
        "            except KeyError:\n",
        "                raise ConfigurationError(f\"Missing sample or spec for IV Interaction on {y_var}, {pair}.\")\n",
        "\n",
        "            if sample.empty:\n",
        "                warnings.warn(f\"Sample for {y_var}, {pair} is empty. Skipping.\", UserWarning)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                results = estimate_model(sample, spec, config)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Estimation failed for IV Interaction on {y_var}, {pair}.\") from e\n",
        "\n",
        "            # --- Step 2: Extract and store results ---\n",
        "            # Programmatically define the names of the key endogenous coefficients.\n",
        "            indicator = f\"I_{u}_{v}\"\n",
        "            interaction_coeff_name = f\"{endog_var}_x_{indicator}\"\n",
        "            target_coeffs = [endog_var, interaction_coeff_name]\n",
        "\n",
        "            summary_table = _extract_result_summary(\n",
        "                results=results,\n",
        "                target_coeffs=target_coeffs,\n",
        "                config=config\n",
        "            )\n",
        "            all_results[y_var][pair] = summary_table\n",
        "            print(f\"Estimation successful for {y_var}, Grades {u} vs {v}.\")\n",
        "\n",
        "    # --- Step 3: Validate expected causal Matthew-effect patterns ---\n",
        "    print(\"\\n--- Validating Causal Matthew-Effect Patterns ---\")\n",
        "    _validate_interaction_iv_patterns(results=all_results, endog_var=endog_var, config=config)\n",
        "    print(\"Pattern validation for IV interaction models complete.\")\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "R1yLJ0RaQZkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Generate empirical CDF plots by PHE group for each subject and selected grades\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Generate empirical CDF plots by PHE group for each subject and\n",
        "#          selected grades\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Helper function to compute an empirical CDF.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_ecdf(data: np.ndarray, num_points: int = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes the x and y coordinates for an empirical CDF.\n",
        "\n",
        "    Args:\n",
        "        data: A 1D numpy array of numerical data.\n",
        "        num_points: If specified, the number of points to down-sample the ECDF\n",
        "                    to for efficient plotting. If None, all points are returned.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two numpy arrays: (sorted_data, cumulative_probabilities).\n",
        "    \"\"\"\n",
        "    # --- ECDF Computation ---\n",
        "    # Get the number of data points.\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    # Sort the data in ascending order.\n",
        "    x = np.sort(data)\n",
        "\n",
        "    # Create the cumulative probability vector.\n",
        "    y = np.arange(1, n + 1) / n\n",
        "\n",
        "    # --- Down-sampling for Efficiency (Optional) ---\n",
        "    # If a specific number of points is requested for plotting.\n",
        "    if num_points is not None and n > num_points:\n",
        "        # Create an array of evenly spaced indices to sample from the full ECDF.\n",
        "        indices = np.linspace(0, n - 1, num=num_points, dtype=int)\n",
        "        # Select the data points at these indices.\n",
        "        x = x[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 2 & 3: Helper function to plot and save a single CDF figure.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _plot_and_save_single_cdf(\n",
        "    df_subset: pd.DataFrame,\n",
        "    score_col: str,\n",
        "    grade: int,\n",
        "    output_dir: str,\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates and saves a single CDF plot for a given subject and grade.\n",
        "\n",
        "    This function plots the empirical CDF of scores for three parental\n",
        "    education (PHE) groups and saves the figure to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        df_subset: DataFrame filtered for the specific grade.\n",
        "        score_col: The name of the score column to plot.\n",
        "        grade: The grade being plotted.\n",
        "        output_dir: The directory in which to save the plot.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        The file path of the saved plot.\n",
        "    \"\"\"\n",
        "    # --- Plotting Setup ---\n",
        "    # Create a new figure and axes for the plot.\n",
        "    fig, ax = plt.subplots(figsize=(8, 6), dpi=100)\n",
        "\n",
        "    # Define plot aesthetics for each PHE group.\n",
        "    phe_labels = {\n",
        "        0.0: 'ISCED 0-2 (Lowest)',\n",
        "        1.0: 'ISCED 3-5 (Middle)',\n",
        "        2.0: 'ISCED 6-8 (Highest)'\n",
        "    }\n",
        "    colors = {0.0: 'red', 1.0: 'black', 2.0: 'blue'}\n",
        "\n",
        "    # --- Data Processing and Plotting Loop ---\n",
        "    # Iterate through the PHE groups to plot each CDF curve.\n",
        "    for phe_level, label in phe_labels.items():\n",
        "        # Filter the data for the current PHE group.\n",
        "        scores = df_subset.loc[df_subset['PHE'] == phe_level, score_col].dropna().values\n",
        "\n",
        "        # If there is no data for this group, skip to the next one.\n",
        "        if len(scores) == 0:\n",
        "            continue\n",
        "\n",
        "        # --- Step 1 (call): Compute the ECDF coordinates ---\n",
        "        x, y = _compute_ecdf(scores, num_points=1000)\n",
        "\n",
        "        # --- Step 2 (plot): Plot the CDF curve ---\n",
        "        ax.plot(x, y, label=label, color=colors[phe_level], linewidth=1.5)\n",
        "\n",
        "    # --- Final Plot Formatting ---\n",
        "    # Set titles and labels for clarity and professional appearance.\n",
        "    subject_name = score_col.replace('Score_', '')\n",
        "    ax.set_title(f'CDF of {subject_name} Scores, Grade {grade}, by Parental Education', fontsize=14)\n",
        "    ax.set_xlabel('Test Score', fontsize=12)\n",
        "    ax.set_ylabel('Cumulative Probability', fontsize=12)\n",
        "    ax.legend(title='Parental Education Group')\n",
        "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    ax.set_xlim(200, 800) # Set reasonable score limits\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    # --- Step 3: Save the figure ---\n",
        "    # Ensure the output directory exists.\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate a descriptive filename.\n",
        "    filename = f\"cdf_{subject_name.lower()}_grade_{grade}.png\"\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Save the figure with high resolution.\n",
        "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig) # Close the figure to free up memory.\n",
        "\n",
        "    return filepath\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_cdf_plots(\n",
        "    full_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all required CDF plots.\n",
        "\n",
        "    This function iterates through the specified subjects and grades, calling\n",
        "    a plotting helper to generate and save a CDF plot for each combination.\n",
        "    It provides a visual check for first-order stochastic dominance.\n",
        "\n",
        "    Args:\n",
        "        full_df: The complete, feature-engineered DataFrame.\n",
        "        config: The validated study configuration dictionary.\n",
        "        output_dir: The path to the directory where plots will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A list of file paths for all the generated plots.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(full_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'full_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "    if not isinstance(output_dir, str):\n",
        "        raise TypeError(\"Input 'output_dir' must be a string.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # Define the subjects and grades for which to generate plots.\n",
        "    subjects_to_plot = config['variable_definitions']['dependent_variables']\n",
        "    grades_to_plot = [3, 10]\n",
        "\n",
        "    # List to store the paths of the saved plots.\n",
        "    saved_plot_paths = []\n",
        "\n",
        "    # --- Plotting Loop ---\n",
        "    # Use nested loops to generate a plot for each required combination.\n",
        "    for subject in subjects_to_plot:\n",
        "        for grade in grades_to_plot:\n",
        "            print(f\"--- Generating CDF plot for: {subject}, Grade {grade} ---\")\n",
        "\n",
        "            # Filter the DataFrame to the specific grade of interest.\n",
        "            df_subset = full_df[full_df['grade'] == grade]\n",
        "\n",
        "            # Call the helper function to create and save the plot.\n",
        "            filepath = _plot_and_save_single_cdf(\n",
        "                df_subset=df_subset,\n",
        "                score_col=subject,\n",
        "                grade=grade,\n",
        "                output_dir=output_dir,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Add the path of the new plot to the list.\n",
        "            saved_plot_paths.append(filepath)\n",
        "            print(f\"Plot saved to: {filepath}\")\n",
        "\n",
        "    return saved_plot_paths\n"
      ],
      "metadata": {
        "id": "91mn6zFvRlGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Generate coefficient plots for global PHE effects and grade-interaction margins\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Generate coefficient plots for global PHE effects and\n",
        "#          grade-interaction margins\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Plot global (pooled) PHE effects.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _plot_global_phe_effects(\n",
        "    ols_results: Dict[str, pd.DataFrame],\n",
        "    iv_results: Dict[str, pd.DataFrame],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates and saves a plot of the global (pooled) PHE effects.\n",
        "\n",
        "    This function visualizes the main persistent effect of parental education\n",
        "    from both OLS and IV models for each subject, replicating Figure 2.\n",
        "\n",
        "    Args:\n",
        "        ols_results: The dictionary of pooled OLS results from Task 13.\n",
        "        iv_results: The dictionary of pooled IV results from Task 18.\n",
        "        config: The validated study configuration dictionary.\n",
        "        output_dir: The directory in which to save the plot.\n",
        "\n",
        "    Returns:\n",
        "        The file path of the saved plot.\n",
        "    \"\"\"\n",
        "    # --- Plotting Setup ---\n",
        "    # Get subject names and PHE dummy names from config.\n",
        "    subjects = config['variable_definitions']['dependent_variables']\n",
        "    p1_name, p2_name = config['variable_definitions']['phe_dummy_names']\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "\n",
        "    # Create a 1x3 subplot grid, one for each subject.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "    fig.suptitle('Global Impact of Parental Highest Education on Student Scores (Pooled Models)', fontsize=16)\n",
        "\n",
        "    # --- Plotting Loop ---\n",
        "    for i, subject in enumerate(subjects):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # --- Plot OLS Results ---\n",
        "        # Extract OLS estimates and confidence intervals.\n",
        "        ols_res = ols_results[subject]\n",
        "        ols_p1_est = ols_res.loc[p1_name, 'Estimate']\n",
        "        ols_p2_est = ols_res.loc[p2_name, 'Estimate']\n",
        "        ols_p1_err = ols_p1_est - ols_res.loc[p1_name, 'CI Lower']\n",
        "        ols_p2_err = ols_p2_est - ols_res.loc[p2_name, 'CI Lower']\n",
        "\n",
        "        # Plot OLS points with error bars.\n",
        "        ax.errorbar([1, 2], [ols_p1_est, ols_p2_est], yerr=[ols_p1_err, ols_p2_err],\n",
        "                    fmt='-o', capsize=5, label='OLS Estimate', color='black')\n",
        "\n",
        "        # --- Plot IV Results ---\n",
        "        # Extract IV estimate and scale it for comparison.\n",
        "        iv_res = iv_results[subject]\n",
        "        iv_est = iv_res.loc[endog_var, 'Estimate']\n",
        "        iv_err = iv_est - iv_res.loc[endog_var, 'CI Lower']\n",
        "\n",
        "        # Plot the IV estimate (per level) and the total effect (scaled by 2).\n",
        "        ax.errorbar([1], [iv_est], yerr=[iv_err], fmt='D', capsize=5,\n",
        "                    label='IV Estimate (per level)', color='blue', markersize=8)\n",
        "        ax.errorbar([2], [iv_est * 2], yerr=[iv_err * 2], fmt='D', capsize=5,\n",
        "                    label='IV Estimate (total effect)', color='blue', alpha=0.5, markersize=8)\n",
        "\n",
        "        # --- Formatting ---\n",
        "        ax.set_title(subject.replace('Score_', ''), fontsize=14)\n",
        "        ax.set_xticks([0, 1, 2])\n",
        "        ax.set_xticklabels(['ISCED 0-2\\n(Reference)', 'ISCED 3-5', 'ISCED 6-8'])\n",
        "        ax.set_xlabel('Parental Education Level')\n",
        "        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        ax.axhline(0, color='black', linewidth=0.8)\n",
        "\n",
        "    # Set common Y-label and legend.\n",
        "    axes[0].set_ylabel('Score Increase (points)')\n",
        "    handles, labels = axes[0].get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05))\n",
        "\n",
        "    # --- Save Figure ---\n",
        "    filepath = os.path.join(output_dir, \"coefplot_global_effects.png\")\n",
        "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Plot grade-interaction marginal effects.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _plot_interaction_effects(\n",
        "    ols_results: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    iv_results: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates and saves a plot of the grade-interaction marginal effects.\n",
        "\n",
        "    This function visualizes the Matthew Effect from both OLS and IV models,\n",
        "    replicating Figures 3 and 8.\n",
        "\n",
        "    Args:\n",
        "        ols_results: The dictionary of interaction OLS results.\n",
        "        iv_results: The dictionary of interaction IV results.\n",
        "        config: The validated study configuration dictionary.\n",
        "        output_dir: The directory in which to save the plot.\n",
        "\n",
        "    Returns:\n",
        "        The file path of the saved plot.\n",
        "    \"\"\"\n",
        "    # --- Plotting Setup ---\n",
        "    subjects = config['variable_definitions']['dependent_variables']\n",
        "    grade_pairs = config['model_specifications']['grade_pair_definitions']\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "    p2_name = config['variable_definitions']['phe_dummy_names'][1]\n",
        "\n",
        "    fig, axes = plt.subplots(len(subjects), len(grade_pairs), figsize=(20, 15), sharex=True, sharey=True)\n",
        "    fig.suptitle('Marginal Impact of Parental Education Across Grades (Matthew Effect)', fontsize=18)\n",
        "\n",
        "    # --- Plotting Loop ---\n",
        "    for i, subject in enumerate(subjects):\n",
        "        for j, pair in enumerate(grade_pairs):\n",
        "            ax = axes[i, j]\n",
        "            u, v = pair\n",
        "\n",
        "            # --- Plot OLS Interaction ---\n",
        "            ols_res = ols_results[subject][pair]\n",
        "            ols_coeff_name = f\"{p2_name}_x_I_{u}_{v}\"\n",
        "            ols_est = ols_res.loc[ols_coeff_name, 'Estimate']\n",
        "            ols_err = ols_est - ols_res.loc[ols_coeff_name, 'CI Lower']\n",
        "            ax.errorbar([0], [ols_est], yerr=[ols_err], fmt='o', capsize=5, label='OLS', color='black')\n",
        "\n",
        "            # --- Plot IV Interaction ---\n",
        "            iv_res = iv_results[subject][pair]\n",
        "            iv_coeff_name = f\"{endog_var}_x_I_{u}_{v}\"\n",
        "            iv_est = iv_res.loc[iv_coeff_name, 'Estimate']\n",
        "            iv_err = iv_est - iv_res.loc[iv_coeff_name, 'CI Lower']\n",
        "            ax.errorbar([1], [iv_est], yerr=[iv_err], fmt='D', capsize=5, label='IV', color='blue', markersize=8)\n",
        "\n",
        "            # --- Formatting ---\n",
        "            ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
        "            ax.set_title(f'{subject.replace(\"Score_\", \"\")}\\nGrades {u} vs {v}', fontsize=12)\n",
        "            ax.grid(True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "    # Set common labels and layout adjustments.\n",
        "    for ax in axes[-1, :]:\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_xticklabels(['OLS', 'IV'])\n",
        "    for ax in axes[:, 0]:\n",
        "        ax.set_ylabel('Marginal Change in PHE Effect (points)')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    # --- Save Figure ---\n",
        "    filepath = os.path.join(output_dir, \"coefplot_interaction_effects.png\")\n",
        "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_coefficient_plots(\n",
        "    ols_results_pooled: Dict[str, pd.DataFrame],\n",
        "    iv_results_pooled: Dict[str, pd.DataFrame],\n",
        "    ols_results_interaction: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    iv_results_interaction: Dict[str, Dict[Tuple[int, int], pd.DataFrame]],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of all primary coefficient plots.\n",
        "\n",
        "    This function creates and saves two key visualizations:\n",
        "    1. A plot of the global (pooled) effects of parental education.\n",
        "    2. A grid of plots showing the grade-interaction (Matthew Effect) margins.\n",
        "\n",
        "    Args:\n",
        "        ols_results_pooled: Results from the pooled OLS models.\n",
        "        iv_results_pooled: Results from the pooled IV models.\n",
        "        ols_results_interaction: Results from the interaction OLS models.\n",
        "        iv_results_interaction: Results from the interaction IV models.\n",
        "        config: The validated study configuration dictionary.\n",
        "        output_dir: The path to the directory where plots will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A list of file paths for all the generated plots.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(isinstance(arg, dict) for arg in [ols_results_pooled, iv_results_pooled, ols_results_interaction, iv_results_interaction]):\n",
        "        raise TypeError(\"All results inputs must be dictionaries.\")\n",
        "    if not isinstance(output_dir, str):\n",
        "        raise TypeError(\"Input 'output_dir' must be a string.\")\n",
        "\n",
        "    # --- Plot Generation ---\n",
        "    saved_paths = []\n",
        "\n",
        "    # --- Step 1: Generate and save the global effects plot ---\n",
        "    print(\"--- Generating global effects coefficient plot ---\")\n",
        "    global_plot_path = _plot_global_phe_effects(\n",
        "        ols_results=ols_results_pooled,\n",
        "        iv_results=iv_results_pooled,\n",
        "        config=config,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "    saved_paths.append(global_plot_path)\n",
        "    print(f\"Global effects plot saved to: {global_plot_path}\")\n",
        "\n",
        "    # --- Step 2: Generate and save the interaction effects plot ---\n",
        "    print(\"--- Generating interaction effects (Matthew Effect) coefficient plot ---\")\n",
        "    interaction_plot_path = _plot_interaction_effects(\n",
        "        ols_results=ols_results_interaction,\n",
        "        iv_results=iv_results_interaction,\n",
        "        config=config,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "    saved_paths.append(interaction_plot_path)\n",
        "    print(f\"Interaction effects plot saved to: {interaction_plot_path}\")\n",
        "\n",
        "    return saved_paths\n"
      ],
      "metadata": {
        "id": "g5xHSlLUTYGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Generate effort and parental-investment effect plots by grade and subject\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Generate effort and parental-investment effect plots by grade\n",
        "#          and subject\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Helper to transform results into a tidy DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_tidy_results_df(\n",
        "    results: Dict[str, Dict[int, pd.DataFrame]],\n",
        "    target_vars: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms nested regression results into a single tidy DataFrame for plotting.\n",
        "\n",
        "    Args:\n",
        "        results: A nested dictionary of regression summary tables.\n",
        "        target_vars: A list of base variable names to extract (e.g.,\n",
        "                     'days_homework_cat').\n",
        "\n",
        "    Returns:\n",
        "        A tidy pandas DataFrame with one row per coefficient per model.\n",
        "    \"\"\"\n",
        "    # --- Data Transformation ---\n",
        "    tidy_data = []\n",
        "    # Iterate through the nested results dictionary.\n",
        "    for subject, grade_results in results.items():\n",
        "        for grade, res_df in grade_results.items():\n",
        "            # Add the reference category (estimate=0) for each target variable.\n",
        "            for var in target_vars:\n",
        "                tidy_data.append({\n",
        "                    'subject': subject.replace('Score_', ''), 'grade': grade,\n",
        "                    'variable': var, 'category': 1, 'estimate': 0.0,\n",
        "                    'ci_lower': 0.0, 'ci_upper': 0.0\n",
        "                })\n",
        "\n",
        "            # Process the actual coefficients from the results table.\n",
        "            for idx, row in res_df.iterrows():\n",
        "                # Parse the variable name and category from the coefficient index.\n",
        "                for var in target_vars:\n",
        "                    if idx.startswith(var):\n",
        "                        try:\n",
        "                            category = int(idx.rsplit('_', 1)[1])\n",
        "                            tidy_data.append({\n",
        "                                'subject': subject.replace('Score_', ''), 'grade': grade,\n",
        "                                'variable': var, 'category': category,\n",
        "                                'estimate': row['Estimate'],\n",
        "                                'ci_lower': row['CI Lower'],\n",
        "                                'ci_upper': row['CI Upper']\n",
        "                            })\n",
        "                        except (ValueError, IndexError):\n",
        "                            continue # Not a parsable coefficient\n",
        "\n",
        "    return pd.DataFrame(tidy_data)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Step 2 & 3: Generic plotting function.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _plot_by_grade_effects(\n",
        "    tidy_df: pd.DataFrame,\n",
        "    variable_name: str,\n",
        "    x_labels: Dict[int, str],\n",
        "    title: str,\n",
        "    output_dir: str,\n",
        "    filename: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generic function to plot coefficient estimates by grade across subjects.\n",
        "\n",
        "    Args:\n",
        "        tidy_df: The tidy DataFrame of results.\n",
        "        variable_name: The specific variable to plot (e.g., 'days_homework_cat').\n",
        "        x_labels: A dictionary mapping category numbers to readable labels.\n",
        "        title: The main title for the figure.\n",
        "        output_dir: The directory in which to save the plot.\n",
        "        filename: The name for the saved plot file.\n",
        "\n",
        "    Returns:\n",
        "        The file path of the saved plot.\n",
        "    \"\"\"\n",
        "    # --- Plotting Setup ---\n",
        "    # Filter the data to the specific variable of interest.\n",
        "    plot_data = tidy_df[tidy_df['variable'] == variable_name].sort_values(by=['subject', 'grade', 'category'])\n",
        "    subjects = plot_data['subject'].unique()\n",
        "\n",
        "    # Create a 1xN subplot grid.\n",
        "    fig, axes = plt.subplots(1, len(subjects), figsize=(20, 6), sharey=True)\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Define aesthetics for each grade's line.\n",
        "    grade_styles = {\n",
        "        3: {'color': 'black', 'linestyle': ':', 'marker': 'o'},\n",
        "        6: {'color': 'gray', 'linestyle': '--', 'marker': 's'},\n",
        "        10: {'color': 'blue', 'linestyle': '-', 'marker': '^'}\n",
        "    }\n",
        "\n",
        "    # --- Plotting Loop ---\n",
        "    for i, subject in enumerate(subjects):\n",
        "        ax = axes[i]\n",
        "        subject_data = plot_data[plot_data['subject'] == subject]\n",
        "\n",
        "        # Plot a separate line for each grade.\n",
        "        for grade in sorted(subject_data['grade'].unique()):\n",
        "            grade_data = subject_data[subject_data['grade'] == grade]\n",
        "            style = grade_styles[grade]\n",
        "\n",
        "            # Plot the point estimates.\n",
        "            ax.plot(grade_data['category'], grade_data['estimate'],\n",
        "                    label=f'Grade {grade}', **style)\n",
        "\n",
        "            # Plot the 95% confidence interval as a shaded region.\n",
        "            ax.fill_between(grade_data['category'], grade_data['ci_lower'], grade_data['ci_upper'],\n",
        "                            color=style['color'], alpha=0.1)\n",
        "\n",
        "        # --- Formatting ---\n",
        "        ax.set_title(subject, fontsize=14)\n",
        "        ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
        "        ax.set_xticks(list(x_labels.keys()))\n",
        "        ax.set_xticklabels(list(x_labels.values()), rotation=30, ha='right')\n",
        "        ax.grid(True, linestyle='--', linewidth=0.5)\n",
        "        ax.legend()\n",
        "\n",
        "    axes[0].set_ylabel('Effect on Score (points)')\n",
        "\n",
        "    # --- Save Figure ---\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_effort_investment_plots(\n",
        "    effort_investment_results: Dict[str, Dict[int, pd.DataFrame]],\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of effort and parental investment plots.\n",
        "\n",
        "    This function creates and saves two key visualizations:\n",
        "    1. A plot showing the effect of student homework effort across grades.\n",
        "    2. A plot showing the effect of parental investment across grades.\n",
        "\n",
        "    Args:\n",
        "        effort_investment_results: The nested dictionary of results from Task 15.\n",
        "        config: The validated study configuration dictionary.\n",
        "        output_dir: The path to the directory where plots will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A list of file paths for all the generated plots.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(effort_investment_results, dict):\n",
        "        raise TypeError(\"Input 'effort_investment_results' must be a dictionary.\")\n",
        "    if not isinstance(output_dir, str):\n",
        "        raise TypeError(\"Input 'output_dir' must be a string.\")\n",
        "\n",
        "    # --- Step 1: Create a single tidy DataFrame from all results ---\n",
        "    target_vars = ['days_homework_cat', 'parent_talk_school']\n",
        "    tidy_results = _create_tidy_results_df(effort_investment_results, target_vars)\n",
        "\n",
        "    # --- Plot Generation ---\n",
        "    saved_paths = []\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Step 2: Plot effort (homework) profiles ---\n",
        "    print(\"--- Generating student effort (homework) effect plot ---\")\n",
        "    homework_labels = {\n",
        "        1: '1 day\\nor less', 2: '2-3 days', 3: '4-5 days', 4: '5+ days'\n",
        "    }\n",
        "    effort_plot_path = _plot_by_grade_effects(\n",
        "        tidy_df=tidy_results,\n",
        "        variable_name='days_homework_cat',\n",
        "        x_labels=homework_labels,\n",
        "        title='Impact of Student Effort (Homework Days) on Achievement',\n",
        "        output_dir=output_dir,\n",
        "        filename='coefplot_effort_effects.png'\n",
        "    )\n",
        "    saved_paths.append(effort_plot_path)\n",
        "    print(f\"Effort plot saved to: {effort_plot_path}\")\n",
        "\n",
        "    # --- Step 3: Plot parental investment (talk frequency) profiles ---\n",
        "    print(\"--- Generating parental investment (talk frequency) effect plot ---\")\n",
        "    talk_labels = {\n",
        "        1: 'Never', 2: '1-2/month', 3: '1-2/week', 4: 'Daily'\n",
        "    }\n",
        "    investment_plot_path = _plot_by_grade_effects(\n",
        "        tidy_df=tidy_results,\n",
        "        variable_name='parent_talk_school',\n",
        "        x_labels=talk_labels,\n",
        "        title='Impact of Parental Investment (Talk About School) on Achievement',\n",
        "        output_dir=output_dir,\n",
        "        filename='coefplot_investment_effects.png'\n",
        "    )\n",
        "    saved_paths.append(investment_plot_path)\n",
        "    print(f\"Investment plot saved to: {investment_plot_path}\")\n",
        "\n",
        "    return saved_paths\n"
      ],
      "metadata": {
        "id": "Q1mqBjBhVKNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Perform composition and balance checks across grades and years\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Perform composition and balance checks across grades and years\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Tabulate PHE shares by grade and academic year.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def check_phe_composition(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    deviation_threshold: float = 0.05\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Checks the stability of PHE group composition across grades and years.\n",
        "\n",
        "    This function calculates the proportion of students in each PHE category for\n",
        "    each grade-year cell and compares it to the overall sample proportions. It\n",
        "    flags any cell where the deviation exceeds a specified threshold, helping to\n",
        "    diagnose potential selection or composition effects.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The fully cleansed DataFrame from Task 4.\n",
        "        deviation_threshold: The maximum acceptable proportional deviation\n",
        "                             (e.g., 0.05 for 5 percentage points).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - pd.DataFrame: A table showing PHE shares for each grade-year cell.\n",
        "        - List[str]: A list of formatted strings describing any significant\n",
        "                     deviations found.\n",
        "    \"\"\"\n",
        "    # --- Overall Proportions ---\n",
        "    # Calculate the overall share of each PHE category in the entire dataset.\n",
        "    overall_shares = cleansed_df['PHE'].value_counts(normalize=True).sort_index()\n",
        "\n",
        "    # --- Cell-wise Proportions ---\n",
        "    # Group by grade and year and calculate PHE shares within each cell.\n",
        "    cell_shares = cleansed_df.groupby(['grade', 'academic_year'])['PHE'] \\\n",
        "                             .value_counts(normalize=True).unstack(level='PHE')\n",
        "\n",
        "    # --- Deviation Check ---\n",
        "    # Initialize a list to store warnings for significant deviations.\n",
        "    deviations = []\n",
        "    # Iterate through each cell to compare its shares to the overall shares.\n",
        "    for idx, row in cell_shares.iterrows():\n",
        "        for phe_level, overall_share in overall_shares.items():\n",
        "            cell_share = row.get(phe_level, 0)\n",
        "            # Check if the absolute difference exceeds the threshold.\n",
        "            if abs(cell_share - overall_share) > deviation_threshold:\n",
        "                deviations.append(\n",
        "                    f\"Composition deviation in {idx}: PHE level {phe_level} share is \"\n",
        "                    f\"{cell_share:.3f} (vs. overall {overall_share:.3f})\"\n",
        "                )\n",
        "\n",
        "    return cell_shares, deviations\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Re-validate score normalization.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def revalidate_score_normalization(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Re-validates score normalization on the final cleansed DataFrame.\n",
        "\n",
        "    This function serves as a final sanity check to ensure that no prior\n",
        "    cleaning steps have inadvertently distorted the score distributions. It\n",
        "    verifies that the mean and standard deviation of scores in each\n",
        "    grade-year cell remain within the specified tolerance.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The fully cleansed DataFrame from Task 4.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A list of formatted strings describing any normalization failures.\n",
        "    \"\"\"\n",
        "    # This logic is identical to _validate_score_normalization from Task 2.\n",
        "    # It is intentionally repeated here as a final diagnostic on the clean data.\n",
        "\n",
        "    # Extract parameters from the configuration.\n",
        "    score_cols = config['variable_definitions']['dependent_variables']\n",
        "    mean_target = config['psychometric_model']['standardization_mean']\n",
        "    std_target = config['psychometric_model']['standardization_std_dev']\n",
        "    tolerance = 3.0\n",
        "    min_sample_size = 30\n",
        "\n",
        "    # Group by grade and year.\n",
        "    grouped = cleansed_df.groupby(['grade', 'academic_year'])\n",
        "\n",
        "    # Initialize a list to store violation messages.\n",
        "    violations = []\n",
        "\n",
        "    # Iterate through each score column to validate its normalization.\n",
        "    for score_col in score_cols:\n",
        "        stats = grouped[score_col].agg(['mean', 'std', 'count']).reset_index()\n",
        "        stats_to_check = stats[stats['count'] >= min_sample_size]\n",
        "\n",
        "        # Check for mean violations.\n",
        "        mean_violations = stats_to_check[~np.isclose(stats_to_check['mean'], mean_target, atol=tolerance)]\n",
        "        for _, row in mean_violations.iterrows():\n",
        "            violations.append(\n",
        "                f\"Mean violation for '{score_col}' in (grade={row['grade']}, year={row['academic_year']}): \"\n",
        "                f\"Found {row['mean']:.2f}\"\n",
        "            )\n",
        "\n",
        "        # Check for standard deviation violations.\n",
        "        std_violations = stats_to_check[~np.isclose(stats_to_check['std'], std_target, atol=tolerance)]\n",
        "        for _, row in std_violations.iterrows():\n",
        "            violations.append(\n",
        "                f\"Std Dev violation for '{score_col}' in (grade={row['grade']}, year={row['academic_year']}): \"\n",
        "                f\"Found {row['std']:.2f}\"\n",
        "            )\n",
        "\n",
        "    return violations\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Document and report sample inclusion flow.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def report_sample_inclusion_flow(\n",
        "    cleansing_log: Dict[str, Any],\n",
        "    sample_size_log: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a publication-ready table documenting the sample inclusion flow.\n",
        "\n",
        "    This function synthesizes logs from the data cleaning and sample creation\n",
        "    tasks to produce a clear and transparent report on sample attrition at\n",
        "    each stage of the pipeline.\n",
        "\n",
        "    Args:\n",
        "        cleansing_log: The log dictionary from the `cleanse_student_survey_data`\n",
        "                       function (Task 4).\n",
        "        sample_size_log: The log dictionary of final sample sizes from the\n",
        "                         `define_analysis_samples` function (Task 9).\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame summarizing the sample flow.\n",
        "    \"\"\"\n",
        "    # --- Data Extraction from Logs ---\n",
        "    # Create a list of dictionaries to build the DataFrame.\n",
        "    flow_data = [\n",
        "        {'Stage': '1. Initial Raw Data', 'N': cleansing_log['initial_rows']},\n",
        "        {'Stage': '2. Kept: Parent Response Complete', 'N': cleansing_log['rows_after_parent_response_filter']},\n",
        "        {'Stage': '3. Kept: Valid Grade (3, 6, 10)', 'N': cleansing_log['rows_after_grade_filter']},\n",
        "        {'Stage': '4. Kept: After Exclusions & ID Checks', 'N': cleansing_log['rows_after_exclusions_and_id_check']},\n",
        "        {'Stage': '5. Final Clean Sample (after deduplication)', 'N': cleansing_log['final_rows_after_deduplication']},\n",
        "    ]\n",
        "\n",
        "    # Add final model-specific sample sizes.\n",
        "    for model_type, subjects in sample_size_log.items():\n",
        "        for subject, sizes in subjects['pooled'].items():\n",
        "            flow_data.append({\n",
        "                'Stage': f\"6. Final Sample: {model_type.upper()} Pooled ({subject.replace('Score_','')})\",\n",
        "                'N': sizes\n",
        "            })\n",
        "\n",
        "    # --- DataFrame Creation ---\n",
        "    # Convert the list to a DataFrame and set the stage as the index.\n",
        "    flow_df = pd.DataFrame(flow_data).set_index('Stage')\n",
        "\n",
        "    return flow_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def perform_diagnostic_checks(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    cleansing_log: Dict[str, Any],\n",
        "    sample_size_log: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all diagnostic and balance checks.\n",
        "\n",
        "    This function runs a series of checks on the final clean data to verify\n",
        "    sample composition stability and score normalization. It also compiles a\n",
        "    comprehensive sample attrition table.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df: The fully cleansed DataFrame from Task 4.\n",
        "        cleansing_log: The log from the data cleansing process (Task 4).\n",
        "        sample_size_log: The log of final sample sizes (Task 9).\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing all diagnostic outputs: the composition table,\n",
        "        a list of normalization failures, and the sample flow table.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(cleansed_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'cleansed_df' must be a pandas DataFrame.\")\n",
        "    if not all(isinstance(arg, dict) for arg in [cleansing_log, sample_size_log, config]):\n",
        "        raise TypeError(\"Log and config inputs must be dictionaries.\")\n",
        "\n",
        "    # --- Step 1: Check PHE Composition ---\n",
        "    print(\"--- Checking PHE composition stability ---\")\n",
        "    composition_table, composition_warnings = check_phe_composition(cleansed_df)\n",
        "    if composition_warnings:\n",
        "        print(\"Composition warnings found:\")\n",
        "        for warning in composition_warnings:\n",
        "            print(f\"  - {warning}\")\n",
        "\n",
        "    # --- Step 2: Re-validate Score Normalization ---\n",
        "    print(\"--- Re-validating score normalization ---\")\n",
        "    normalization_failures = revalidate_score_normalization(cleansed_df, config)\n",
        "    if normalization_failures:\n",
        "        print(\"Score normalization failures found:\")\n",
        "        for failure in normalization_failures:\n",
        "            print(f\"  - {failure}\")\n",
        "\n",
        "    # --- Step 3: Report Sample Inclusion Flow ---\n",
        "    print(\"--- Generating sample inclusion flow report ---\")\n",
        "    sample_flow_table = report_sample_inclusion_flow(cleansing_log, sample_size_log)\n",
        "\n",
        "    # --- Assemble Final Output ---\n",
        "    # Compile all diagnostic results into a single dictionary.\n",
        "    diagnostics = {\n",
        "        \"composition_table\": composition_table,\n",
        "        \"composition_warnings\": composition_warnings,\n",
        "        \"normalization_failures\": normalization_failures,\n",
        "        \"sample_flow_table\": sample_flow_table\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Diagnostic Checks Complete ---\")\n",
        "    return diagnostics\n"
      ],
      "metadata": {
        "id": "BihskHMQWiFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25: Design and implement an orchestrator function for the end-to-end pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Design and implement an orchestrator function for the end-to-end\n",
        "#          pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "def run_parental_environment_study_pipeline(\n",
        "    student_parent_survey_raw: pd.DataFrame,\n",
        "    barro_lee_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"results\"\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full end-to-end replication of the study's main analysis.\n",
        "\n",
        "    This master function executes the entire research pipeline, from raw data\n",
        "    ingestion and validation through feature engineering, all primary model\n",
        "    estimations (OLS and IV), and the generation of all main results,\n",
        "    diagnostics, and figures. This amended version is designed to be a component\n",
        "    of a larger workflow, returning not only the final results but also key\n",
        "    intermediate data artifacts required for subsequent downstream tasks, such\n",
        "    as robustness checks, without requiring redundant computation.\n",
        "\n",
        "    Args:\n",
        "        student_parent_survey_raw: The raw student survey DataFrame, unprocessed.\n",
        "        barro_lee_raw: The raw Barro-Lee instrument DataFrame, unprocessed.\n",
        "        config: The study's master configuration dictionary, which governs all\n",
        "                steps of the pipeline.\n",
        "        output_dir: The root directory where output artifacts, such as figures,\n",
        "                    will be saved.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two dictionaries:\n",
        "        1.  A deeply nested dictionary holding all primary outputs of the\n",
        "            analysis, including every result table, diagnostic report, and\n",
        "            a list of paths to the generated figures.\n",
        "        2.  A dictionary of key intermediate artifacts required for downstream\n",
        "            tasks. This includes the final feature-engineered DataFrame\n",
        "            ('final_df'), the complete list of generated control variable names\n",
        "            ('control_vars_list'), and the log from the initial data cleansing\n",
        "            ('cleansing_log').\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationError: If the config dictionary is invalid.\n",
        "        SchemaValidationError: If input data does not match the expected schema.\n",
        "        DataIntegrityError: If input data violates content rules.\n",
        "        RuntimeError: If any estimation step fails.\n",
        "    \"\"\"\n",
        "    # --- Stage 0: Reproducibility and Initialization ---\n",
        "    # Set the random seed from the configuration to ensure reproducibility for any\n",
        "    # stochastic processes (though this specific pipeline is deterministic).\n",
        "    np.random.seed(config['reproducibility']['random_seed'])\n",
        "\n",
        "    # Initialize the main output dictionary to structure all final results.\n",
        "    pipeline_output: Dict[str, Any] = {\n",
        "        \"results\": {}, \"diagnostics\": {}, \"figures\": {},\n",
        "        \"metadata\": {\"version\": config['reproducibility']['version']}\n",
        "    }\n",
        "\n",
        "    # --- Stage 1: Validation ---\n",
        "    # This stage acts as a gatekeeper, ensuring all inputs are valid before proceeding.\n",
        "    print(\"--- STAGE 1: VALIDATION ---\")\n",
        "    # Task 1: Validate the configuration dictionary itself. Halts if invalid.\n",
        "    validate_configuration(config)\n",
        "    # Task 2: Validate the schema and content of the primary student survey data.\n",
        "    validate_student_survey_data(student_parent_survey_raw, config)\n",
        "    # Task 3: Validate the schema and content of the external instrument data.\n",
        "    validate_barro_lee_data(barro_lee_raw, student_parent_survey_raw, config)\n",
        "    print(\"Validation successful.\\n\")\n",
        "\n",
        "    # --- Stage 2: Data Preparation and Feature Engineering ---\n",
        "    # This stage transforms the raw data into the final, analysis-ready dataset.\n",
        "    print(\"--- STAGE 2: DATA PREPARATION AND FEATURE ENGINEERING ---\")\n",
        "    # Task 4: Apply inclusion criteria and remove invalid/duplicate records.\n",
        "    cleansed_df, cleansing_log = cleanse_student_survey_data(student_parent_survey_raw, config)\n",
        "    # Task 5: Standardize formats (e.g., country codes) and document reference categories.\n",
        "    harmonized_df, _, ref_cats = harmonize_and_standardize_data(cleansed_df, config)\n",
        "    # Task 6: Construct the main endogenous variable (PHE) and its dummies.\n",
        "    df_with_phe, _ = construct_parental_education_variable(harmonized_df, config)\n",
        "    # Task 7: Construct all dummy variables for the control set.\n",
        "    df_with_controls, control_list = construct_control_variables(df_with_phe, ref_cats)\n",
        "    # Task 8: Construct the instrumental variable (GG) by merging with Barro-Lee data.\n",
        "    final_df, _ = construct_instrumental_variable(df_with_controls, barro_lee_raw, config)\n",
        "    print(\"Data preparation and feature engineering complete.\\n\")\n",
        "\n",
        "    # --- Stage 3: Model Specification and Sample Definition ---\n",
        "    # This stage prepares the blueprints for all models and the specific datasets they run on.\n",
        "    print(\"--- STAGE 3: MODEL SPECIFICATION AND SAMPLE DEFINITION ---\")\n",
        "    # Task 9: Define all model-specific analysis samples using listwise deletion.\n",
        "    analysis_samples, sample_size_log = define_analysis_samples(final_df, control_list, config)\n",
        "    # Task 10: Programmatically generate specifications for all OLS models.\n",
        "    ols_specs = specify_ols_models(control_list, config)\n",
        "    # Task 11: Programmatically generate specifications for all IV models.\n",
        "    iv_specs = specify_iv_models(control_list, config)\n",
        "    print(\"Model specifications and samples are ready.\\n\")\n",
        "\n",
        "    # --- Stage 4: Econometric Estimation ---\n",
        "    # This stage executes all primary regressions of the study.\n",
        "    print(\"--- STAGE 4: ECONOMETRIC ESTIMATION ---\")\n",
        "    # Task 13-20: Run all primary OLS and IV estimations.\n",
        "    pooled_ols_res = run_pooled_ols_estimation(analysis_samples['ols']['pooled'], ols_specs['pooled'], config)\n",
        "    inter_ols_res = run_interaction_ols_estimation(analysis_samples['ols']['interaction'], ols_specs['interaction'], config)\n",
        "    effort_res = run_effort_investment_ols_estimation(final_df, ols_specs['effort_investment'], control_list, config)\n",
        "    first_stage_res = run_iv_first_stage_estimation(analysis_samples['iv'], iv_specs, config)\n",
        "    reduced_form_res = run_iv_reduced_form_estimation(analysis_samples['iv'], iv_specs, first_stage_res, config)\n",
        "    pooled_iv_res = run_pooled_iv_estimation(analysis_samples['iv']['pooled'], iv_specs['pooled'], pooled_ols_res, first_stage_res, config)\n",
        "    by_grade_iv_res = run_by_grade_iv_estimation(analysis_samples['iv']['by_grade'], iv_specs['by_grade'], first_stage_res, config)\n",
        "    inter_iv_res = run_interaction_iv_estimation(analysis_samples['iv']['interaction'], iv_specs['interaction'], config)\n",
        "    print(\"All models estimated successfully.\\n\")\n",
        "\n",
        "    # Store all estimation results in the main output dictionary.\n",
        "    pipeline_output[\"results\"] = {\n",
        "        \"pooled_ols\": pooled_ols_res, \"interaction_ols\": inter_ols_res,\n",
        "        \"effort_investment_ols\": effort_res, \"iv_first_stage\": first_stage_res,\n",
        "        \"iv_reduced_form\": reduced_form_res, \"pooled_iv\": pooled_iv_res,\n",
        "        \"by_grade_iv\": by_grade_iv_res, \"interaction_iv\": inter_iv_res\n",
        "    }\n",
        "\n",
        "    # --- Stage 5: Visualization and Final Diagnostics ---\n",
        "    # This stage generates all figures and summary diagnostic tables.\n",
        "    print(\"--- STAGE 5: VISUALIZATION AND DIAGNOSTICS ---\")\n",
        "    # Define the output directory for figures.\n",
        "    figure_dir = os.path.join(output_dir, \"figures\")\n",
        "    # Task 21-23: Generate all plots.\n",
        "    cdf_paths = generate_cdf_plots(final_df, config, figure_dir)\n",
        "    coef_plot_paths = generate_coefficient_plots(pooled_ols_res, pooled_iv_res, inter_ols_res, inter_iv_res, config, figure_dir)\n",
        "    effort_plot_paths = generate_effort_investment_plots(effort_res, config, figure_dir)\n",
        "\n",
        "    # Store the paths to the generated figures.\n",
        "    pipeline_output[\"figures\"] = {\n",
        "        \"cdf_plots\": cdf_paths, \"coefficient_plots\": coef_plot_paths,\n",
        "        \"effort_investment_plots\": effort_plot_paths\n",
        "    }\n",
        "\n",
        "    # Task 24: Perform final diagnostic checks on the data.\n",
        "    diagnostics = perform_diagnostic_checks(cleansed_df, cleansing_log, sample_size_log, config)\n",
        "    pipeline_output[\"diagnostics\"] = diagnostics\n",
        "    print(\"Visualization and diagnostics complete.\\n\")\n",
        "\n",
        "    # --- Assemble Intermediate Artifacts for Return ---\n",
        "    # Package key objects needed by subsequent pipeline stages (e.g., robustness checks).\n",
        "    intermediate_artifacts = {\n",
        "        \"final_df\": final_df,\n",
        "        \"control_vars_list\": control_list,\n",
        "        \"cleansing_log\": cleansing_log,\n",
        "        \"sample_size_log\": sample_size_log\n",
        "    }\n",
        "\n",
        "    # Return both the final outputs and the intermediate artifacts.\n",
        "    return pipeline_output, intermediate_artifacts\n"
      ],
      "metadata": {
        "id": "07tuxOSGX1Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26: Execute robustness analyses and sensitivity checks\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Execute robustness analyses and sensitivity checks\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 1: Re-estimate pooled OLS with and without household controls.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def check_control_sensitivity_ols(\n",
        "    full_df: pd.DataFrame,\n",
        "    full_ols_results: Dict[str, pd.DataFrame],\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Checks the sensitivity of pooled OLS estimates to the inclusion of controls.\n",
        "\n",
        "    This function re-estimates the pooled OLS models using a minimal set of\n",
        "    controls (excluding household characteristics) and compares the key PHE\n",
        "    coefficients to the full model estimates to assess robustness. It performs\n",
        "    the re-estimation from scratch to ensure a complete and accurate comparison.\n",
        "\n",
        "    Args:\n",
        "        full_df: The complete, feature-engineered DataFrame.\n",
        "        full_ols_results: The results from the main pooled OLS estimation (Task 13).\n",
        "        control_vars_list: The full list of all control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame comparing the estimates, standard errors, and sample sizes\n",
        "        from the full and minimal models.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(full_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'full_df' must be a pandas DataFrame.\")\n",
        "    if not isinstance(full_ols_results, dict):\n",
        "        raise TypeError(\"Input 'full_ols_results' must be a dictionary.\")\n",
        "\n",
        "    print(\"--- Running OLS control sensitivity check ---\")\n",
        "\n",
        "    # --- 1. Define Minimal Control Set ---\n",
        "    # Programmatically determine the minimal control set by excluding household controls.\n",
        "    household_controls_base = config['variable_definitions']['household_controls']\n",
        "    minimal_control_vars = [\n",
        "        var for var in control_vars_list\n",
        "        if not any(var.startswith(hc_base) for hc_base in household_controls_base)\n",
        "    ]\n",
        "\n",
        "    # --- 2. Initialize Comparison Data Storage ---\n",
        "    comparison_data = []\n",
        "\n",
        "    # --- 3. Re-estimation Loop for each Subject ---\n",
        "    for subject in config['variable_definitions']['dependent_variables']:\n",
        "        print(f\"  - Re-estimating for {subject} with minimal controls...\")\n",
        "\n",
        "        # --- 3a. Specify Minimal Model ---\n",
        "        # Generate the model specification for the minimal control set.\n",
        "        minimal_spec = specify_ols_models(minimal_control_vars, config)['pooled'][subject]\n",
        "\n",
        "        # --- 3b. Create Minimal Sample ---\n",
        "        # The sample must be recreated as listwise deletion depends on the variable set.\n",
        "        required_cols_minimal = [minimal_spec['dependent']] + minimal_spec['exog']\n",
        "        minimal_sample = _create_analysis_sample(full_df, required_cols_minimal)\n",
        "\n",
        "        # --- 3c. Estimate Minimal Model ---\n",
        "        minimal_results_obj = estimate_model(minimal_sample, minimal_spec, config)\n",
        "\n",
        "        # --- 3d. Extract Minimal Model Results ---\n",
        "        minimal_summary = _extract_result_summary(\n",
        "            minimal_results_obj,\n",
        "            config['variable_definitions']['phe_dummy_names'],\n",
        "            config\n",
        "        )\n",
        "\n",
        "        # --- 3e. Compare Full vs. Minimal Results ---\n",
        "        for coeff in config['variable_definitions']['phe_dummy_names']:\n",
        "            # Extract statistics from the full model results.\n",
        "            full_est = full_ols_results[subject].loc[coeff, 'Estimate']\n",
        "            full_se = full_ols_results[subject].loc[coeff, 'Std. Error']\n",
        "            full_n = full_ols_results[subject].loc[coeff, 'N']\n",
        "\n",
        "            # Extract statistics from the minimal model results.\n",
        "            minimal_est = minimal_summary.loc[coeff, 'Estimate']\n",
        "            minimal_se = minimal_summary.loc[coeff, 'Std. Error']\n",
        "            minimal_n = minimal_summary.loc[coeff, 'N']\n",
        "\n",
        "            # Calculate the percentage change in the coefficient estimate.\n",
        "            pct_change = ((minimal_est - full_est) / full_est) * 100 if full_est != 0 else float('inf')\n",
        "\n",
        "            # Append all comparison data to the list.\n",
        "            comparison_data.append({\n",
        "                'Subject': subject.replace('Score_', ''),\n",
        "                'Coefficient': coeff,\n",
        "                'Estimate_Full': full_est,\n",
        "                'StdErr_Full': full_se,\n",
        "                'N_Full': int(full_n),\n",
        "                'Estimate_Minimal': minimal_est,\n",
        "                'StdErr_Minimal': minimal_se,\n",
        "                'N_Minimal': int(minimal_n),\n",
        "                'Pct_Change': pct_change,\n",
        "            })\n",
        "\n",
        "    # --- 4. Create Final Comparison DataFrame ---\n",
        "    # Convert the list of dictionaries to a well-structured DataFrame.\n",
        "    comparison_df = pd.DataFrame(comparison_data).set_index(['Subject', 'Coefficient'])\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 2: Validate IV diagnostics robustness across specifications.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def review_iv_diagnostics(first_stage_results: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reviews and reports on the validity of the instrument across all models.\n",
        "\n",
        "    This function systematically checks the first-stage results for all IV\n",
        "    models, filtering and returning a DataFrame that lists any specifications\n",
        "    that fail the weak instrument test (F-stat < 10) or have an unexpected\n",
        "    sign on the instrument coefficient.\n",
        "\n",
        "    Args:\n",
        "        first_stage_results: The DataFrame of results from Task 16.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame listing all IV specifications that failed one or more\n",
        "        diagnostic checks. An empty DataFrame is returned if all checks pass.\n",
        "    \"\"\"\n",
        "    print(\"--- Reviewing all IV first-stage diagnostics ---\")\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(first_stage_results, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'first_stage_results' must be a pandas DataFrame.\")\n",
        "    required_cols = ['is_strong', 'sign_is_correct']\n",
        "    if not all(col in first_stage_results.columns for col in required_cols):\n",
        "        raise SchemaValidationError(f\"Missing required diagnostic columns: {required_cols}\")\n",
        "\n",
        "    # --- Filtering for Failures ---\n",
        "    # Create a boolean mask to identify any models that fail either check.\n",
        "    failed_diagnostics_mask = (\n",
        "        (first_stage_results['is_strong'] == False) |\n",
        "        (first_stage_results['sign_is_correct'] == False)\n",
        "    )\n",
        "\n",
        "    # Apply the mask to get the DataFrame of failed models.\n",
        "    failed_diagnostics_df = first_stage_results[failed_diagnostics_mask]\n",
        "\n",
        "    # --- Reporting ---\n",
        "    if failed_diagnostics_df.empty:\n",
        "        print(\"All IV first-stage diagnostics passed successfully.\")\n",
        "    else:\n",
        "        # Issue a single, comprehensive warning listing the failed models.\n",
        "        warnings.warn(\n",
        "            f\"Found {len(failed_diagnostics_df)} IV specification(s) with potential \"\n",
        "            f\"instrument validity issues. See the returned DataFrame for details.\",\n",
        "            UserWarning\n",
        "        )\n",
        "\n",
        "    return failed_diagnostics_df\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Validate effort and investment pattern consistency.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_developmental_patterns(\n",
        "    effort_investment_results: Dict[str, Dict[int, pd.DataFrame]],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formally validates results against expected developmental patterns.\n",
        "\n",
        "    This function programmatically tests a series of hypotheses about the signs\n",
        "    and trends of effort and investment coefficients across grades and subjects,\n",
        "    producing a detailed, auditable report of the outcomes.\n",
        "\n",
        "    Args:\n",
        "        effort_investment_results: The nested dictionary of results from Task 15.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame detailing each pattern check and its outcome.\n",
        "    \"\"\"\n",
        "    print(\"--- Validating developmental patterns in effort/investment models ---\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    report_data = []\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1']\n",
        "\n",
        "    # --- Define Hypotheses to Test ---\n",
        "    # Each dictionary defines a specific pattern to check.\n",
        "    hypotheses = [\n",
        "        {'check': 'Homework G3 Sign (Math)', 'subject': 'Score_Math', 'grade': 3, 'coeff': 'days_homework_cat_4', 'test': 'est < 0'},\n",
        "        {'check': 'Homework G10 Sign (Math)', 'subject': 'Score_Math', 'grade': 10, 'coeff': 'days_homework_cat_4', 'test': 'est > 0 and pval < alpha'},\n",
        "        {'check': 'Parental Talk Trend (English)', 'subject': 'Score_Eng', 'coeff': 'parent_talk_school_4', 'test': 'est_g10 > est_g3'},\n",
        "        {'check': 'Parental Talk Trend (Literature)', 'subject': 'Score_Lit', 'coeff': 'parent_talk_school_4', 'test': 'est_g10 < est_g3'},\n",
        "    ]\n",
        "\n",
        "    # --- Validation Loop ---\n",
        "    for hypo in hypotheses:\n",
        "        passed = False\n",
        "        details = \"Check not performed due to missing data.\"\n",
        "        try:\n",
        "            if 'trend' in hypo['check'].lower():\n",
        "                # This is a cross-grade comparison.\n",
        "                est_g3 = effort_investment_results[hypo['subject']][3].loc[hypo['coeff'], 'Estimate']\n",
        "                est_g10 = effort_investment_results[hypo['subject']][10].loc[hypo['coeff'], 'Estimate']\n",
        "                passed = eval(hypo['test'])\n",
        "                details = f\"G3 Est: {est_g3:.3f}, G10 Est: {est_g10:.3f}\"\n",
        "            else:\n",
        "                # This is a single-cell check.\n",
        "                res_df = effort_investment_results[hypo['subject']][hypo['grade']]\n",
        "                est = res_df.loc[hypo['coeff'], 'Estimate']\n",
        "                pval = res_df.loc[hypo['coeff'], 'P-value']\n",
        "                passed = eval(hypo['test'])\n",
        "                details = f\"Est: {est:.3f}, P-val: {pval:.3f}\"\n",
        "        except KeyError:\n",
        "            # Gracefully handle cases where a coefficient might be missing.\n",
        "            pass\n",
        "\n",
        "        report_data.append({\n",
        "            'Pattern Check': hypo['check'],\n",
        "            'Expectation': hypo['test'],\n",
        "            'Passed': passed,\n",
        "            'Details': details\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(report_data).set_index('Pattern Check')\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_robustness_checks(\n",
        "    pipeline_outputs: Dict[str, Any],\n",
        "    full_df: pd.DataFrame,\n",
        "    control_vars_list: List[str],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all robustness and sensitivity checks.\n",
        "\n",
        "    This function runs a series of post-estimation checks to validate the\n",
        "    stability and credibility of the main findings. It assesses sensitivity\n",
        "    to control variable inclusion, reviews all IV diagnostics, and formally\n",
        "    validates key developmental patterns in the data.\n",
        "\n",
        "    Args:\n",
        "        pipeline_outputs: The main dictionary of results from the pipeline.\n",
        "        full_df: The complete, feature-engineered DataFrame.\n",
        "        control_vars_list: The full list of all control variable names.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the outputs of all robustness checks, including\n",
        "        DataFrames for sensitivity analysis and diagnostic failures, and a\n",
        "        report on developmental pattern validation.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- STAGE 6: ROBUSTNESS AND SENSITIVITY CHECKS ---\")\n",
        "\n",
        "    # --- Step 1: OLS Control Sensitivity ---\n",
        "    sensitivity_report = check_control_sensitivity_ols(\n",
        "        full_df=full_df,\n",
        "        full_ols_results=pipeline_outputs['results']['pooled_ols'],\n",
        "        control_vars_list=control_vars_list,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: IV Diagnostics Review ---\n",
        "    iv_diagnostic_failures = review_iv_diagnostics(\n",
        "        first_stage_results=pipeline_outputs['results']['iv_first_stage']\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Developmental Pattern Validation ---\n",
        "    pattern_report = validate_developmental_patterns(\n",
        "        effort_investment_results=pipeline_outputs['results']['effort_investment_ols'],\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Assemble Final Output ---\n",
        "    robustness_results = {\n",
        "        \"ols_control_sensitivity\": sensitivity_report,\n",
        "        \"iv_diagnostic_failures\": iv_diagnostic_failures,\n",
        "        \"developmental_pattern_validation\": pattern_report\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Robustness Checks Complete ---\")\n",
        "    return robustness_results\n"
      ],
      "metadata": {
        "id": "6tjGqy1vegsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Synthesize and interpret the final results in alignment with the study's conclusions\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Synthesize and interpret the final results in alignment with the\n",
        "#          study's conclusions\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Step 1: Helper to summarize persistent effects.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _summarize_persistent_effects(\n",
        "    pooled_iv_results: Dict[str, pd.DataFrame],\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a narrative summary of the causal persistent effects.\n",
        "\n",
        "    Args:\n",
        "        pooled_iv_results: The dictionary of pooled IV results from Task 18.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string summarizing the main causal findings.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    summary_lines = [\"Summary of Causal Persistent Effects (Pooled IV Models):\"]\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_3'] # p < 0.01\n",
        "\n",
        "    # --- Narrative Generation Loop ---\n",
        "    try:\n",
        "        for subject_raw, res_df in pooled_iv_results.items():\n",
        "            subject = subject_raw.replace('Score_', '')\n",
        "            estimate = res_df.loc[endog_var, 'Estimate']\n",
        "            p_value = res_df.loc[endog_var, 'P-value']\n",
        "\n",
        "            # Determine significance level for the narrative.\n",
        "            if p_value < alpha:\n",
        "                significance_text = f\"(p < {alpha})\"\n",
        "            else:\n",
        "                significance_text = f\"(p = {p_value:.3f})\"\n",
        "\n",
        "            # Assemble the sentence.\n",
        "            line = (\n",
        "                f\"- For {subject}, a one-level increase in parental education (PHE) has a \"\n",
        "                f\"statistically significant causal effect, increasing student scores by \"\n",
        "                f\"{estimate:.2f} points {significance_text}.\"\n",
        "            )\n",
        "            summary_lines.append(line)\n",
        "\n",
        "        # --- Validate Ordering ---\n",
        "        eng_eff = pooled_iv_results['Score_Eng'].loc[endog_var, 'Estimate']\n",
        "        lit_eff = pooled_iv_results['Score_Lit'].loc[endog_var, 'Estimate']\n",
        "        math_eff = pooled_iv_results['Score_Math'].loc[endog_var, 'Estimate']\n",
        "\n",
        "        if eng_eff > lit_eff > math_eff:\n",
        "            ordering_text = \"As expected, the magnitude of this effect is largest in English, followed by Literature, and then Mathematics.\"\n",
        "        else:\n",
        "            ordering_text = \"The expected ordering of effect sizes (English > Literature > Math) was not observed.\"\n",
        "        summary_lines.append(f\"- {ordering_text}\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        return f\"Could not generate persistent effect summary due to missing result: {e}\"\n",
        "\n",
        "    return \"\\n\".join(summary_lines)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Step 2: Helper to summarize Matthew-effect patterns.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _summarize_matthew_effects(\n",
        "    interaction_iv_results: Dict[str, Dict[tuple, pd.DataFrame]],\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a narrative summary of the causal Matthew-effect patterns.\n",
        "\n",
        "    Args:\n",
        "        interaction_iv_results: The dictionary of interaction IV results.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string summarizing the dynamic findings for each subject.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    summary_lines = [\"\\nSummary of Causal Matthew Effects (Interaction IV Models):\"]\n",
        "    endog_var = config['variable_definitions']['endogenous_variable']\n",
        "    alpha = config['inference_parameters']['significance_levels']['star_1'] # p < 0.10\n",
        "\n",
        "    # --- Narrative Generation Loop ---\n",
        "    try:\n",
        "        for subject_raw in config['variable_definitions']['dependent_variables']:\n",
        "            subject = subject_raw.replace('Score_', '')\n",
        "\n",
        "            # Extract the key interaction coefficients (α̂₂) for the two transitions.\n",
        "            res_3_6 = interaction_iv_results[subject_raw][(3, 6)]\n",
        "            res_6_10 = interaction_iv_results[subject_raw][(6, 10)]\n",
        "\n",
        "            coeff_name_3_6 = f\"{endog_var}_x_I_3_6\"\n",
        "            coeff_name_6_10 = f\"{endog_var}_x_I_6_10\"\n",
        "\n",
        "            est_3_6 = res_3_6.loc[coeff_name_3_6, 'Estimate']\n",
        "            pval_3_6 = res_3_6.loc[coeff_name_3_6, 'P-value']\n",
        "            est_6_10 = res_6_10.loc[coeff_name_6_10, 'Estimate']\n",
        "            pval_6_10 = res_6_10.loc[coeff_name_6_10, 'P-value']\n",
        "\n",
        "            # --- Pattern Recognition Logic ---\n",
        "            line = f\"- For {subject}: \"\n",
        "            if est_6_10 < 0 and pval_6_10 < alpha and est_3_6 < 0:\n",
        "                line += \"The influence of parental education shows a pattern of decline, becoming weaker as students age (emancipation effect).\"\n",
        "            elif est_3_6 > 0 and est_6_10 < 0:\n",
        "                line += \"The influence follows a bell-shaped curve, increasing in middle childhood and then declining in adolescence.\"\n",
        "            elif est_3_6 > 0 and pval_3_6 < alpha and est_6_10 > 0 and pval_6_10 < alpha:\n",
        "                line += \"The influence is continuously and significantly increasing, indicating a strong Matthew Effect and growing social determinism.\"\n",
        "            else:\n",
        "                line += \"The pattern is mixed or not statistically significant.\"\n",
        "\n",
        "            summary_lines.append(line)\n",
        "\n",
        "    except KeyError as e:\n",
        "        return f\"Could not generate Matthew Effect summary due to missing result: {e}\"\n",
        "\n",
        "    return \"\\n\".join(summary_lines)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: Helper to confirm policy implications.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _confirm_policy_implications(\n",
        "    matthew_effect_summary: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a narrative of policy implications based on the findings.\n",
        "\n",
        "    Args:\n",
        "        matthew_effect_summary: The string summary from the previous step.\n",
        "\n",
        "    Returns:\n",
        "        A formatted string outlining the policy implications.\n",
        "    \"\"\"\n",
        "    # --- Policy Narrative Generation ---\n",
        "    implication_lines = [\"\\nPolicy Implications based on Findings:\"]\n",
        "\n",
        "    # This logic maps the findings from the summary text to policy statements.\n",
        "    if \"English: The influence is continuously and significantly increasing\" in matthew_effect_summary:\n",
        "        implication_lines.append(\n",
        "            \"- The strong Matthew Effect in English suggests that interventions must be early and sustained. \"\n",
        "            \"Heavy reliance on foreign language skills in high-stakes admissions may exacerbate inequality of opportunity.\"\n",
        "        )\n",
        "\n",
        "    if \"Mathematics: The influence of parental education shows a pattern of decline\" in matthew_effect_summary or \\\n",
        "       \"Literature: The influence follows a bell-shaped curve\" in matthew_effect_summary:\n",
        "        implication_lines.append(\n",
        "            \"- The declining parental influence in Mathematics and Literature in later grades (adolescence) \"\n",
        "            \"indicates a window of opportunity. Remediation programs targeting student effort and cognitive skills \"\n",
        "            \"can be effective in these subjects for older students.\"\n",
        "        )\n",
        "\n",
        "    implication_lines.append(\n",
        "        \"- Overall, the results justify a transition from a uniform educational policy to a dynamic, \"\n",
        "        \"subject- and age-specific strategy to maximize equality of opportunity.\"\n",
        "    )\n",
        "\n",
        "    return \"\\n\".join(implication_lines)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def synthesize_final_report(\n",
        "    pipeline_outputs: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Synthesizes and interprets the final results into a narrative report.\n",
        "\n",
        "    This function takes the complete set of pipeline outputs and generates a\n",
        "    human-readable text summary that covers the main causal findings, the\n",
        "    dynamic Matthew-effect patterns, and their direct policy implications,\n",
        "    aligning with the conclusions of the original study.\n",
        "\n",
        "    Args:\n",
        "        pipeline_outputs: The main dictionary of results from the pipeline.\n",
        "        config: The validated study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A single, formatted string containing the complete summary report.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(pipeline_outputs, dict):\n",
        "        raise TypeError(\"Input 'pipeline_outputs' must be a dictionary.\")\n",
        "    if 'results' not in pipeline_outputs:\n",
        "        raise ValueError(\"'results' key missing from pipeline_outputs.\")\n",
        "\n",
        "    # --- Report Generation ---\n",
        "    # Initialize a list to hold the sections of the report.\n",
        "    report_sections = [f\"Final Report: Synthesis of Findings (Version {config['reproducibility']['version']})\", \"=\"*80]\n",
        "\n",
        "    # --- Step 1: Summarize Persistent Effects ---\n",
        "    persistent_summary = _summarize_persistent_effects(\n",
        "        pooled_iv_results=pipeline_outputs['results']['pooled_iv'],\n",
        "        config=config\n",
        "    )\n",
        "    report_sections.append(persistent_summary)\n",
        "\n",
        "    # --- Step 2: Summarize Matthew-Effect Patterns ---\n",
        "    matthew_effect_summary = _summarize_matthew_effects(\n",
        "        interaction_iv_results=pipeline_outputs['results']['interaction_iv'],\n",
        "        config=config\n",
        "    )\n",
        "    report_sections.append(matthew_effect_summary)\n",
        "\n",
        "    # --- Step 3: Confirm Policy Implications ---\n",
        "    policy_implications = _confirm_policy_implications(matthew_effect_summary)\n",
        "    report_sections.append(policy_implications)\n",
        "\n",
        "    # --- Final Assembly ---\n",
        "    # Join all sections into a single string.\n",
        "    final_report = \"\\n\".join(report_sections)\n",
        "\n",
        "    print(\"\\n--- Final Report Generated Successfully ---\")\n",
        "    print(final_report)\n",
        "\n",
        "    return final_report\n"
      ],
      "metadata": {
        "id": "GyyKiR5Oh-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator: A single function to run the entire study\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_full_study(\n",
        "    student_parent_survey_raw: pd.DataFrame,\n",
        "    barro_lee_raw: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str = \"results\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire end-to-end research pipeline for the study.\n",
        "\n",
        "    This top-level orchestrator serves as the main entry point for the project.\n",
        "    It runs the three primary stages of the analysis in sequence:\n",
        "    1.  The main analytical pipeline (data validation, cleaning, feature\n",
        "        engineering, all OLS and IV estimations, and primary visualizations).\n",
        "    2.  A suite of robustness and sensitivity checks on the main results.\n",
        "    3.  The synthesis of a final, human-readable report summarizing the key\n",
        "        findings and policy implications.\n",
        "\n",
        "    Args:\n",
        "        student_parent_survey_raw: The raw student survey DataFrame.\n",
        "        barro_lee_raw: The raw Barro-Lee instrument DataFrame.\n",
        "        config: The study's master configuration dictionary.\n",
        "        output_dir: The root directory to save all output artifacts (e.g., figures).\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing all outputs from the entire\n",
        "        study, including all result tables, diagnostic reports, robustness\n",
        "        checks, figure paths, and the final narrative report.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the primary inputs are of the correct type before starting.\n",
        "    if not isinstance(student_parent_survey_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'student_parent_survey_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(barro_lee_raw, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'barro_lee_raw' must be a pandas DataFrame.\")\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Input 'config' must be a dictionary.\")\n",
        "\n",
        "    # --- Master Execution Block ---\n",
        "    # Wrap the entire process in a try-except block for robust, top-level error handling.\n",
        "    try:\n",
        "        # --- STAGE I: MAIN ANALYTICAL PIPELINE ---\n",
        "        # Announce the start of the main analysis stage.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Executing Main Analytical Pipeline (Task 25)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Execute the main pipeline. This single call performs all tasks from 1 to 25.\n",
        "        # It returns the primary results and the intermediate artifacts needed for the next stage.\n",
        "        main_pipeline_outputs, intermediate_artifacts = run_parental_environment_study_pipeline(\n",
        "            student_parent_survey_raw=student_parent_survey_raw,\n",
        "            barro_lee_raw=barro_lee_raw,\n",
        "            config=config,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # --- STAGE II: ROBUSTNESS AND SENSITIVITY CHECKS ---\n",
        "        # Announce the start of the robustness checks stage.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Executing Robustness and Sensitivity Checks (Task 26)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Execute the robustness checks from Task 26.\n",
        "        # This function now receives the necessary data artifacts directly from the\n",
        "        # previous stage, eliminating redundant processing and ensuring consistency.\n",
        "        robustness_outputs = run_robustness_checks(\n",
        "            pipeline_outputs=main_pipeline_outputs,\n",
        "            full_df=intermediate_artifacts[\"final_df\"],\n",
        "            control_vars_list=intermediate_artifacts[\"control_vars_list\"],\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # --- STAGE III: FINAL REPORT SYNTHESIS ---\n",
        "        # Announce the start of the final report generation stage.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Synthesizing Final Report (Task 27)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Execute the report synthesis from Task 27, which translates the\n",
        "        # quantitative results into a narrative summary.\n",
        "        final_report_text = synthesize_final_report(\n",
        "            pipeline_outputs=main_pipeline_outputs,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # --- Final Output Assembly ---\n",
        "        # Combine all outputs from all stages into a single, comprehensive dictionary.\n",
        "        full_study_output = {\n",
        "            \"main_analysis\": main_pipeline_outputs,\n",
        "            \"robustness_checks\": robustness_outputs,\n",
        "            \"final_summary_report\": final_report_text\n",
        "        }\n",
        "\n",
        "        # Announce the successful completion of the entire pipeline.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"TOP-LEVEL ORCHESTRATOR COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Return the final, complete archive of the study's execution.\n",
        "        return full_study_output\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any part of the pipeline fails, provide a clear, top-level error message.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"!!! TOP-LEVEL ORCHESTRATOR FAILED !!!\")\n",
        "        print(f\"An unrecoverable error occurred during the pipeline execution: {e}\")\n",
        "        print(\"=\"*80)\n",
        "        # Re-raise the exception to halt execution and provide a full traceback for debugging.\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "NxYsFxXnmVjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}